{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d726cc",
   "metadata": {
    "id": "65d726cc"
   },
   "source": [
    "# Sequências - Aula Prática\n",
    "## RNNs (Recurrent Neural Networks)\n",
    "\n",
    "Neste notebook iremos continuar nossos estudos de redes neurais recorrentes (RNNs), trabalhando dessa vez com modelos `seq2seq` (*sequence-to-sequence*) para construir um tradutor de francês para inglês, uma ideia simples mas poderosa onde duas RNNs trabalham em conjunto para transformar uma sequência em outra.\n",
    "\n",
    "![](https://github.com/ThiagoPoppe/ciag2024/blob/main/imagens/seq2seq.png?raw=true)\n",
    "\n",
    "- Esse notebook foi fortemente inspirado no terceiro tutorial da série [NLP From Scratch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), disponibilizado no site do PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CcxLr5dWIU3z",
   "metadata": {
    "id": "CcxLr5dWIU3z"
   },
   "source": [
    "- **Importante:** caso esteja rodando esse notebook no ambiente da Tatu, favor executar a seguinte célula. Caso contrário, basta ignorar a sua execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t00jcp0mIU3z",
   "metadata": {
    "id": "t00jcp0mIU3z",
    "outputId": "87b4ff8a-6787-4f08-8a00-70f399e4c78f"
   },
   "outputs": [],
   "source": [
    "%load_ext nbproxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wa27V4VBIU30",
   "metadata": {
    "id": "Wa27V4VBIU30"
   },
   "source": [
    "Instalando pacotes necessários para realizarmos manipulações e outras operações com texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WxvavD60IU30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxvavD60IU30",
    "outputId": "768cb130-dd88-4976-e5c6-ae8763530575"
   },
   "outputs": [],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071c310f",
   "metadata": {
    "id": "071c310f"
   },
   "source": [
    "## Importação de pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a1662",
   "metadata": {
    "id": "2f2a1662"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b6b30-6d1a-4b86-b15f-67e02560c004",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "795b6b30-6d1a-4b86-b15f-67e02560c004",
    "outputId": "ba08ea4e-1c70-4135-f085-eff2571b9360"
   },
   "outputs": [],
   "source": [
    "# Verificando se temos CUDA disponível e selecionando o device que será utilizado\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device escolhido:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2rncJrNgIU31",
   "metadata": {
    "id": "2rncJrNgIU31"
   },
   "source": [
    "## Entendimento da base de dados\n",
    "\n",
    "A base de dados trabalhada durante esse notebook consiste em um arquivo `.txt` que possui pares de frases tanto em inglês quanto em francês, separados por um *tab*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9qLr4SlnIU31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qLr4SlnIU31",
    "outputId": "bb9c0bef-d905-4daa-e611-b29496616efa"
   },
   "outputs": [],
   "source": [
    "!head /pgeoprj/ciag2023/datasets/sequence_datasets/seq2seq/eng-fra.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cefc1bd-91a1-4c01-a1df-8cb2c1ccc31a",
   "metadata": {
    "id": "4cefc1bd-91a1-4c01-a1df-8cb2c1ccc31a"
   },
   "source": [
    "## Processamento da base de dados\n",
    "\n",
    "De maneira similar à codificação feita em notebooks anteriores, iremos representar cada palavra de uma linguagem como sendo um índice numérico. Como iremos trabalhar com mais de uma linguagem, é natural criarmos uma classe, denominada de `Language`, para conter os dicionários que irão mapear palavras para índices (`word2idx`) e índices para palavras (`idx2word`).\n",
    "\n",
    "Além disso, manteremos um terceiro dicionário chamado de `word_counter` para manter salvo a frequência de cada palavra nos dados daquela linguagem. Futuramente, iremos utilizar essa frequência para filtrar entradas que contém palavras raras, reduzindo assim a gigantesca base de dados para termos algo que executa de forma mais rápida, indiretamente facilitando o treinamento da nossa rede.\n",
    "\n",
    "> Note que nesse notebook nós faremos o uso dos *tokens* especiais: **\\<pad\\>**, **\\<sos\\>** e **\\<eos\\>**, definindo de forma manual os índices relacionados com cada *token*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IzXYG_DkIU31",
   "metadata": {
    "id": "IzXYG_DkIU31"
   },
   "outputs": [],
   "source": [
    "PAD_INDEX = 0\n",
    "SOS_INDEX = 1\n",
    "EOS_INDEX = 2\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.idx_counter = 3\n",
    "\n",
    "        self.word2idx = {}\n",
    "        self.word_counter = {}\n",
    "        self.idx2word = {PAD_INDEX: '<pad>', SOS_INDEX: '<sos>', EOS_INDEX: '<eos>'}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.idx_counter\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word_counter[word] = 1\n",
    "            self.word2idx[word] = self.idx_counter\n",
    "            self.idx2word[self.idx_counter] = word\n",
    "            self.idx_counter += 1\n",
    "        else:\n",
    "            self.word_counter[word] += 1\n",
    "\n",
    "    def trim(self, threshold):\n",
    "        keep_words = []\n",
    "\n",
    "        for word, count in self.word_counter.items():\n",
    "            if count >= threshold:\n",
    "                keep_words.append(word)\n",
    "\n",
    "        ratio = len(keep_words) / len(self.word2idx)\n",
    "        print(f'Razão de palavras mantidos: {ratio:.4f}')\n",
    "\n",
    "        # Reinicializando dicionários\n",
    "        self.word2idx = {}\n",
    "        self.word2count = {}\n",
    "        self.idx2word = {PAD_INDEX: '<pad>', SOS_INDEX: '<sos>', EOS_INDEX: '<eos>'}\n",
    "        self.idx_counter = 3\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.add_word(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PscccSS0IU31",
   "metadata": {
    "id": "PscccSS0IU31"
   },
   "source": [
    "Os textos do arquivo estão todos em [Unicode](https://pt.wikipedia.org/wiki/Unicode). Para simplificar o dado, iremos converter todos os caracteres Unicode em [ASCII](https://pt.wikipedia.org/wiki/ASCII), tornando tudo em caixa baixa e removendo boa parte dos sinais diacríticos através da função `preprocess_string`.\n",
    "\n",
    "> O módulo `re` é para aproveitarmos do poder de utilizar *expressões regulares*, ou do inglês *regular expressions*, permitindo realizar casamento e processamento de *strings* rapidamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q_EjwJCeIU31",
   "metadata": {
    "id": "q_EjwJCeIU31",
    "outputId": "36318012-a66d-4151-a80e-c4626464ed1d"
   },
   "outputs": [],
   "source": [
    "def preprocess_string(string):\n",
    "    ascii = unidecode(string).lower().strip()\n",
    "    ascii = re.sub(r\"([.!?])\", r\" \\1\", ascii)  # colocando um espaço entre texto e pontuação\n",
    "    ascii = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", ascii)  # removendo tudo que não é um token válido\n",
    "    ascii = ascii.strip()\n",
    "\n",
    "    return ascii\n",
    "\n",
    "# Exemplos de uso do preprocess_string\n",
    "print(preprocess_string('kožušček'))\n",
    "print(preprocess_string('François'))\n",
    "print(preprocess_string(\"J'ai froid.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NLDGLEotIU31",
   "metadata": {
    "id": "NLDGLEotIU31"
   },
   "source": [
    "Para ler os dados do arquivo `.txt`, nós iremos separar o conteúdo em linhas e depois cada linha em pares `(frase1, frase2)`. Como vimos anteriormente, as frases estão separadas por *tab* (`\\t`), onde a primeira frase está em inglês e a segunda em francês. Já que queremos fazer uma tradução no sentido francês $\\rightarrow$ inglês, iremos salvar os pares na ordem inversa, mantendo uma ideia de `(entrada, saída)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08b11f-e3ff-4a75-9f06-c958a18ad791",
   "metadata": {
    "id": "3b08b11f-e3ff-4a75-9f06-c958a18ad791",
    "outputId": "589431ae-2360-4644-fd67-c763593a44d8"
   },
   "outputs": [],
   "source": [
    "def build_pairs():\n",
    "    with open('/pgeoprj/ciag2023/datasets/sequence_datasets/seq2seq/eng-fra.txt', 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "\n",
    "    pairs = []\n",
    "    for line in lines:\n",
    "        sentence1, sentence2 = line.split('\\t')\n",
    "        pairs.append((preprocess_string(sentence2), preprocess_string(sentence1)))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "pairs = build_pairs()\n",
    "print('Número de pares:', len(pairs))\n",
    "print('Exemplo dos primeiros 3 pares:', pairs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A7X6iDTAIU32",
   "metadata": {
    "id": "A7X6iDTAIU32"
   },
   "source": [
    "Como temos muitos exemplos de frases, e queremos treinar algo relativamente rápido, iremos reduzir o número de sentenças e usar apenas entradas relativamente simples e curtas, com no máximo 10 palavras (incluindo `<eos>`). Além disso, iremos trabalhar apenas com entradas que traduzem para algo do tipo \"*I am*\", \"*He is*\", etc. Tudo isso será condensado na função `filter_pair`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8148e-7955-4599-b63f-c297e9ec2a70",
   "metadata": {
    "id": "c8a8148e-7955-4599-b63f-c297e9ec2a70",
    "outputId": "84596921-904a-4ef4-c896-5ed2335ae972"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    'i am ', 'i m ',\n",
    "    'he is', 'he s ',\n",
    "    'she is', 'she s ',\n",
    "    'you are', 'you re ',\n",
    "    'we are', 'we re ',\n",
    "    'they are', 'they re '\n",
    ")\n",
    "\n",
    "def filter_pair(pair):\n",
    "    text1_length = len(pair[0].split(' '))\n",
    "    text2_length = len(pair[1].split(' '))\n",
    "\n",
    "    return text1_length < MAX_LENGTH and \\\n",
    "           text2_length < MAX_LENGTH and \\\n",
    "           pair[1].startswith(eng_prefixes)\n",
    "\n",
    "pairs = list(filter(filter_pair, pairs))\n",
    "print('Novo número de pares:', len(pairs))\n",
    "print('Exemplo dos três primeiros novos pares:', pairs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j-Nu4kLPIU32",
   "metadata": {
    "id": "j-Nu4kLPIU32"
   },
   "source": [
    "Finalmente, iremos criar o escopo de cada linguagem que iremos trabalhar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7C4VAXRIU32",
   "metadata": {
    "id": "c7C4VAXRIU32",
    "outputId": "775efbf7-60d4-407f-c3c6-d804bfbe5b41"
   },
   "outputs": [],
   "source": [
    "def build_languages(pairs):\n",
    "    input_language = Language('fra')\n",
    "    output_language = Language('eng')\n",
    "\n",
    "    for sentence1, sentence2 in pairs:\n",
    "        input_language.add_sentence(sentence1)\n",
    "        output_language.add_sentence(sentence2)\n",
    "\n",
    "    return input_language, output_language\n",
    "\n",
    "input_language, output_language = build_languages(pairs)\n",
    "print(f'Número de tokens da linguagem {input_language.name}: {len(input_language)}')\n",
    "print(f'Número de tokens da linguagem {output_language.name}: {len(output_language)}')\n",
    "print('Exemplo aleatório de um par:', random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4BjmEtjlIU32",
   "metadata": {
    "id": "4BjmEtjlIU32"
   },
   "source": [
    "## Construção de um `Dataset` e `DataLoader`\n",
    "\n",
    "Finalmente, iremos estudar uma forma de agrupar sequências de tamanhos diferentes em um único *batch* de dados, permitindo trabalhar com `batch_size` em modelos recorrentes.\n",
    "\n",
    "Para isso, faremos um uso extensivo do módulo `torch.nn.utils.rnn`, ou `rnn_utils` como definimos nos imports no começo desse notebook. A partir desse módulo, conseguimos aplicar *padding* em um conjunto de sequências, uniformizando o tamanho das sequências. Além disso, PyTorch disponibiliza um tipo específico de dados chamado de `PackedSequence`, destinado exclusivamente para modelos recorrentes, onde podemos usar as funções `pack_padded_sequence` e `pad_packed_sequence`, para converter de tensores para `PackedSequences` e vice-versa.\n",
    "\n",
    "> **Importante:** O uso de `PackedSequences` é interessante quando trabalhamos com dados de tamanho variável, uma vez que os modelos do PyTorch conseguem efetivamente \"pular\" *tokens* `<pad>`, poupando memória e computação! Porém, o seu uso é bastante confuso e mal documentado. Como alternativa, podemos criar uma máscara e mascarar a função de perda do nosso modelo, fazendo com que gradientes associados com `<pad>` não sejam usados durante a atualização dos pesos da rede.\n",
    "\n",
    "Como estamos trabalhando com sequências de tamanho variável, precisamos \"ensinar\" para o PyTorch como que iremos \"colar\" todas essas sequências para formar um único *batch*. Isso pode ser feito através da definição de uma função chamada `collate_fn` (algo como função de combinação), onde nela podemos usar a função `pad_sequence` para conseguir construir um *batch* de dados sem problemas. Aqui, iremos construir um tensor que já tem o tamanho máximo, predefinido anteriormente no código, descartando a necessidade de termos uma `collate_fn`.\n",
    "\n",
    "- Por ora não iremos nos preocupar com a separação entre conjuntos de treino, validação e teste. O objetivo desse notebook é de ensinar como trabalhamos em cenários `seq2seq`, além de como utilizar modelos mais complexos como GRUs e LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f0b0b",
   "metadata": {},
   "source": [
    "1. Implemente as funções `sentence2tensor` e `tensor2sentence` no Dataset. **Dica:** Lembre-se que as sequências de entrada podem ter tamanhos diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2R7YKXg7IU32",
   "metadata": {
    "id": "2R7YKXg7IU32"
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pairs = build_pairs()\n",
    "        self.pairs = list(filter(filter_pair, pairs))\n",
    "        self.input_language, self.output_language = build_languages(self.pairs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sentence, output_sentence = self.pairs[idx]\n",
    "        input_tensor = self.sentence2tensor(input_sentence, is_input=True)\n",
    "        output_tensor = self.sentence2tensor(output_sentence, is_input=False)\n",
    "\n",
    "        return input_tensor, output_tensor\n",
    "\n",
    "    def sentence2tensor(self, sentence, is_input=True):\n",
    "        # Implemente a sua solução aqui\n",
    "\n",
    "    def tensor2sentence(self, tensor, is_input=True):\n",
    "        # Implemente a sua solução aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t5RXI6I4IU32",
   "metadata": {
    "id": "t5RXI6I4IU32"
   },
   "source": [
    "Exemplo de uma amostra do `TranslationDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bySogsOtIU32",
   "metadata": {
    "id": "bySogsOtIU32",
    "outputId": "f3374a82-85be-4514-8207-f0e8c65c3faf"
   },
   "outputs": [],
   "source": [
    "dataset = TranslationDataset()\n",
    "input_tensor, output_tensor = dataset[42]\n",
    "\n",
    "print('Tensor de entrada:', input_tensor)\n",
    "print('Tensor de saída  :', output_tensor)\n",
    "\n",
    "print('\\nFrase de entrada:', dataset.tensor2sentence(input_tensor, is_input=True))\n",
    "print('Frase de saída  :', dataset.tensor2sentence(output_tensor, is_input=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFzFIwxAIU32",
   "metadata": {
    "id": "oFzFIwxAIU32"
   },
   "source": [
    "Exemplo de uma amostra do nosso `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9463DZPIU32",
   "metadata": {
    "id": "b9463DZPIU32",
    "outputId": "a8dadb44-4f2c-4742-8afb-ccc91f38f78b"
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "input_tensor, output_tensor = next(iter(dataloader))\n",
    "print('Tamanho da entrada e saída:', (input_tensor.shape, output_tensor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5soaAqZ-IU32",
   "metadata": {
    "id": "5soaAqZ-IU32",
    "outputId": "aca46bd1-ff97-40f3-b9a4-185b6d4d811b"
   },
   "outputs": [],
   "source": [
    "first_input = input_tensor[0]\n",
    "first_output = output_tensor[0]\n",
    "\n",
    "print('Tensor de entrada:', first_input)\n",
    "print('Tensor de saída  :', first_output)\n",
    "\n",
    "print('\\nFrase de entrada:', dataset.tensor2sentence(first_input, is_input=True))\n",
    "print('Frase de saída  :', dataset.tensor2sentence(first_output, is_input=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ni5XNgyQIU32",
   "metadata": {
    "id": "ni5XNgyQIU32"
   },
   "source": [
    "## Modelo *seq2seq*\n",
    "\n",
    "Uma rede *sequence to sequence*, *seq2seq*, ou rede *encoder-decoder*, é um modelo que é composto por 2 RNNs, denominadas de *encoder* e *decoder*. O papel do *encoder* é de ler a sequência de entrada e gerar um **contexto** da entrada (última camada de *hidden state*); enquanto que o *decoder* utilizará esse **contexto** para produzir uma sequência de saída. Esse tipo de modelagem permite com que modelos *seq2seq* produza uma sequência de tamanho diferente da sequência de entrada, sendo ideal para traduções por exemplo.\n",
    "\n",
    "![](https://github.com/ThiagoPoppe/ciag2024/blob/main/imagens/seq2seq.png?raw=true)\n",
    "\n",
    "Além disso, por produzir um vetor de **contexto**, o modelo consegue lidar com problemas relacionado à ordem das palavras em diferentes línguas. Considere por exemplo a sentença: *Je ne suis pas le chat noir*, traduzida para *I am not the black cat*. Perceba que boa parte das palavras da entrada possuem uma tradução 1:1 com as palavras da saída, mas em uma ordem um pouco diferente, como no caso de *chat noir* ser traduzido para *black cat*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074377d",
   "metadata": {},
   "source": [
    "2. Implemente o modelo `Encoder`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZJf4ebgwIU33",
   "metadata": {
    "id": "ZJf4ebgwIU33"
   },
   "outputs": [],
   "source": [
    "# Implemente a sua solução aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Se2iMOT7IU33",
   "metadata": {
    "id": "Se2iMOT7IU33"
   },
   "source": [
    "> **Notas:** É muito comum observarmos uma camada de *dropout* depois da camada de *embedding* na rede *encoder*. Aqui, o *dropout* atua no sentido de desconsiderar alguns *tokens* da entrada da rede, aumentando a capacidade da rede de aprender a traduzir uma frase mesmo com *tokens* faltantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dbf993",
   "metadata": {},
   "source": [
    "3. Implemente o modelo `Decoder`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wvPRkMSSIU33",
   "metadata": {
    "id": "wvPRkMSSIU33"
   },
   "outputs": [],
   "source": [
    "# Implemente a sua solução aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_MI3pI5gIU33",
   "metadata": {
    "id": "_MI3pI5gIU33"
   },
   "source": [
    "Verificando o comportamento da rede com um *batch* aleatório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GcWVO2TpIU33",
   "metadata": {
    "id": "GcWVO2TpIU33",
    "outputId": "05a6fd08-21ba-47d0-98a1-1fb886dde72e"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(len(input_language), embed_size=32, hidden_size=64).to(device)\n",
    "decoder = Decoder(len(output_language), embed_size=32, hidden_size=64).to(device)\n",
    "\n",
    "x, y = next(iter(dataloader))\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "context = encoder(x)\n",
    "outputs = decoder(context, y)\n",
    "predictions = outputs.argmax(dim=-1)\n",
    "\n",
    "print('Frase original   :', dataset.tensor2sentence(x[0], is_input=True))\n",
    "print('Tradução original:', dataset.tensor2sentence(y[0], is_input=False))\n",
    "print('Tradução da rede :', dataset.tensor2sentence(predictions[0], is_input=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p8q4JhN-IU33",
   "metadata": {
    "id": "p8q4JhN-IU33"
   },
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f0a51",
   "metadata": {},
   "source": [
    "4. Implemente a função `train_epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cokLowfIU33",
   "metadata": {
    "id": "9cokLowfIU33"
   },
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    # Implemente a sua solução aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9KqEBDRpIU33",
   "metadata": {
    "id": "9KqEBDRpIU33",
    "outputId": "ecc650c5-33c3-4ac6-b0dc-8019ccbd4458"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "print_every = 5\n",
    "save_loss_every = 5\n",
    "\n",
    "dataset = TranslationDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "encoder = Encoder(len(dataset.input_language), embed_size=100, hidden_size=128).to(device)\n",
    "decoder = Decoder(len(dataset.output_language), embed_size=100, hidden_size=128).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_INDEX)  # iremos ignorar o gradiente vindo dos tokens <pad>\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "losses = []\n",
    "save_loss = 0\n",
    "print_loss = 0\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train_epoch(dataloader, encoder, decoder,\n",
    "                       encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "    save_loss += loss\n",
    "    print_loss += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss /= print_every\n",
    "        elapsed_time = time() - start_time\n",
    "\n",
    "        print(f'Epoch [{epoch}/{num_epochs}] => average loss: {print_loss:.5f}, ' \\\n",
    "              f'elapsed time: {elapsed_time:.2f} seconds')\n",
    "        print_loss = 0\n",
    "\n",
    "    if epoch % save_loss_every == 0:\n",
    "        save_loss /= save_loss_every\n",
    "        losses.append(save_loss)\n",
    "        save_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dwS49aJYIU33",
   "metadata": {
    "id": "dwS49aJYIU33"
   },
   "source": [
    "Plotando o gráfico da função de perda ao longo das épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edn4EUXeIU33",
   "metadata": {
    "id": "edn4EUXeIU33",
    "outputId": "b2a6d870-ea68-4b9c-bb7b-805b6fcf60dd"
   },
   "outputs": [],
   "source": [
    "plt.title('Evolução da função de perda ao longo das épocas')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Função de perda')\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xticks(range(len(losses)), range(save_loss_every, num_epochs+1, save_loss_every))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8imtknHNIU33",
   "metadata": {
    "id": "8imtknHNIU33"
   },
   "source": [
    "## Avaliação qualitativa dos resultados\n",
    "\n",
    "O método de *evaluation* da rede é similar ao treinamento. Porém, dessa vez nós não iremos aplicar *teacher forcing*, uma vez que não temos a palavra-alvo para o tempo $t$. Sendo assim, a predição do tempo $t$ será dada pela **previsão** da rede *decoder* no tempo $t-1$. Além disso, caso a rede preveja um *token* `<eos>`, iremos parar a geração antes de gerar uma sentença com o tamanho máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UU9Q6ekTIU37",
   "metadata": {
    "id": "UU9Q6ekTIU37"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence_tensor, encoder, decoder):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sentence_tensor = sentence_tensor.to(device)\n",
    "        context = encoder(sentence_tensor)\n",
    "\n",
    "        predictions = []\n",
    "        decoder_hidden = context\n",
    "        decoder_input = torch.tensor([SOS_INDEX], dtype=torch.long, device=device)\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            output, decoder_hidden  = decoder.forward_step(decoder_input, decoder_hidden)\n",
    "\n",
    "            prediction = output.argmax(dim=-1)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "            if prediction.item() == EOS_INDEX:\n",
    "                break\n",
    "\n",
    "            decoder_input = prediction.detach()  # precisamos dar um detach para evitar loops no grafo\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8VheGVt0IU37",
   "metadata": {
    "id": "8VheGVt0IU37",
    "outputId": "5de44b58-2e80-4694-cbc4-59255dab0d66"
   },
   "outputs": [],
   "source": [
    "sentence, label = pairs[0]\n",
    "\n",
    "sentence_tensor = dataset.sentence2tensor(sentence, is_input=True)\n",
    "sentence_tensor = sentence_tensor.to(device)\n",
    "\n",
    "predictions = evaluate(sentence_tensor, encoder, decoder)\n",
    "print(f'> {sentence}')\n",
    "print(f'= {label}')\n",
    "print(f'< {dataset.tensor2sentence(predictions, is_input=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2hGkS1q6IU38",
   "metadata": {
    "id": "2hGkS1q6IU38",
    "outputId": "03d19f9d-cd45-492c-9619-d34485331f2a"
   },
   "outputs": [],
   "source": [
    "sentence, label = pairs[10]\n",
    "\n",
    "sentence_tensor = dataset.sentence2tensor(sentence, is_input=True)\n",
    "sentence_tensor = sentence_tensor.to(device)\n",
    "\n",
    "predictions = evaluate(sentence_tensor, encoder, decoder)\n",
    "print(f'> {sentence}')\n",
    "print(f'= {label}')\n",
    "print(f'< {dataset.tensor2sentence(predictions, is_input=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D7DSVdBAIU38",
   "metadata": {
    "id": "D7DSVdBAIU38",
    "outputId": "10301278-3503-4196-d721-33bc0e1fb12a"
   },
   "outputs": [],
   "source": [
    "sentence, label = pairs[42]\n",
    "\n",
    "sentence_tensor = dataset.sentence2tensor(sentence, is_input=True)\n",
    "sentence_tensor = sentence_tensor.to(device)\n",
    "\n",
    "predictions = evaluate(sentence_tensor, encoder, decoder)\n",
    "print(f'> {sentence}')\n",
    "print(f'= {label}')\n",
    "print(f'< {dataset.tensor2sentence(predictions, is_input=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2XMbM_lyIU38",
   "metadata": {
    "id": "2XMbM_lyIU38",
    "outputId": "52420423-d959-4e7e-e354-27e52f86d3e4"
   },
   "outputs": [],
   "source": [
    "sentence = 'je vais me baigner'\n",
    "\n",
    "sentence_tensor = dataset.sentence2tensor(sentence, is_input=True)\n",
    "sentence_tensor = sentence_tensor.to(device)\n",
    "\n",
    "predictions = evaluate(sentence_tensor, encoder, decoder)\n",
    "print(f'> {sentence}')\n",
    "print(f'< {dataset.tensor2sentence(predictions, is_input=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cOdFjQaPIU38",
   "metadata": {
    "id": "cOdFjQaPIU38",
    "outputId": "2065f22c-0b57-488f-fc11-88bdcb6e8f64"
   },
   "outputs": [],
   "source": [
    "sentence = 'tu as la confiance de tous'\n",
    "\n",
    "sentence_tensor = dataset.sentence2tensor(sentence, is_input=True)\n",
    "sentence_tensor = sentence_tensor.to(device)\n",
    "\n",
    "predictions = evaluate(sentence_tensor, encoder, decoder)\n",
    "print(f'> {sentence}')\n",
    "print(f'< {dataset.tensor2sentence(predictions, is_input=False)}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
