{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6f71a2-22b9-4495-87a1-c90c3fa89c64",
   "metadata": {
    "id": "4a6f71a2-22b9-4495-87a1-c90c3fa89c64"
   },
   "source": [
    "# Sequências - Aula Prática\n",
    "## Word2Vec\n",
    "\n",
    "Neste notebook iremos trabalhar um pouco com a parte introdutória de modelos de linguagem, implementando um modelo `Word2Vec`, que utiliza a arquitetura *Constant Bag-of-Words* (CBOW), em uma base de dados simples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f4a16-9eb8-43dd-a250-43663898eaaa",
   "metadata": {},
   "source": [
    "- **Importante:** caso esteja rodando esse notebook no ambiente da Tatu, favor executar a seguinte célula. Caso contrário, basta ignorar a sua execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec14b7b-74b8-44d7-85dd-96b4b3879576",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nbproxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5b060-73b1-4e0d-9089-c8fb743e1d66",
   "metadata": {
    "id": "4cb5b060-73b1-4e0d-9089-c8fb743e1d66"
   },
   "source": [
    "## Importação de pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-eNFgRayffAA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4893,
     "status": "ok",
     "timestamp": 1734358288921,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "-eNFgRayffAA",
    "outputId": "9a9e0437-9956-4f3a-c508-c2b2d4090551"
   },
   "outputs": [],
   "source": [
    "!pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165d22a-0759-464d-bbfc-da5536140c3b",
   "metadata": {
    "executionInfo": {
     "elapsed": 4359,
     "status": "ok",
     "timestamp": 1734358293278,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "7165d22a-0759-464d-bbfc-da5536140c3b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from conllu import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f539a8e7-4164-4a90-a5a6-6b431cc368d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1734358293279,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "f539a8e7-4164-4a90-a5a6-6b431cc368d0",
    "outputId": "a302f8c9-b1d6-409c-c76a-e720d9a14ce6"
   },
   "outputs": [],
   "source": [
    "# Definindo se o código será executado na CPU ou na GPU\n",
    "has_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if has_cuda else 'cpu')\n",
    "\n",
    "print('O código será executado em:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd96627-d0b2-4488-b013-7debf3f8690a",
   "metadata": {
    "id": "7cd96627-d0b2-4488-b013-7debf3f8690a"
   },
   "source": [
    "A representação de palavras ou termos por *embeddings* é um dos conceitos mais fundamentais de Deep Learning em Processamento de Linguagem Naturais. O [*word2vec*](https://arxiv.org/pdf/1301.3781.pdf), proposto por Tomas Mikolov et al. na Google em 2013, foi um dos modelos iniciais utilizados para se aprender esse tipo de representação. Apesar de já ser considerado antigo, os conceitos desenvolvidos nas primeiras publicações ainda são úteis para o desenvolvimento de modelos mais avançados.\n",
    "\n",
    "A ideia central do *word2vec* é a de que o significado de uma palavra está diretamente relacionado às palavras ao redor da mesma, ou seja, seu **contexto**. Por exemplo, podemos imaginar que as palavras que se encaixam em uma frase do tipo `hoje eu comi ___ no café da manhã` tenham uma certa proximidade de significado em alguns aspectos, e portanto possuam um grau de similaridade entre seus *embeddings*.\n",
    "\n",
    "O artigo original propõe duas arquiteturas distintas para isso: **CBOW** (Continuous Bag-of-Words) e **Skip-Gram**, sendo a imagem a seguir uma representação do esquema por trás dessas arquiteturas.\n",
    "\n",
    "<img width=600 src=\"https://github.com/ThiagoPoppe/ciag2024/blob/main/imagens/cbow_and_skipgram_scheme.png?raw=true\">\n",
    "\n",
    "Pelo esquema acima conseguimos perceber que a arquitetura CBOW recebe como entrada o conjunto de termos que formam o contexto, representados por $w(t-2) \\dots w(t+2)$, e tenta prever o termo central da sequência, no caso $w(t)$. Por outro lado, Skip-Gram tenta prever os termos do contexto com base na palavra central.\n",
    "\n",
    "> Apesar dessa diferença, o objetivo em ambos os casos é gerar os *embeddings* dos termos. Portanto o que importa para nós no final são as representações aprendidas pelo modelo durante o treinamento e armazenados como parâmetros da rede."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe6096-ad13-4753-9378-35f5f0178f4b",
   "metadata": {
    "id": "1cbe6096-ad13-4753-9378-35f5f0178f4b"
   },
   "source": [
    "### Implementação de um Word2Vec (CBOW)\n",
    "\n",
    "Começaremos buscando um conjunto de dados e gerando um vocabulário a partir disso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VqaYGfKFgGQR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1108,
     "status": "ok",
     "timestamp": 1734358299222,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "VqaYGfKFgGQR",
    "outputId": "6c8219c1-4648-42f6-ac8b-1dd1336cb9c8"
   },
   "outputs": [],
   "source": [
    "with open(\"/pgeoprj2/ciag2024/dados/Petro1.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "text = parse(data)\n",
    "\n",
    "sentences = []\n",
    "for sentence in text:\n",
    "    t = []\n",
    "    last = -1\n",
    "    for token in sentence:\n",
    "        if type(token['id']) is tuple:\n",
    "            last = token['id'][-1]\n",
    "        if token['form'] and token['upostag'] != 'PUNCT' and (type(token['id']) is tuple or token['id'] > last):\n",
    "            t.append(token[\"form\"])\n",
    "    sentences.append(\" \".join(t))\n",
    "\n",
    "for i in range(1, 10):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2553e13-ac8b-4498-87f7-bd97c9177de1",
   "metadata": {
    "id": "c2553e13-ac8b-4498-87f7-bd97c9177de1"
   },
   "source": [
    "Para definir o nosso vocabulário, primeiro iremos realizar um processo de \"tokenização\" do texto definido anteriormente, onde, por simplicidade, definimos que o nosso *token* será igual à uma palavra completa. Após isso, o nosso vocabulário será definido como o conjunto de *tokens* únicos, podendo ser obtidos a partir de um `set`.\n",
    "\n",
    "> **Importante**: O *pipeline* de processamento de dados textuais pode conter diversas outras etapas além da \"tokenização\", como por exemplo a remoção de *stop words*, ou seja, palavras que são muito comuns na linguagem e que não carregam muita informação útil. Porém, para manter a simplicidade do notebook, iremos apenas realizar a etapa de \"tokenização\" do texto. Para mais informações sobre *stop words* consulte o seguinte [link](https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a), assim como esse outro [link](https://medium.com/analytics-vidhya/nlp-preprocessing-pipeline-what-when-why-2fc808899d1f) para um detalhamento maior do *pipeline* de processamento de dados em NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZX0XxZJUJikZ",
   "metadata": {
    "id": "ZX0XxZJUJikZ"
   },
   "source": [
    "1. Implemente a função `build_vocabulary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ca7b1-3cb7-4727-aa54-b1101b4a8c32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1734358299222,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "520ca7b1-3cb7-4727-aa54-b1101b4a8c32",
    "outputId": "d45d6fb2-e88a-4686-a744-6ebac0e0d044"
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(sentences: list):\n",
    "    # Implemente a sua solução aqui\n",
    "\n",
    "tokenized_lines, vocabulary = build_vocabulary(sentences)\n",
    "print(f'Temos um total de {len(vocabulary)} tokens no nosso vocabulário')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64acd2b-274e-4b36-9780-3d6dfac9c67b",
   "metadata": {
    "id": "a64acd2b-274e-4b36-9780-3d6dfac9c67b"
   },
   "source": [
    "Podemos então definir dicionários para mapear um índice numérico para cada termo do vocabulário, assim como retornar o termo dado um índice.\n",
    "\n",
    "> **Importante**: Não confundir esse índice numérico com a posição da palavra no texto original! Ele é apenas um mapeamento entre palavra e número.\n",
    "\n",
    "<img width=600 src=\"https://github.com/ThiagoPoppe/ciag2024/blob/main/imagens/vocabulario.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce98f27c-289e-48a2-8b77-fa1889655dd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1734358299222,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "ce98f27c-289e-48a2-8b77-fa1889655dd5",
    "outputId": "7e7bc5d4-add4-41c4-f860-b5d9079a866c"
   },
   "outputs": [],
   "source": [
    "# Utilizaremos comprehension to Python para fazer um código one-liner\n",
    "word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "idx2word = {idx: word for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "print('Índice da palavra \"petróleo\" no dicionário:', word2idx['petróleo'])\n",
    "print('Palavra do índice 150 do dicionário:', idx2word[150])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6dbe6-679e-4653-9750-406594b18ab4",
   "metadata": {
    "id": "55d6dbe6-679e-4653-9750-406594b18ab4"
   },
   "source": [
    "Com isso, conseguimos definir uma função que cria tensores para um contexto contendo múltiplos termos do nosso vocabulário!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m_XhIcLKJtwN",
   "metadata": {
    "id": "m_XhIcLKJtwN"
   },
   "source": [
    "2. Implemente a função `make_context_tensor`. Ela recebe uma string de contexto e deve retornar um tensor que representa o contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974b5c1-a0b6-49a1-b609-2e9e343a5683",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1734358299222,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "a974b5c1-a0b6-49a1-b609-2e9e343a5683",
    "outputId": "ef2781ec-16d1-4f42-cc36-b9ce429513f5"
   },
   "outputs": [],
   "source": [
    "def make_context_tensor(context: str, word2idx: dict[str, int]):\n",
    "    # Implemente a sua solução aqui\n",
    "    \n",
    "context = 'qualidade dos produtos petrolíferos'\n",
    "make_context_tensor(context, word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205193f3-6a33-41b6-8bf0-3a6c9edb3efc",
   "metadata": {
    "id": "205193f3-6a33-41b6-8bf0-3a6c9edb3efc"
   },
   "source": [
    "#### Definição de uma base de dados simples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab426aa-8221-4804-91c4-a9876b62c24f",
   "metadata": {
    "id": "4ab426aa-8221-4804-91c4-a9876b62c24f"
   },
   "source": [
    "A partir do que foi desenvolvido até então, conseguimos definir uma base de dados simples composta por pares `(context, target)` para treinar o nosso modelo *word2vec*. Iremos percorrer o texto completo, tomando termo a termo como o alvo e as palavras ao redor como seu contexto.\n",
    "\n",
    "> Para o exemplo dessa aula, utilizaremos uma janela de 2 termos em cada direção para delimitar o contexto. É recomendável alterar essa janela e verificar a qualidade final do nosso modelo. Além disso, não estamos preocupados aqui em criar um conjunto de treino, validação e teste, mas sim de apenas entender o funcionamento de modelos *word2vec*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121b288-f8e1-4a83-b703-393e95244f09",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1734358299222,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "0121b288-f8e1-4a83-b703-393e95244f09"
   },
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, sentences: list, context_size: int = 2):\n",
    "        tokenized_lines, vocabulary = build_vocabulary(sentences)\n",
    "\n",
    "        # Definindo dicionários para mapear palavra -> número e vice-versa\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "        self.idx2word = {idx: word for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "        self.data = []\n",
    "        for tokens in tokenized_lines:\n",
    "            for i in range(context_size, len(tokens) - context_size):\n",
    "                left_words = [tokens[i-j] for j in range(context_size, 0, -1)]\n",
    "                right_words = [tokens[i+j] for j in range(1, context_size + 1)]\n",
    "\n",
    "                target = tokens[i]\n",
    "                context = left_words + right_words\n",
    "                context = ' '.join(context)  # convertendo de lista para string\n",
    "\n",
    "                self.data.append((context, target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        context, target = self.data[idx]\n",
    "        context_tensor = make_context_tensor(context, self.word2idx)\n",
    "        target_tensor = torch.tensor(self.word2idx[target], dtype=torch.long)\n",
    "\n",
    "        return context_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c67e743-24fd-4c46-a7a0-3bad0f5e2d7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1734358299222,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "7c67e743-24fd-4c46-a7a0-3bad0f5e2d7b",
    "outputId": "13f2a5af-8ae7-45ad-92d9-ae2538414525"
   },
   "outputs": [],
   "source": [
    "dataset = EmbeddingsDataset(sentences, context_size=2)\n",
    "print('Primeiro exemplo de treino:', dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85caf644-e113-43f5-97a9-bfe9dce10eb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1734358299222,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "85caf644-e113-43f5-97a9-bfe9dce10eb9",
    "outputId": "eefbad70-1491-4029-a595-c8c7b0422d0b"
   },
   "outputs": [],
   "source": [
    "# Podemos usar o dicionário idx2word para ver qual que é o contexto e palavra alvo\n",
    "context, target = dataset[0]\n",
    "\n",
    "context = [dataset.idx2word[idx.item()] for idx in context]\n",
    "context = ' '.join(context)\n",
    "\n",
    "target = dataset.idx2word[target.item()]\n",
    "\n",
    "print('Primeiro exemplo de treino:', (context, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ccde5-4d53-4fa2-bd08-87850af92168",
   "metadata": {
    "id": "d27ccde5-4d53-4fa2-bd08-87850af92168"
   },
   "source": [
    "#### Definição da arquitetura\n",
    "\n",
    "A nossa arquitetura será composta por uma camada `nn.Embedding`, na qual aprenderemos as representações dos termos, seguida de algumas camadas lineares, com o propósito de realizar a previsão da palavra alvo.\n",
    "\n",
    "> A camada `nn.Embedding` é similar à uma camada `nn.Linear`, sem o termo de viés, no sentido de também ser definida como uma matriz. Porém, diferentemente da `nn.Linear`, nós não realizamos um produto matricial com a matriz produzida pela camada de *embedding*, mas sim realizamos uma operação de *lookup* na matriz, como se ela fosse uma tabela, como observado na imagem a seguir.\n",
    "\n",
    "<img width=600 src=\"https://github.com/ThiagoPoppe/ciag2024/blob/main/imagens/lookup_embedding.png?raw=true\">\n",
    "\n",
    "**Importante:** Note que na imagem acima o produto matricial do vetor de entrada com a matriz de *embeddings* dará o mesmo resultado, afinal um vetor de entrada *one-hot encoded* irá efetivamente selecionar uma linha, ou coluna, da nossa matriz. Porém, na prática ainda é preferível a utilização da camada `nn.Embedding` ao invés de uma camada `nn.Linear` com o viés desabilitado por questões de eficiência. Para mais detalhes, consulte o seguinte [link](https://medium.com/@gautam.e/what-is-nn-embedding-really-de038baadd24)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe59ce-26a1-41cf-a66f-1b22de2fc335",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1734358299222,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "abbe59ce-26a1-41cf-a66f-1b22de2fc335",
    "outputId": "992f2c59-9133-492f-8f9e-2bb71d5ce31f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([0, 1, 2])\n",
    "embedding = nn.Embedding(10, 100)  # 10 palavras com 100 de conceito\n",
    "\n",
    "embedding(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_51QNjPvK2WN",
   "metadata": {
    "id": "_51QNjPvK2WN"
   },
   "source": [
    "3. Implemente a arquitetura `CBOW`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9b2ec-1d97-47c1-a313-4f24fcabc18d",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1734358299222,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "1fc9b2ec-1d97-47c1-a313-4f24fcabc18d"
   },
   "outputs": [],
   "source": [
    "# Implemente a sua solução aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925495b-eb66-4302-a1bc-9c63c7271b2a",
   "metadata": {
    "id": "9925495b-eb66-4302-a1bc-9c63c7271b2a"
   },
   "source": [
    "#### Treinamento do modelo\n",
    "\n",
    "Como estamos utilizando uma camada de `LogSoftmax` no final da nossa rede, temos que utilizar a função de perda `NLLLoss` ao invés da `CrossEntropyLoss`, já que a primeira espera que a saída do modelo sejam **log-probabilidades** ao invés de **logits** (saídas não ativadas), como esperado pela segunda função de perda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dSTb1ePvLLh4",
   "metadata": {
    "id": "dSTb1ePvLLh4"
   },
   "source": [
    "4. Treine o modelo `CBOW` com o `EmbeddingsDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af9f495-fa14-4684-82f4-badb7f89cc5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541634,
     "status": "ok",
     "timestamp": 1734358840854,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "3af9f495-fa14-4684-82f4-badb7f89cc5b",
    "outputId": "d49d384d-8ab1-494b-fd3f-f0c692df62a2"
   },
   "outputs": [],
   "source": [
    "# Implemente a sua solução aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb70ff-6666-4574-9309-02ab624859da",
   "metadata": {
    "id": "acbb70ff-6666-4574-9309-02ab624859da"
   },
   "source": [
    "Agora podemos testar a rede, fornecendo um contexto e verificando qual palavra o modelo prevê como termo central!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60880e5c-2a56-4c1a-88d0-8a057d822e3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1734351958238,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "60880e5c-2a56-4c1a-88d0-8a057d822e3e",
    "outputId": "05b8b250-3ecc-4845-b9f8-3ce221073c4d"
   },
   "outputs": [],
   "source": [
    "context = 'metade da de hidrogênio'\n",
    "context_vector = make_context_tensor(context, dataset.word2idx)\n",
    "\n",
    "# Convertendo para executar no dispositivo correto e adicionando batch size\n",
    "context_vector = context_vector.to(device)\n",
    "context_vector = context_vector.unsqueeze(dim=0)\n",
    "\n",
    "outputs = model(context_vector)\n",
    "print(f'Contexto:', context)\n",
    "print(f'Predição:', dataset.idx2word[torch.argmax(outputs[0]).item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713849b6-b536-4cf6-be22-40a76981fcb8",
   "metadata": {
    "id": "713849b6-b536-4cf6-be22-40a76981fcb8"
   },
   "source": [
    "- Além disso conseguimos visualizar o *embedding* gerado para a predição do modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166387db-fead-4d6c-a323-c9fc51465606",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1734352062250,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "166387db-fead-4d6c-a323-c9fc51465606",
    "outputId": "9680c435-009a-42d9-9f41-e411bdd6d609"
   },
   "outputs": [],
   "source": [
    "word = torch.LongTensor([torch.argmax(outputs[0])]).to(device)\n",
    "embedded_word = model.embedding(word)\n",
    "\n",
    "plt.matshow(embedded_word.detach().cpu().numpy(), aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b10b1-2cf5-4418-8200-550ec0e0ae1c",
   "metadata": {
    "id": "bf4b10b1-2cf5-4418-8200-550ec0e0ae1c"
   },
   "source": [
    "- Também podemos realizar uma visualização do espaço latente produzido pela camada de *embedding* do nosso `word2vec`. Para isso, realizaremos um TSNE para recuperar os 2 componentes principais do vetor de *embedding* e visualizar esse espaço em 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8bf43-0dc2-4886-bf97-f7c289bf2a71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1009,
     "status": "ok",
     "timestamp": 1734358841843,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "79d8bf43-0dc2-4886-bf97-f7c289bf2a71",
    "outputId": "49772bcd-82e6-4fc1-ab68-462c9658f0a5"
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "for token in vocabulary:\n",
    "    word = torch.LongTensor([dataset.word2idx[token]]).to(device)\n",
    "    embedded_word = model.embedding(word)\n",
    "    embedded_word = embedded_word.squeeze()  # removendo dimensões unitárias\n",
    "    embedded_word = embedded_word.detach().cpu().numpy()  # convertendo para numpy na cpu\n",
    "\n",
    "    words.append(embedded_word)\n",
    "\n",
    "words = np.array(words)\n",
    "print('Dimensão dos dados:', words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce674c-bd7f-4908-a80f-20171e835e5e",
   "metadata": {
    "executionInfo": {
     "elapsed": 3051,
     "status": "ok",
     "timestamp": 1734359206194,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "9bce674c-bd7f-4908-a80f-20171e835e5e"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "components = tsne.fit_transform(words[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be435ac-c5fb-4aa8-a571-34b0e3e37f34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827
    },
    "executionInfo": {
     "elapsed": 5046,
     "status": "ok",
     "timestamp": 1734359221151,
     "user": {
      "displayName": "Manoela Werneck",
      "userId": "16662601149123035023"
     },
     "user_tz": 180
    },
    "id": "5be435ac-c5fb-4aa8-a571-34b0e3e37f34",
    "outputId": "8b104d9d-11af-4d8f-8b38-96b25a7f4d3b"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 16))\n",
    "ax.scatter(components[:, 0], components[:, 1])\n",
    "\n",
    "for i, token in enumerate(vocabulary):\n",
    "    if i > 399:\n",
    "        break\n",
    "    ax.annotate(token, (components[i, 0], components[i, 1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
