{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d726cc",
   "metadata": {
    "id": "65d726cc"
   },
   "source": [
    "# Sequências - Aula Prática\n",
    "## Embeddings\n",
    "\n",
    "Neste notebook iremos trabalhar um pouco com a parte introdutória de modelos de linguagem, realizando **analogias** a partir de representações distribuídas (*embeddings*) geradas pelo algoritmo de aprendizagem não-supervisionada `GloVe` (*Global Vectors for Word Representation*).\n",
    "> Para saber mais sobre como o `GloVe` funciona, recomendo a leitura do seguinte [notebook](https://colab.research.google.com/github/jaygala24/pytorch-implementations/blob/master/Global%20Vectors%20for%20Word%20Representation.ipynb#scrollTo=oEXgG-hDIMdT). Esse notebook prático não tem como foco explicar como que o algoritmo funciona, mas sim explicar as operações que podemos fazer com as representações distribuídas geradas por algoritmos de *word embedding*.\n",
    "\n",
    "- Este notebook foi inspirado pelos trabalhos disponibilizados nos sites [d2l.ai](https://d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html) e [notebook.community](https://notebook.community/spro/practical-pytorch/glove-word-vectors/glove-word-vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071c310f",
   "metadata": {
    "id": "071c310f"
   },
   "source": [
    "## Importação de pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a1662",
   "metadata": {
    "id": "2f2a1662"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "from os.path import join as ospj\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "sns.set_style('dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f20f8-663a-4c74-9987-a0fab09e6063",
   "metadata": {
    "id": "fc1f20f8-663a-4c74-9987-a0fab09e6063"
   },
   "source": [
    "## Analogias\n",
    "\n",
    "Analogias são associações de mesma natureza entre palavras (como flexões de gênero ou número). A geometria dessas associações pode ser visualizada no espaço vetorial onde as palavras são projetadas e, em modelos bem treinados, deve ser possível encontrar semelhanças entre associações de mesma natureza.\n",
    "\n",
    "<img width=800 src=\"https://github.com/ThiagoPoppe/ciag2024/blob/main/imagens/analogias.png?raw=true\"/>\n",
    "\n",
    "Ao longo deste notebook, utilizaremos o modelo `GloVe` para realizar a projeção de palavras em um espaço vetorial. Mais especificamente, iremos carregar um modelo pré-treinado em uma base de dados de 6 bilhões de tokens, realizando uma projeção das palavras em um espaço 100 dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a52893-8ac8-4b61-ad19-961e89e32779",
   "metadata": {
    "id": "29a52893-8ac8-4b61-ad19-961e89e32779"
   },
   "source": [
    "### Carregamento de um modelo GloVe pré-treinado\n",
    "\n",
    "Iremos fazer o download de um modelo `GloVe` pré-treinado. A classe `GloVe` serve para representar o modelo.\n",
    "\n",
    "> O modelo possui alguns atributos, como o dicionário `stoi` (*string* to *int*) para mapear uma palavra para um índice numérico e a lista `itos` (*int* to *string*), mapeando um índice númerico para uma palavra. Além disso, conseguimos acessar a matriz de *embeddings* através do atributo `vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J-VU1kR7FuXo",
   "metadata": {
    "id": "J-VU1kR7FuXo"
   },
   "outputs": [],
   "source": [
    "class GloVe:\n",
    "    def __init__(self):\n",
    "        self.itos = []\n",
    "        self.stoi = {}\n",
    "\n",
    "        vectors = []\n",
    "\n",
    "        with open('/pgeoprj2/ciag2024/dados/glove.6B.100d.txt', encoding=\"utf8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                word, *vector = line.split()\n",
    "                vectors.append(np.array(vector, dtype=np.float32)[:100])\n",
    "                self.itos.append(word)\n",
    "                self.stoi[word] = i\n",
    "\n",
    "        self.vectors = torch.tensor(np.array(vectors))\n",
    "\n",
    "    def get_vecs_by_tokens(self, tokens):\n",
    "        embeddings = []\n",
    "        to_reduce = False\n",
    "\n",
    "        if not isinstance(tokens, list):\n",
    "            tokens = [tokens]\n",
    "            to_reduce = True\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.stoi:\n",
    "                idx = self.stoi[token]\n",
    "                embeddings.append(self.vectors[idx])\n",
    "            else:\n",
    "                embeddings.append(torch.zeros(100))\n",
    "\n",
    "        vecs = torch.stack(embeddings)\n",
    "        return vecs[0] if to_reduce else vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45da18-cb6a-4d15-bf1b-1496aa7e9a58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f45da18-cb6a-4d15-bf1b-1496aa7e9a58",
    "outputId": "765a0a9c-cc4d-44b9-be89-702fc4817545"
   },
   "outputs": [],
   "source": [
    "glove = GloVe()\n",
    "\n",
    "random_tokens = np.random.choice(glove.itos, size=5)\n",
    "\n",
    "print('Número de tokens mapeados pelo glove.6B.100d:', len(glove.stoi))\n",
    "print('Aqui temos 5 tokens aleatórios mapeados pelo glove.6B.100d:', random_tokens)\n",
    "print('A dimensionalidade da matriz de embeddings é:', glove.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9afd1-47f4-4503-99d2-326dd9976ebd",
   "metadata": {
    "id": "f0d9afd1-47f4-4503-99d2-326dd9976ebd"
   },
   "source": [
    "### Projeções vetoriais utilizando o GloVe\n",
    "\n",
    "Podemos realizar projeções de palavras utilizando a matriz de *embeddings* computada pelo `glove.6B.100d` através do método `get_vecs_by_tokens`, como visto a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae92be-a232-4e21-acfd-7101cd416ce1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "id": "79ae92be-a232-4e21-acfd-7101cd416ce1",
    "outputId": "1fbbe419-bcfa-4549-d9ca-9d6288fd4f08"
   },
   "outputs": [],
   "source": [
    "tokens = ['man', 'king', 'woman', 'queen']\n",
    "vecs = glove.get_vecs_by_tokens(tokens)\n",
    "\n",
    "plt.matshow(vecs, aspect='auto')\n",
    "plt.yticks(range(4), tokens)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6967080c-815d-4a6f-ad66-17e2f998ac63",
   "metadata": {
    "id": "6967080c-815d-4a6f-ad66-17e2f998ac63"
   },
   "source": [
    "Percebemos que o GloVe também possui alguns *tokens* bem específicos sobre algumas áreas do conhecimento, como a palavra **maastrichtian** (geologia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488c0e0-7e5e-418b-9bb1-abbc7455a7e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1488c0e0-7e5e-418b-9bb1-abbc7455a7e7",
    "outputId": "9dbb97bf-58b6-4cfb-f0d5-a611c68b2899"
   },
   "outputs": [],
   "source": [
    "glove.get_vecs_by_tokens(['maastrichtian'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9b8afe-91d8-402e-9fdc-67b614fd4770",
   "metadata": {
    "id": "0f9b8afe-91d8-402e-9fdc-67b614fd4770"
   },
   "source": [
    "Caso o GloVe não tenha um *token* mapeado, não encontramos nenhum erro, apenas um *embedding* de zeros.\n",
    "> Talvez o *token* **pre-salt** possa aparecer em modelos maiores: como o `glove.840B.300d`, por ter sido treinado num *corpus* bem maior (840 bilhões de *tokens*) do que o que estamos utilizando nesse notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3510f-6b54-433d-b0b3-cc2371e11414",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddf3510f-6b54-433d-b0b3-cc2371e11414",
    "outputId": "5ab4480a-35ec-402e-bad9-f48f6b7efb84"
   },
   "outputs": [],
   "source": [
    "glove.get_vecs_by_tokens(['pre-salt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8e0fd7-1137-47e2-a5f9-0fe725f1277c",
   "metadata": {
    "id": "fa8e0fd7-1137-47e2-a5f9-0fe725f1277c"
   },
   "source": [
    "Além disso, podemos visualizar esses vetores no espaço latente para analisar algumas propriedades interessantes. Para tornar a visualização factível, podemos utilizar um algoritmo de redução de dimensionalidade.\n",
    "\n",
    "> Para esse exemplo iremos utilizar o algoritmo de redução de dimensionalidade PCA (*Principal Components Analysis*), devido a uma melhor interpretabilidade para o exemplo escolhido. Porém, é mais comum vermos algoritmos como o t-SNE (*t-distributed Stochastic Neighbor Embedding*), uma vez que este algoritmo preserva a estrutura local do dado original. Para mais detalhes na diferença entre t-SNE e PCA, recomendo a leitura do seguinte [link](https://medium.com/analytics-vidhya/pca-vs-t-sne-17bcd882bf3d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5aa571-789d-499d-b332-e0314e8437b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca5aa571-789d-499d-b332-e0314e8437b2",
    "outputId": "bda8d5c5-f411-4abe-deb2-174ee2bb1f08"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tokens = ['man', 'woman', 'king', 'queen', 'brother', 'sister']\n",
    "vecs = glove.get_vecs_by_tokens(tokens)\n",
    "\n",
    "# Calculando a redução de dimensionalidade para um espaço bidimensional\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(vecs.numpy())\n",
    "\n",
    "print('Tamanho dos componentes:', components.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e5224-fe43-4d2a-b443-6752b2a78a7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "2f3e5224-fe43-4d2a-b443-6752b2a78a7a",
    "outputId": "02a97244-21c4-44bd-bd68-2973f27ec5cc"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(components[:, 0], components[:, 1])\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    ax.annotate(token, (components[i, 0], components[i, 1]))\n",
    "\n",
    "for i in range(0, len(tokens) - 1, 2):\n",
    "    x = (components[i, 0], components[i+1, 0])\n",
    "    y = (components[i, 1], components[i+1, 1])\n",
    "    ax.plot(x, y, linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c15a0b-da3d-4383-953d-daa3561052b7",
   "metadata": {
    "id": "f8c15a0b-da3d-4383-953d-daa3561052b7"
   },
   "source": [
    "Note pela imagem acima como a operação vetorial: $(\\overrightarrow{\\text{king}} - \\overrightarrow{\\text{queen}}) + \\overrightarrow{\\text{sister}} \\approx \\overrightarrow{\\text{brother}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14dfc68-e90d-4223-a91a-66bd9d6cb3dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c14dfc68-e90d-4223-a91a-66bd9d6cb3dd",
    "outputId": "13b24871-12f8-4c5d-b746-200a290c827f"
   },
   "outputs": [],
   "source": [
    "king = components[tokens.index('king')]\n",
    "queen = components[tokens.index('queen')]\n",
    "sister = components[tokens.index('sister')]\n",
    "brother = components[tokens.index('brother')]\n",
    "\n",
    "result = (king - queen) + sister\n",
    "print('Vetor resultante:', result)\n",
    "print('Vetor para a palavra \"brother\":', brother)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046dde3-7b30-4ba8-8b0f-f13f28d59c82",
   "metadata": {
    "id": "4046dde3-7b30-4ba8-8b0f-f13f28d59c82"
   },
   "source": [
    "<br> Podemos refazer o plot para verificar onde a posição do vetor resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b52e1e0-32bc-4963-937b-d90d6a054227",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "2b52e1e0-32bc-4963-937b-d90d6a054227",
    "outputId": "c1368fcb-57dc-41ab-ed5a-89b995a5ae9e"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Adicionando o ponto do vetor resultante\n",
    "ax.scatter(result[0], result[1], color='red')\n",
    "ax.annotate('resultante', (result[0], result[1]))\n",
    "\n",
    "ax.scatter(components[:, 0], components[:, 1])\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    ax.annotate(token, (components[i, 0], components[i, 1]))\n",
    "\n",
    "for i in range(0, len(tokens) - 1, 2):\n",
    "    x = (components[i, 0], components[i+1, 0])\n",
    "    y = (components[i, 1], components[i+1, 1])\n",
    "    ax.plot(x, y, linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69816c-412a-4d5d-90ba-cef13b79336d",
   "metadata": {
    "id": "7e69816c-412a-4d5d-90ba-cef13b79336d"
   },
   "source": [
    "### Realizando analogias através do GloVe\n",
    "\n",
    "Através de operações vetoriais, como visto anteriormente, somos capazes de criar analogias no espaço latente da nossa representação distribuída, conseguindo responder perguntas como: `rainha está para rei assim como irmã está para ???`.\n",
    "\n",
    "Possuímos diversas estratégias para retornar a palavra correta para tal pergunta. Por exemplo, a partir da imagem da célula anterior, uma possível estratégia é de calcular o top-1 vizinho mais próximo do vetor resultante através de um algoritmo de `KNN`, porém tal estratégia pode ser menos viável em dimensões maiores devido à [maldição da dimensionalidade](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6065061). Para esse notebook, iremos utilizar uma estratégia que envolve a similaridade de cosseno entre os vetores no espaço latente, selecionando como analogia aquele vetor com maior similaridade.\n",
    "\n",
    "A similaridade de cosseno pode ser computada através da seguinte equação, onde temos uma operação de similaridade no numerador (produto interno), dividido pelo produto da norma de cada vetor. Também introduzimos um fator $\\epsilon \\approx 1\\mathrm{e}{-6}$ no denominador como um termo para evitar divisão por zero. A distância, ou similaridade, de cosseno possui imagem entre $[-1, +1]$, onde $-1$ indica que os vetores estão apontados para sentidos opostos; $+1$ indica que os vetores estão apontando para a mesma direção; e $0$ indica que os vetores são perpendiculares.\n",
    "\n",
    "$$\n",
    "\\text{similaridade} = \\frac{\\overrightarrow{x_1} \\cdot \\overrightarrow{x_2}}{\\max(\\lVert \\overrightarrow{x_1} \\rVert_2 * \\lVert \\overrightarrow{x_2} \\rVert_2, \\epsilon)}\n",
    "$$\n",
    "\n",
    "Primeiramente, iremos observar os top vetores retornados pela distância de cosseno a partir de uma *query* (palavra) fornecida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb97fd3",
   "metadata": {
    "id": "cfb97fd3"
   },
   "source": [
    "1. Implemente a função `get_similar_tokes`. Ela deve retornar um array com os tokens mais próximos de *query*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd794a-e3b1-473f-a103-8ec3551d115f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05fd794a-e3b1-473f-a103-8ec3551d115f",
    "outputId": "c89b00ce-c40d-42ec-906c-f6c1e0910999"
   },
   "outputs": [],
   "source": [
    "def get_similar_tokens(query, embedding, k=10):\n",
    "    # Implemente a sua solução aqui\n",
    "\n",
    "similar_tokens = get_similar_tokens('computer', glove, k=5)\n",
    "for i, (token, similarity) in enumerate(similar_tokens):\n",
    "    print(f'{i+1}. {token} -> {similarity:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3688da-cb4d-4df7-a6d8-a2fb409e8cac",
   "metadata": {
    "id": "0e3688da-cb4d-4df7-a6d8-a2fb409e8cac"
   },
   "source": [
    "<br> Agora iremos utilizar a distância de cosseno para computar analogias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3effa96",
   "metadata": {
    "id": "f3effa96"
   },
   "source": [
    "2. Implemente a função `get_topk_analogies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7dbbe2-899e-4026-a6c2-cab8ee93b130",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab7dbbe2-899e-4026-a6c2-cab8ee93b130",
    "outputId": "a8add346-2550-481a-917e-8cfd0538811f"
   },
   "outputs": [],
   "source": [
    "def get_topk_analogies(token1, token2, token3, embedding, k=5):\n",
    "    # Implemente a sua solução aqui\n",
    "\n",
    "get_topk_analogies('queen', 'king', 'wife', glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e070319-9ddd-49e0-86cc-93b07d48a647",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e070319-9ddd-49e0-86cc-93b07d48a647",
    "outputId": "824bb11e-da07-4421-b502-f9cb2f563b19"
   },
   "outputs": [],
   "source": [
    "get_topk_analogies('beijing', 'china', 'tokyo', glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b176d-339a-4527-9d99-1d6892e1dec0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "368b176d-339a-4527-9d99-1d6892e1dec0",
    "outputId": "5519b62d-87b2-4f24-aec6-3b9430c8e411"
   },
   "outputs": [],
   "source": [
    "get_topk_analogies('do', 'does', 'go', glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9465b647-a3f6-4c82-9fc8-d0d73c289f77",
   "metadata": {
    "id": "9465b647-a3f6-4c82-9fc8-d0d73c289f77"
   },
   "source": [
    "## *Fine-tuning* de *embeddings* pré-treinados\n",
    "\n",
    "No mundo de processamento de linguagem natural, é muito comum utilizarmos *embeddings* pré-treinados como um pontapé inicial para a nossa rede, muito por conta desses *embeddings* serem treinados em uma base de dados imensa, necessitando de muito poder computacional se quisermos treinar os nossos próprios *embeddings* do zero.\n",
    "\n",
    "Caso você queira utilizar os pesos pré-treinados do `GloVe`, ou de qualquer outro modelo pré-treinado, em uma camada `nn.Embedding` do seu modelo, você pode inicializar os pesos dessa camada através da função `.from_pretrained`, como ilustrado a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea21ce-b466-4dec-b4d0-fb235a3ee7e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dea21ce-b466-4dec-b4d0-fb235a3ee7e3",
    "outputId": "7f2828e4-584d-42dc-ef7e-5a6cf65dd60a"
   },
   "outputs": [],
   "source": [
    "my_embedding = nn.Embedding.from_pretrained(glove.vectors)\n",
    "my_embedding.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49ca35-9ee2-4889-8011-860ada4cf26e",
   "metadata": {
    "id": "cf49ca35-9ee2-4889-8011-860ada4cf26e"
   },
   "source": [
    "<br>Também temos a opção de manualmente dizer que queremos que esses pesos sofram um *fine-tuning* ao longo do treinamento.\n",
    "\n",
    "> Por padrão, os pesos do modelo pré-treinado são \"congelados\", indicando que eles não sofrerão atualizações via *backpropagation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27890af-7b34-41ec-a8a3-86a61cdb950d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c27890af-7b34-41ec-a8a3-86a61cdb950d",
    "outputId": "887d5fde-a324-4dae-e099-b28de57e7991"
   },
   "outputs": [],
   "source": [
    "my_embedding = nn.Embedding.from_pretrained(glove.vectors, freeze=False)\n",
    "my_embedding.weight"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
