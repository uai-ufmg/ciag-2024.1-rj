{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptU9jdhjCw7R"
   },
   "source": [
    "# Transformers\n",
    "\n",
    "## Componentes essenciais\n",
    "\n",
    "Neste notebook, iremos trabalhar e entender um pouco com os componentes essencias que compõem os *Transformers*. Mais especificamente, vamos retomar alguns conceitos que vimos previamente no curso (como *skip connections*) e mais recentemente em mecanismos de *atenção*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy1-fRPOCw7V"
   },
   "source": [
    "### Mecanismo de atenção *scaled dot-product*\n",
    "\n",
    "Como visto em aulas passadas, existe uma infinidade de mecanismos de atenção propostos por artigos da área de processamento de linguagem natural e afins, como: *additive attention*, *deep attention*, entre outros. Porém, todos comportilham da mesma ideia de computar similaridade entre uma *query* (consulta) e uma \"base de dados\" (*keys*), criando assim o que é chamado de *attention weights*, que serão utilizados como pesos para uma soma ponderada de certos valores (*values*).\n",
    "\n",
    "Uma forma natural de computar similaridade entre duas coisas é através do produto interno entre vetores. E é assim que a ideia de *dot-product attention* surge! Essa forma de atenção assume a seguinte fórmula, que recebe o nome de *self-attention*:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\text{Softmax}[\\mathbf{X} \\mathbf{X^T}] \\mathbf{X}\n",
    "$$\n",
    "\n",
    "Note que com essa formulação, nós não temos parâmetros aprendíveis durante a atenção, logo podemos utilizar a matriz $\\mathbf{\\tilde{X}} = \\mathbf{XU}$ como sendo a matriz que iremos computar a atenção:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\text{Softmax}[\\mathbf{\\tilde{X}} \\mathbf{\\tilde{X}^T}] \\mathbf{\\tilde{X}} = \\text{Softmax}[\\mathbf{XU} \\mathbf{U^{T}X^{T}}] \\mathbf{XU}\n",
    "$$\n",
    "\n",
    "Mesmo introduzindo essa flexibilidade através de um parâmetro aprendível, note que a matriz $\\mathbf{XU} \\mathbf{U^{T}X^{T}}$ é simétrica, algo que nem sempre queremos que seja verdade. Tome como exemplo as palavras `banana` e `fruta`. Queremos que `banana` tenha uma associação forte com a palavra `fruta` e que `fruta` tenha uma associação mais fraca com a palavra `banana`, uma vez que toda banana é uma fruta mas nem toda fruta é uma banana. Para conseguirmos tal comportamento, podemos criar uma matriz de pesos diferentes para cada componente da nossa atenção, criando assim as matrizes de *query* $(\\mathbf{Q} = \\mathbf{XW^{(q)}})$, *key* $(\\mathbf{K} = \\mathbf{XW^{(k)}})$ e *value* $(\\mathbf{V} = \\mathbf{XW^{(v)}})$.\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\text{Softmax}[\\mathbf{Q} \\mathbf{K^T}] \\mathbf{V}\n",
    "$$\n",
    "\n",
    "- Nesse caso, a matriz $\\mathbf{X}$ possui tamanho $N \\times D$, sendo $N$ o tamanho da sequência e $D$ a dimensão de *embedding* escolhida. Já as matrizes $\\mathbf{W^{(q)}}$, $\\mathbf{W^{(k)}}$, $\\mathbf{W^{(v)}}$, possuem tamanho: $D \\times D_k$, $D \\times D_k$ e $D \\times D_v$. As matrizes $\\mathbf{W^{(q)}}$ e $\\mathbf{W^{(k)}}$ possuem a mesma dimensão para que o produto interno funcione e, tipicamente, definimos $D_k = D$, bem como $D_v = D$, para que a saída final tenha as mesmas dimensões da entrada, facilitando a inclusão de conexões residuais.\n",
    "\n",
    "Existe um último refinamento que podemos fazer para a nossa camada de *self-attention*. Note que ao derivarmos a função `softmax` para computar os gradientes durante o *backpropagation*, as entradas de maior magnitude terão gradientes menores (similar com o que acontece com as funções `tanh` e `sigmoid`). Para ajudar a prevenir esse problema, podemos reescalar o produto interno $\\mathbf{Q} \\mathbf{K^T}$ antes de computarmos uma `softmax`. Para isso, iremos dividir essa parte por $\\sqrt{D_k}$, sendo $D_k$ a dimensão \"interna\" do produto matricial $\\mathbf{Q} \\mathbf{K^T}$. A intuição por trás disso pode ser lida no livro do [Bishop](https://www.bishopbook.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIH4-c45Cw7W"
   },
   "source": [
    "Agora, vamos implementar essa ideia em PyTorch!\n",
    "\n",
    "Utilizaremos o seguinte fluxo como referência, onde `mat mul` representa uma operação de multiplicação de matrizes e `scale` refere-se a normalização por $\\sqrt{D_k}$.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://github.com/ThiagoPoppe/ciag2024/blob/main/imagens/transformers/attention_scheme.png?raw=true\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implemente o `AttentionBlock` com o *scaled dot-product*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTvtWFghCw7W"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Defina a normalizacao pela raiz de Dk\n",
    "        # 2. Defina as matrizes Wq, Wk e Wv\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "\n",
    "        # 1. Multiplique q e k com bmm\n",
    "        # 2. Normalize e passe pela Softmax\n",
    "        # 3. Multiplique o resultado por v\n",
    "\n",
    "        return attn_outputs, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7vk7JgaCw7X"
   },
   "source": [
    "Iremos criar um vetor aleatório de dados apenas para verificar se fizemos as contas de forma correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDWWg3w-Cw7X",
    "outputId": "2d7e7353-c853-47d8-9745-a94f7e7d0399"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.rand(1, 10, 5)  # (batch_size, seq_lengh, n_features)\n",
    "attention = AttentionBlock(5, 5)\n",
    "\n",
    "attn_outputs, attn_weights = attention(x, x, x)\n",
    "print('Tamanho da saída da atenção:', attn_outputs.shape) # torch.Size([1, 10, 5])\n",
    "print('Dimensão da matriz de atenção:', attn_weights.shape) # torch.Size([1, 10, 10])\n",
    "\n",
    "plt.matshow(attn_weights[0].detach().numpy(), cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tP6PaTN7Cw7Y"
   },
   "source": [
    "O código implementado pela classe `AttentionBlock` representa o que chamamos de uma **cabeça de atenção**. Tipicamente, utilizaremos múltiplas cabeças para capturar diferentes padrões da entrada, tendo como análogo a passagem de múltiplos filtros em uma camada convolucional quando trabalhamos com imagens!\n",
    "\n",
    "Suponha que temos $H$ cabeças de atenção, onde a cabeça $h$ possui a seguinte formulação: $\\mathbf{H_h} = \\text{Attention}(\\mathbf{Q_h}, \\mathbf{K_h}, \\mathbf{V_h})$. Então, a nossa saída será representada por $\\mathbf{Y} = \\text{concat}[\\mathbf{H_1}, \\mathbf{H_2}, \\dots, \\mathbf{H}_H]$. Como cada matriz $\\mathbf{H}_h$ possui dimensão $N \\times D_v$, a matriz final $\\mathbf{Y}$ terá dimensão $N \\times HD_v$. Para combinarmos as múltiplas interpretações e aplicarmos futuramente conexões residuais, precisaremos realizar uma múltiplicação da concatenação das cabeças por uma matriz $\\mathbf{W}^{(o)}$, de dimensão $HD_v \\times D$. Com isso, teremos que a saída do nosso mecanismo de atenção de múltiplas cabeças (*multi-head attention*) será: $\\mathbf{Y} = \\text{concat}[\\mathbf{H_1}, \\mathbf{H_2}, \\dots, \\mathbf{H}_H]\\mathbf{W}^{(o)}$.\n",
    "\n",
    "> Tipicamente, escolhemos $D_v = D / H$, de forma que a matriz concatenada tenha dimensão $N \\times D$.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img width=600 src=\"https://github.com/ThiagoPoppe/ciag2024/blob/main/imagens/transformers/mha_details.png?raw=true\"/>\n",
    "</div>\n",
    "\n",
    "Vamos implementar essa ideia em Pytorch.\n",
    "\n",
    "Utilizaremos o seguinte fluxo como referência, onde cada bloco *self-attention* é uma instância da classe `AttentionBlock` passando `q = k = v = x`.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img width=600 src=\"https://github.com/ThiagoPoppe/ciag2024/blob/main/imagens/transformers/mha_scheme.png?raw=true\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implemente a `MultiHeadAttention`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znaHLEGxCw7Z"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super().__init__()\n",
    "        assert d_model % nhead == 0, f'{nhead} must divide {d_model}'\n",
    "\n",
    "        # 1. Defina a matriz Wo\n",
    "        # 2. Crie os multiplos blocos de atencao (nhead)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        attn_outputs = []\n",
    "        attn_weights = []\n",
    "\n",
    "        # 1. Passe os valores q, k e v por todos os blocos de atencao\n",
    "        # 2. Concatene os resultados\n",
    "        # 3. Passe pela camada linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLxfxrSECw7Z"
   },
   "source": [
    "Iremos criar um vetor aleatório de dados apenas para verificar se fizemos as contas de forma correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSHKWjTaCw7Z",
    "outputId": "17eb6833-4abc-45f9-b486-95eb071242c3"
   },
   "outputs": [],
   "source": [
    "x = torch.rand(1, 10, 40)  # (batch_size, seq_lenght, n_features)\n",
    "mha = MultiHeadAttention(40, 4)\n",
    "\n",
    "attn_outputs, attn_weights = mha(x, x, x)\n",
    "print('Tamanho da saída da atenção:', attn_outputs.shape) # torch.Size([1, 10, 40])\n",
    "print('Número de matrizes de atenção:', len(attn_weights)) # 4\n",
    "print('Dimensão de uma matriz de atenção:', attn_weights[0].shape) # torch.Size([1, 10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8V2gYPECw7a",
    "outputId": "ce144875-5138-4f27-c2bb-36317f529299"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "counter = 0\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axs[i, j].set_title(f'Head {counter}')\n",
    "        axs[i,j].matshow(attn_weights[counter][0].detach().numpy(), cmap='gray')\n",
    "        counter += 1\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2_wKaZoCw7a"
   },
   "source": [
    "### Construção de uma camada do *Transformer*\n",
    "\n",
    "Entendendo o que um bloco de *multi-head attention* faz, conseguimos prosseguir na construção da camada de um *Transformer*. Após a aplicação de um mecanismo de atenção, iremos realizar um *batch normalization* da saída do mecanismo juntamente com a adição da sua entrada, tendo assim uma conexão residual da seguinte forma, onde simplificaremos a notação de *self-attention* como sendo $\\text{MHA}(\\mathbf{X})$:\n",
    "\n",
    "$$\n",
    "\\mathbf{Z} = \\text{LayerNorm}[\\text{MHA}(\\mathbf{X}) + \\mathbf{X}]\n",
    "$$\n",
    "\n",
    "- Há a opção de também aplicarmos uma camada de *batch normalization* antes de computarmos a atenção da entrada da camada, já que isso pode resultar em uma otimização mais efetiva, ficando da seguinte forma: $\\mathbf{Z} = \\text{MHA}(\\text{LayerNorm}[\\mathbf{X}]) + \\mathbf{X}$.\n",
    "\n",
    "Note que até então, todas as operações feitas pela camada do nosso *Transformer* são operações lineares, cujas saídas residem no subespaço vetorial definido pelos vetores de entrada, limitando a capacidade de expressividade da nossa rede. Para lidar com isso, podemos introduzir uma sequência de camadas não-lineares após o cálculo de $\\mathbf{Z}$. Em outras palavras, podemos pós-processar a saída da nossa camada de atenção através de uma rede MLP (*multilayer perceptron*) da seguinte forma (note que ainda trabalharemos com conexões residuais e *batch normalization*):\n",
    "\n",
    "$$\n",
    "\\mathbf{\\tilde{X}} = \\text{LayerNorm}[\\text{MLP}(\\mathbf{Z}) + \\mathbf{Z}]\n",
    "$$\n",
    "\n",
    "- Aqui também temos a opção de aplicar a camada de normalização antes de pós-processarmos o vetor $\\mathbf{Z}$, ficando da seguinte forma: $\\mathbf{\\tilde{X}} = \\text{MLP}(\\text{LayerNorm}[\\mathbf{Z}]) + \\mathbf{Z}$.\n",
    "\n",
    "\n",
    "Com isso, concluímos os componentes fundamentais de uma camada da arquitetura *Transformers*. Uma sumarização de todos esses passos pode ser vista através do seguinte esquema.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img width=250 src=\"https://github.com/ThiagoPoppe/ciag2024/blob/main/imagens/transformers/transformer_layer_scheme.png?raw=true\"/>\n",
    "</div>\n",
    "\n",
    "Vamos implementar essa ideia em PyTorch agora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63_C52LAW58a"
   },
   "source": [
    "3. Implemente a classe `TransformerLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciCNfIN6Cw7a"
   },
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, norm_first):\n",
    "        \"\"\"\n",
    "        Implementação de uma camada da arquitetura Transformers.\n",
    "\n",
    "        Argumentos\n",
    "        ----------\n",
    "            d_model (int): dimensão do vetor de entrada\n",
    "            nhead (int): número de cabeças de atenção\n",
    "            dim_feedforward (int): tamanho do estado oculto da MLP\n",
    "            norm_first (bool): flag indicando se queremos aplicar batch normalization antes ou depois das operações\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Defina o componente de MultiHeadAttention\n",
    "        # 2. Defina duas camadas de normalizacao\n",
    "        # 3. Defina uma MLP\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 1. Passe a entrada pelo componente de atencao\n",
    "        # 2. Aplique uma camada de normalizacao, com a skip-connection de x\n",
    "        # 3. Passe o resultado z pela MLP\n",
    "        # 4. Aplique a segunda camada de normalizacao, com a skip-connection de z\n",
    "\n",
    "    def apply_batch_norm(self, x, norm_layer):\n",
    "        x = x.transpose(1, 2)  # batch norm espera dimensões (batch_size, n_features, seq_length)\n",
    "        x = norm_layer(x)\n",
    "        x = x.transpose(1, 2)  # camadas futuras esperam (batch_size, seq_length, n_features)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve9X1gzNCw7a"
   },
   "source": [
    "Iremos criar um vetor aleatório de dados apenas para verificar se fizemos as contas de forma correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sLaZ19mCw7a",
    "outputId": "c1432182-a885-4963-b559-330664b9e373"
   },
   "outputs": [],
   "source": [
    "x = torch.rand(1, 10, 512)  # (batch_size, seq_lengh, n_features)\n",
    "layer = TransformerLayer(d_model=512, nhead=8, dim_feedforward=2048, norm_first=False)\n",
    "\n",
    "outputs, attn_weights = layer(x)\n",
    "print('Tamanho da saída da camada:', outputs.shape) # torch.Size([1, 10, 512])\n",
    "print('Número de matrizes de atenção:', len(attn_weights)) # 8\n",
    "print('Dimensão de uma matriz de atenção:', attn_weights[0].shape) # torch.Size([1, 10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZ1q2AhzCw7a",
    "outputId": "7fb5cd22-839b-46b0-cd50-4f411a62f81c"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4, figsize=(12, 8))\n",
    "\n",
    "counter = 0\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        axs[i, j].set_title(f'Head {counter}')\n",
    "        axs[i,j].matshow(attn_weights[counter][0].detach().numpy(), cmap='gray')\n",
    "        counter += 1\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4f9_SkNCw7b"
   },
   "source": [
    "### Positional encoding\n",
    "\n",
    "Na arquitetura de um *Transformer*, as matrizes $\\mathbf{W_h^{(q)}}$, $\\mathbf{W_h^{(k)}}$, $\\mathbf{W_h^{(v)}}$ são compartilhadas pelos *tokens* de entrada, assim como o restante da rede. Sendo assim, um *Transformer* é **equivariante** à permutações da entrada! As frases em inglês: *The food was bad, not good at all* e *The food was good, not bad at all*, possuem as mesmas palavras, porém, possuem sentidos completamente diferentes.\n",
    "\n",
    "Para solucionar esse problema, iremos adicionar ao *embedding* da nossa entrada uma noção de posição através de um *positional encoding*, em outras palavras, o nosso novo *embedding* para o n-ésimo token será: $\\mathbf{\\tilde{x}}_n = \\mathbf{x}_n + \\mathbf{p}_n$, sendo $\\mathbf{x}_n$ o *embedding* original do n-ésimo *token* e $\\mathbf{p}_n$ um vetor de mesma dimensão que possui uma informação da posição daquele *token* na entrada. Existem diversas formas de gerar o vetor $\\mathbf{p}$_n, para mais discussões sobre essas diferentes estratégias, consulte o livro do [Bishop](https://www.bishopbook.com/).\n",
    "\n",
    "Aqui, iremos utilizar uma técnica baseada em funções senoidais introduzida por *Vaswani et al.*, em 2017, no seu artigo [*Attention Is All You Need*](https://arxiv.org/pdf/1706.03762.pdf). Para uma da posição $n$, teremos um vetor de *position-encoding* $\\mathbf{p}_n$ associado, tendo seus componentes construídos da seguinte forma:\n",
    "$$\n",
    "\\mathbf{p}_n^{(i)} =\n",
    "\\begin{cases}\n",
    "   \\sin{\\left(\\dfrac{n}{L^{i / D}}\\right)}, & \\text{ se } i \\text{ for par (da forma } 2k \\text{),} \\\\\n",
    "   \\cos{\\left(\\dfrac{n}{L^{(i-1) / D}}\\right)}, & \\text{ se } i \\text{ for ímpar (da forma } 2k+1 \\text{),} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "   - No artigo original, temos que $L = 10000$ (escolhido de forma empírica).\n",
    "\n",
    "Vamos implementar essa ideia em PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVx0kY04Cw7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)  # fazendo com que \"pe\" seja um buffer (variável não treinável)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1]]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZPWvcgsCw7b"
   },
   "source": [
    "Iremos criar um vetor aleatório de dados apenas para verificar se fizemos as contas de forma correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pBsSXaYCw7b",
    "outputId": "f2e6aa11-50f7-4b2b-f086-11cb02939d8a"
   },
   "outputs": [],
   "source": [
    "x = torch.rand(1, 128, 512)\n",
    "encoding = PositionalEncoding(d_model=512)\n",
    "\n",
    "x = encoding(x)\n",
    "print('Tamanho dos vetores depois do positional encoding:', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "du8o6X6ZCw7b",
    "outputId": "425b5858-61ff-43d4-bbb2-6211932415c3"
   },
   "outputs": [],
   "source": [
    "plt.imshow(encoding.pe[0, :1000], cmap='magma', aspect='auto')\n",
    "plt.xlabel('Dimensão do embedding')\n",
    "plt.ylabel('Posição')\n",
    "\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giC3x1XbCw7c"
   },
   "source": [
    "Finalmente, iremos juntar todos os bloquinhos desenvolvidos até então para criar um *Transformers* de múltiplas camadas.\n",
    "\n",
    "> - O que estamos fazendo aqui na verdade é implementando os bloquinhos que fazem parte da etapa do *encoder* da arquitetura final, justificando assim o nome da classe ser `TransformerEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0hNuFrTYeV_"
   },
   "source": [
    "4. Implemente o modelo `TransformerEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnUMLanHCw7c"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, dim_feedforward, norm_first, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Defina o componente de PositionalEncoding\n",
    "        # 2. Defina uma camada linear para os embbedings\n",
    "        # 3. Defina num_layers camadas Transformer\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 1. Passe a entrada pela camada linear e pelo positional encoding\n",
    "        # 2. Envie o resultado para a primeira camada Transformer\n",
    "        # 3. Percorra todas as camadas, passando o resultado da camada atual como entrada para a proxima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VP4i2VNPCw7c"
   },
   "source": [
    "Iremos criar um vetor aleatório de dados apenas para verificar se fizemos as contas de forma correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrnZ2n0HCw7c",
    "outputId": "d776c94d-87f3-4b02-9d47-b003620e0ff7"
   },
   "outputs": [],
   "source": [
    "x = torch.rand(1, 10, 4)\n",
    "encoder = TransformerEncoder(input_size=4, d_model=16, nhead=4,\n",
    "                             dim_feedforward=64, norm_first=False, num_layers=2)\n",
    "\n",
    "outputs, attn_weights = encoder(x)\n",
    "print('Dimensão da saída:', outputs.shape) # torch.Size([1, 10, 16])\n",
    "print('Número de atenções de cada camada:', len(attn_weights[0])) # 4\n",
    "print('Dimensão das matrizes de atenção:', attn_weights[0][0].shape) # torch.Size([1, 10, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn3UvzlZCw7c",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exemplo prático: previsão de litologia utilizando *Transformers*\n",
    "\n",
    "Para finalizar, iremos realizar um exemplo prático do que desenvolvemos até então em um contexto geológico através da previsão de litologia dado um conjunto de séries temporais como entrada. Para mais detalhes sobre os dados utilizados, bem como uma versão do mesmo exemplo utilizando LSTMs, consulte o seguinte [notebook](https://colab.research.google.com/drive/1PHgm0yveHSfFXt-ODtQ9_XFdqufN7qP6?usp=sharing#scrollTo=V9xCHeZxv_9c), desenvolvido pelo monitor João Pedro.\n",
    "\n",
    "> - Iremos apenas carregar os dados de interesse e usar o mesmo pré-processamento discutido nesse outro notebook, logo não daremos muito foco para essas partes aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5A2RN8PCw7c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lithology_keys = {\n",
    "    0: 'Sandstone',\n",
    "    1: 'Sandstone/Shale',\n",
    "    2: 'Shale',\n",
    "    3: 'Marl',\n",
    "    4: 'Dolomite',\n",
    "    5: 'Limestone',\n",
    "    6: 'Chalk',\n",
    "    7: 'Halite',\n",
    "    8: 'Anhydrite',\n",
    "    9: 'Tuff',\n",
    "    10: 'Coal',\n",
    "    11: 'Basement'\n",
    "}\n",
    "\n",
    "lithology_numbers = {\n",
    "    30000: 0,\n",
    "    65030: 1,\n",
    "    65000: 2,\n",
    "    80000: 3,\n",
    "    74000: 4,\n",
    "    70000: 5,\n",
    "    70032: 6,\n",
    "    88000: 7,\n",
    "    86000: 8,\n",
    "    99000: 9,\n",
    "    90000: 10,\n",
    "    93000: 11\n",
    "}\n",
    "\n",
    "def process_data(df):\n",
    "    interested = ['WELL', 'FORCE_2020_LITHOFACIES_LITHOLOGY', 'GR', 'NPHI', 'RHOB', 'DTC']\n",
    "    df = df[interested]\n",
    "\n",
    "    df = df.rename(columns={'FORCE_2020_LITHOFACIES_LITHOLOGY' : 'CLASS'})\n",
    "    df['CLASS'] = df['CLASS'].map(lithology_numbers)\n",
    "\n",
    "    df = df[['WELL', 'GR', 'NPHI', 'RHOB', 'DTC', 'CLASS']]\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPRhsuT4Cw7c"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/pgeoprj/ciag2023/datasets/force_dataset/train.csv', sep=';')\n",
    "test_df = pd.read_csv('/pgeoprj/ciag2023/datasets/force_dataset/hidden_test.csv', sep=';')\n",
    "\n",
    "train_df = process_data(train_df)\n",
    "test_df = process_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DH-X074Cw7c",
    "outputId": "1811a6ac-579b-47b5-f31f-67ec982e6f59"
   },
   "outputs": [],
   "source": [
    "print('Dimensão dos dados de treino:', train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fnLfLHJCw7c",
    "outputId": "2f94accd-3f5a-4433-d10b-4570d5b79975"
   },
   "outputs": [],
   "source": [
    "train_df[['GR', 'NPHI', 'RHOB', 'DTC']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32YesqdDCw7d",
    "outputId": "980100e2-d2d7-4562-a3df-babb7a7dfa3c"
   },
   "outputs": [],
   "source": [
    "print('Dimensão dos dados de teste:', test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8HcVrMaCw7d",
    "outputId": "e071b404-197c-4f7e-a528-e7debcda88de"
   },
   "outputs": [],
   "source": [
    "test_df[['GR', 'NPHI', 'RHOB', 'DTC']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKFjBed4Cw7d"
   },
   "source": [
    "Visualização simples da distribuição de em ambos conjuntos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADOEsHzECw7d",
    "outputId": "2bd0ee16-223b-4215-ab54-fc0a95a0f6a8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_names = []\n",
    "train_percentage = []\n",
    "train_counts = train_df['CLASS'].value_counts()\n",
    "\n",
    "test_names = []\n",
    "test_percentage = []\n",
    "test_counts = test_df['CLASS'].value_counts()\n",
    "\n",
    "for item in train_counts.items():\n",
    "    train_names.append(lithology_keys[item[0]])\n",
    "    train_percentage.append(100 * float(item[1])/train_df.shape[0])\n",
    "\n",
    "for item in test_counts.items():\n",
    "    test_names.append(lithology_keys[item[0]])\n",
    "    test_percentage.append(100 * float(item[1])/test_df.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "\n",
    "ax[0].set_title('Train set')\n",
    "ax[0].bar(x=np.arange(len(train_names)), height=train_percentage)\n",
    "ax[0].set_xticks(np.arange(len(train_names)))\n",
    "ax[0].set_xticklabels(train_names, rotation=45)\n",
    "\n",
    "ax[1].set_title('Hidden test set')\n",
    "ax[1].bar(x=np.arange(len(test_names)), height=test_percentage)\n",
    "ax[1].set_xticks(np.arange(len(test_names)))\n",
    "ax[1].set_xticklabels(test_names, rotation=45)\n",
    "\n",
    "fig.supylabel('Lithology presence (\\%)')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf59eNpeCw7d"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FORCE(Dataset):\n",
    "    def __init__(self, dataframe, window_size = 50):\n",
    "        # Convert dataframe to NumPy array\n",
    "        self.data_array = dataframe.drop(columns=['WELL']).values\n",
    "        self.groups = dataframe['WELL'].values\n",
    "        self.window_size = window_size\n",
    "        self.group_indices = self.compute_group_indices()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.group_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        group_idx, data_idx = self.group_indices[idx]\n",
    "        sequence_ = self.data_array[data_idx:data_idx+self.window_size]\n",
    "        sequence = sequence_[:,:-1]\n",
    "        label = sequence_[:,-1]\n",
    "\n",
    "        sequence = (sequence - sequence.mean())/sequence.std()\n",
    "\n",
    "        return torch.from_numpy(sequence).to(torch.float32), torch.from_numpy(label).to(torch.long)\n",
    "\n",
    "    def compute_group_indices(self):\n",
    "        unique_groups, group_counts = np.unique(self.groups, return_counts=True)\n",
    "        group_indices = []\n",
    "        start_idx = 0\n",
    "        for group, count in zip(unique_groups, group_counts):\n",
    "            end_idx = start_idx + count - self.window_size - 1\n",
    "            indices = [(i, idx) for i, idx in enumerate(range(start_idx, end_idx))]\n",
    "            group_indices.extend(indices)\n",
    "            start_idx = end_idx\n",
    "        return group_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdABsMf8Cw7d",
    "outputId": "d5cb4b7a-d36e-464e-b521-80f80a185a71"
   },
   "outputs": [],
   "source": [
    "window_size = 50\n",
    "train_dataset = FORCE(train_df, window_size)\n",
    "test_dataset = FORCE(test_df, window_size)\n",
    "\n",
    "print('Número de dados de treino:', len(train_dataset))\n",
    "print('Número de dados de teste:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUmFbLBzCw7d",
    "outputId": "a3dd040f-18e1-4346-9e3c-cc4dbc430f28"
   },
   "outputs": [],
   "source": [
    "X, y = train_dataset[0]\n",
    "print('Dimensão das features:', X.shape)\n",
    "print('Dimensão das anotações:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RVnKJXUCw7d"
   },
   "source": [
    "### Treinamento do modelo sobre os dados da competição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOHHuYF1Cw7d",
    "outputId": "c28f8c26-f0ef-4c3e-a556-ea4c9f75f4e2"
   },
   "outputs": [],
   "source": [
    "has_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if has_cuda else 'cpu')\n",
    "\n",
    "print('Utilizando:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COYnkvBcCw7r"
   },
   "outputs": [],
   "source": [
    "class LithologyModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, d_model, nhead, dim_feedforward, norm_first, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(input_size, d_model, nhead,\n",
    "                                          dim_feedforward, norm_first, num_layers)\n",
    "        self.classifier = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, attn_weights = self.encoder(x)\n",
    "        outputs = self.classifier(outputs)\n",
    "\n",
    "        return outputs, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xq2u-gybCw7r",
    "outputId": "e2bb1a19-63bc-4532-874b-4deec957b71b"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 1024\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True, shuffle=False)\n",
    "\n",
    "model = LithologyModel(input_size=4, output_size=12, d_model=16, nhead=4,\n",
    "                           dim_feedforward=32, norm_first=False, num_layers=1)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad == True)\n",
    "print('Número de parâmetros do nosso modelo:', n_params)\n",
    "\n",
    "print('\\nComponentes do modelo final:')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uz-C-20ZCw7r"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        outputs, attn_weights = model(X)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs.transpose(1, 2), y)  # a loss function espera que a saída do modelo seja (batch_size, out_size, seq_lengh)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Batch [{batch+1}/{len(dataloader)}] -> batch loss: {loss.item():.5f}')\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d7983648f6a445408773632c74f62efb",
      "e3ceccfd540748e6b7419f360b3c99cd",
      "12e427bcd49a4d588baa57e9072b0ec4",
      "c8b86f1166d54f75abd97e25e65a2896",
      "a3ba77ac129f4b36847e4674ed3aafc5",
      "d19b76da47114df6b64b965c8b36765b",
      "d0528fc2084047b192fc21738bcc78c3",
      "7417d98a21e54b189bbd3c6b6e072ae6",
      "a7eecf83266d47ab833c311c48c59a59",
      "e4ee50929b20433e87d4075f4fcada92",
      "3aaa27364f714206bd8c1a3228148e2b",
      "17d0200c4dee453596641e535533c4a6",
      "e0a0eeb7818045319f9e038a79b4102a",
      "cae8c71ee3444db6aa0f734e2f58529a",
      "18d4bc53548f4142b91a418ae0924fb2",
      "24ca322710944f6ea8b0003a08aa0a82",
      "e2e59b67d2914157a3155224bb5f7494",
      "92c449c342b140a5b288d94c965a82ca",
      "ff4554dcee174814a96e9b8bc2483f57",
      "b99c6a3b773349eea71e4841bbbdc5b4"
     ]
    },
    "id": "TsCXkRJaCw7r",
    "outputId": "bcfb9364-ca07-4ed7-94f0-871360212db1"
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = train_epoch(model, train_dataloader, optimizer, criterion)\n",
    "\n",
    "    print(f'Epoch [{epoch}/{num_epochs}] -> mean epoch loss: {epoch_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lb9Iv8QkCw7r"
   },
   "source": [
    "### Avaliando a performance da rede em dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kwe-ZOO1Cw7r"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    predictions = torch.zeros(len(dataloader), batch_size, window_size).to(device)\n",
    "    labels = torch.zeros_like(predictions)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs, attn_weights = model(X)\n",
    "\n",
    "            loss = criterion(outputs.transpose(1, 2), y)  # a loss function espera que a saída do modelo seja (batch_size, out_size, seq_lengh)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions[batch] = outputs.argmax(dim=-1)\n",
    "            labels[batch] = y\n",
    "\n",
    "    total_loss /= len(dataloader)\n",
    "    return total_loss, predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b6fe08d3efa84c62a4430252871b2cce"
     ]
    },
    "id": "KxnXdlz5Cw7r",
    "outputId": "453572c1-98ee-401d-d99a-16382903da3e"
   },
   "outputs": [],
   "source": [
    "total_loss, predictions, labels = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "print(f'Mean hidden test loss: {total_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBRN2CxnCw7r",
    "outputId": "1939715a-cb9f-477c-8358-b95687adb143"
   },
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy, Precision, Recall, ConfusionMatrix\n",
    "\n",
    "acc = Accuracy(task = 'multiclass', num_classes = 12).to(device)\n",
    "prec = Precision(task = 'multiclass', average='macro', num_classes = 12).to(device)\n",
    "recall = Recall(task = 'multiclass', average='macro', num_classes = 12).to(device)\n",
    "\n",
    "print(f'Acurácia: {(100*acc(labels, predictions)):.2f}%')\n",
    "print(f'Precisão: {(100*prec(labels, predictions)):.2f}%')\n",
    "print(f'Recall: {(100*recall(labels, predictions)):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wERai-mQCw7r"
   },
   "source": [
    "Computando uma matriz de confusão para verificarmos a qualidade do nosso modelo nos dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I58NClNtCw7s",
    "outputId": "c1a7fd81-008a-41e9-9ba5-5fdb66a65c6e"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix = ConfusionMatrix(task = 'multiclass', num_classes = 12).to(device)\n",
    "cm = confusion_matrix(labels, predictions).cpu()\n",
    "cm = (cm.float() / cm.sum(axis=1)[:, np.newaxis]).nan_to_num()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt='.2f', xticklabels=lithology_keys.values(), yticklabels=lithology_keys.values())\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show(block=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
