{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgF3qLus-Fve"
   },
   "source": [
    "# Transformers\n",
    "\n",
    "## Encoder-Decoder Transformers\n",
    "\n",
    "Neste notebook, iremos trabalhar e entender um pouco mais dos componentes essencias que compõem a parte de *decoder* de um* Transformers*, além de utilizar a arquitetura completa para realizar a previsão de litologia de perfis de poço (séries temporais)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chave:  f5qn\n",
      "Senha: ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variáveis de ambiente http_proxy e https_proxy configuradas!\n"
     ]
    }
   ],
   "source": [
    "%load_ext nbproxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNA3Fxtp-Fvj"
   },
   "source": [
    "### Revisão rápida dos conceitos da última aula\n",
    "\n",
    "O mecanismo de atenção utilizado por um *Transformers* se dá através do processo chamado de *scaled dot-product*, tendo a seguinte fórmula:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Softmax}\\left[\\dfrac{\\mathbf{Q}\\mathbf{K^T}}{d\\_model}\\right] \\mathbf{V}\n",
    "$$\n",
    "\n",
    "Além disso, é proposto um aprimoramento desse conceito de atenção através do uso de múltiplas **cabeças de atenção**, um análogo ao uso de múltiplos filtros convolucionais em uma camada convolucional de uma rede CNN.\n",
    "\n",
    "$$\n",
    "\\text{MHA}(Q, K, V) = \\text{concat}\\left[H_1, \\dots, H_n\\right]\\mathbf{W^{(o)}} \\text{ , onde } H_i = Attention(Q\\mathbf{W^{(q)}_i}, K\\mathbf{W^{(k)}_i}, V\\mathbf{W^{(v)}_i})\n",
    "$$\n",
    "\n",
    "O mecanismo de atenção MHA é usado em diversas regiões do nosso transformer, tanto no *encoder* quanto no *decoder*. Durante o *encoder*, utilizamos o que é chamado de **self-attention**, formulado por $\\text{MHA}(X, X, X)$. Já no *decoder* utilizamos um **masked self-attention**, que opera identicamente ao **self-attention** visto anteriormente, porém agora multiplicamos a atenção por uma máscara binária, suprimindo algumas entradas da matriz de atenção; bem como um **cross-attention**, onde temos a formulação $\\text{MHA}(Q, K, V)$ usual, onde $Q$ é informado pelo *decoder* e $K, V$ pelo *encoder*.\n",
    "\n",
    "A partir da imagem a seguir, podemos observar a relação e utilidade desses diversos blocos ao longo de um *Transformers*:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img width=400 src=\"https://github.com/ThiagoPoppe/ciag2024/blob/main/imagens/transformers/transformers.png?raw=true\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAhKvVH--Fvl"
   },
   "source": [
    "Como fazemos tudo isso em PyTorch? Utilizamos o módulo `nn.Transformer`, cuja documentação pode ser acessada [aqui](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)!\n",
    "\n",
    "> Manipulando as máscaras de atenção do *encoder* e *decoder*, podemos adaptar esse módulo para suportar diversas tarefas relacionadas com sequência (*many-to-one*, *one-to-many*, *many-to-many*), respeitando ou não a sequencialidade e o viés temporal da entrada.\n",
    "\n",
    "- **Importante:** Note que a camada de *Positional Encoding* e *Embedding* não fazem parte do módulo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sVFHqU9N-Fvl",
    "outputId": "7780f3ff-5633-42b2-d373-4bee1b323afc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "transformer = nn.Transformer(d_model=512, nhead=8, dim_feedforward=2048, activation='gelu',\n",
    "                             num_encoder_layers=1, num_decoder_layers=1, batch_first=True)\n",
    "\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "omUXtDvx-Fvn",
    "outputId": "d6ad58ac-f66b-4565-a7b8-62792e0ac4f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de parâmetros: 7358464\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in transformer.parameters() if p.requires_grad == True)\n",
    "print('Número total de parâmetros:', num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vh7xg00N-Fvo"
   },
   "source": [
    "Iremos passar apenas vetores aleatórios para observar o comportamento da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "N0F_ptHw-Fvp",
    "outputId": "e6e0142f-0e71-45a6-edbd-43d8f51c80a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão da saída: torch.Size([4, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "src = torch.rand((4, 16, 512))\n",
    "tgt = torch.rand((4, 16, 512))\n",
    "out = transformer(src, tgt)\n",
    "\n",
    "print('Dimensão da saída:', out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Muq30VRQDt-J"
   },
   "source": [
    "## Tradução utilizando *Transformers*\n",
    "\n",
    "Agora, vamos implementar um modelo `encoder-decoder` para tradução. Esse tipo de modelo recebe uma sequência na entrada, e gera uma nova sequência de saída, que é naturalmente relacionada à entrada.\n",
    "\n",
    "O dataset `IWSLT 2017` será usado para treinarmos um modelo que traduz do inglês para o francês. Lembre-se que os dados de texto precisam ser tokenizados, para que o modelo possa compreendê-los como números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /u/f5qn/.local/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /u/f5qn/.local/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /u/f5qn/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /u/f5qn/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /u/f5qn/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /u/f5qn/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sentencepiece in /u/f5qn/.local/lib/python3.10/site-packages (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sacremoses in /u/f5qn/.local/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.4.28)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1V7-IriJLOMd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232825 232825\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "dataset = load_dataset(\"iwslt2017\", \"iwslt2017-en-fr\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "def tokenize_data(example):\n",
    "    src = tokenizer(example[\"translation\"][\"en\"], truncation=True, padding=\"max_length\", max_length=50, return_tensors=\"pt\")\n",
    "    tgt = tokenizer(example[\"translation\"][\"fr\"], truncation=True, padding=\"max_length\", max_length=50, return_tensors=\"pt\")\n",
    "    return { \"src\": src.input_ids.squeeze(0), \"tgt\": tgt.input_ids.squeeze(0) }\n",
    "\n",
    "dataset = dataset.map(tokenize_data, remove_columns=[\"translation\"])\n",
    "\n",
    "print(len(dataset[\"train\"][\"src\"]), len(dataset[\"train\"][\"tgt\"])) # devem ter o mesmo tamanho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wycTB1VPUmXb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = torch.tensor(self.data[idx][\"src\"], dtype=torch.long)\n",
    "        tgt = torch.tensor(self.data[idx][\"tgt\"], dtype=torch.long)\n",
    "        return src, tgt\n",
    "\n",
    "train_data = TranslationDataset(dataset[\"train\"])\n",
    "\n",
    "for src, tgt in train_data:\n",
    "    print(type(src), type(tgt))\n",
    "    print(src.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7ksYxp8SzVr"
   },
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "c9qDKbOtSxS7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "has_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if has_cuda else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4LoR75oTlxM"
   },
   "source": [
    "##### 1. Implemente o modelo `TransformerTranslator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "iKEe6WhyPixb"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\" Código baseado de https://pytorch.org/tutorials/beginner/transformer_tutorial.html \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)  # fazendo com que \"pe\" seja um buffer (variável não treinável)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1]]\n",
    "        return x\n",
    "\n",
    "class TransformerTranslator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=512, num_heads=8, num_layers=6, dropout=0.1, batch_first=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(embed_size)\n",
    "        self.transformer = nn.Transformer(d_model=embed_size, nhead=num_heads, num_encoder_layers=num_layers,\n",
    "                                          num_decoder_layers=num_layers, dim_feedforward=2048, dropout=dropout, batch_first=batch_first)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "        self.src_pad_idx = tokenizer.pad_token_id\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = self._generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        tgt_mask = self._generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "\n",
    "        src_emb = self.positional_encoding(self.embedding(src))\n",
    "        tgt_emb = self.positional_encoding(self.embedding(tgt))\n",
    "\n",
    "        # print(\"sizes: \", src_mask.shape, tgt_mask.shape, src_emb.shape, tgt_emb.shape)\n",
    "\n",
    "        output = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask)\n",
    "\n",
    "        return self.fc_out(output)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_M49HmhvTv4C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parâmetros do nosso modelo: 105142394\n",
      "\n",
      "Componentes do modelo final:\n",
      "TransformerTranslator(\n",
      "  (embedding): Embedding(59514, 512)\n",
      "  (positional_encoding): PositionalEncoding()\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=512, out_features=59514, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = TransformerTranslator(vocab_size=tokenizer.vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad == True)\n",
    "print('Número de parâmetros do nosso modelo:', n_params)\n",
    "\n",
    "print('\\nComponentes do modelo final:')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "EEJFmZ8LVEcG"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in tqdm(dataloader):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        tgt = tgt[:, :-1] # Shifted tgt para treinamento\n",
    "        output = model(src, tgt)  \n",
    "        output = output.reshape(-1, tokenizer.vocab_size)\n",
    "        tgt = tgt.reshape(-1)\n",
    "\n",
    "        loss = criterion(output.reshape(-1, tokenizer.vocab_size), tgt.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "sBq87dTSVEWE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 657/7276 [00:26<04:28, 24.69it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[39], line 16\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m tgt \u001b[38;5;241m=\u001b[39m tgt\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39mvocab_size), tgt\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:534\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    526\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    527\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    532\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 534\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:767\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTETYrvz-Fvq"
   },
   "source": [
    "## Previsão de litologia utilizando *Transformers*\n",
    "\n",
    "Iremos agora realizar um exercício prático do que desenvolvemos até então em um contexto geológico, através da previsão de litologia dado um conjunto de séries temporais como entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLAcc3XM-Fvq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lithology_keys = {\n",
    "    0: 'Sandstone',\n",
    "    1: 'Sandstone/Shale',\n",
    "    2: 'Shale',\n",
    "    3: 'Marl',\n",
    "    4: 'Dolomite',\n",
    "    5: 'Limestone',\n",
    "    6: 'Chalk',\n",
    "    7: 'Halite',\n",
    "    8: 'Anhydrite',\n",
    "    9: 'Tuff',\n",
    "    10: 'Coal',\n",
    "    11: 'Basement'\n",
    "}\n",
    "\n",
    "lithology_numbers = {\n",
    "    30000: 0,\n",
    "    65030: 1,\n",
    "    65000: 2,\n",
    "    80000: 3,\n",
    "    74000: 4,\n",
    "    70000: 5,\n",
    "    70032: 6,\n",
    "    88000: 7,\n",
    "    86000: 8,\n",
    "    99000: 9,\n",
    "    90000: 10,\n",
    "    93000: 11\n",
    "}\n",
    "\n",
    "def process_data(df):\n",
    "    interested = ['WELL', 'FORCE_2020_LITHOFACIES_LITHOLOGY', 'GR', 'NPHI', 'RHOB', 'DTC']\n",
    "    df = df[interested]\n",
    "\n",
    "    df = df.rename(columns={'FORCE_2020_LITHOFACIES_LITHOLOGY' : 'CLASS'})\n",
    "    df['CLASS'] = df['CLASS'].map(lithology_numbers)\n",
    "\n",
    "    df = df[['WELL', 'GR', 'NPHI', 'RHOB', 'DTC', 'CLASS']]\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIgJZQjj-Fvr"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/pgeoprj/ciag2023/datasets/force_dataset/train.csv', sep=';')\n",
    "test_df = pd.read_csv('/pgeoprj/ciag2023/datasets/force_dataset/hidden_test.csv', sep=';')\n",
    "\n",
    "train_df = process_data(train_df)\n",
    "test_df = process_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtmbgpvq-Fvr",
    "outputId": "c82051d4-3875-45a3-cee8-69b0d1a03579"
   },
   "outputs": [],
   "source": [
    "print('Dimensão dos dados de treino:', train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4iuj7l8E-Fvr",
    "outputId": "db3ce4af-9c17-445b-a113-ff5e90d27ca3"
   },
   "outputs": [],
   "source": [
    "train_df[['GR', 'NPHI', 'RHOB', 'DTC']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKKxR3LF-Fvs",
    "outputId": "0bd30101-52c4-4fe9-d65e-782bcb197048"
   },
   "outputs": [],
   "source": [
    "print('Dimensão dos dados de teste:', test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VciyhT1A-Fvs",
    "outputId": "250b3f87-56df-4a2f-dea3-819380d7e5d5"
   },
   "outputs": [],
   "source": [
    "test_df[['GR', 'NPHI', 'RHOB', 'DTC']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq_pah_K-Fvs"
   },
   "source": [
    "Visualização simples da distribuição de classes em ambos conjuntos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUCg_22Y-Fvs",
    "outputId": "4a1fe1ab-2007-41d1-eff3-153ec4679fe5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_names = []\n",
    "train_percentage = []\n",
    "train_counts = train_df['CLASS'].value_counts()\n",
    "\n",
    "test_names = []\n",
    "test_percentage = []\n",
    "test_counts = test_df['CLASS'].value_counts()\n",
    "\n",
    "for item in train_counts.items():\n",
    "    train_names.append(lithology_keys[item[0]])\n",
    "    train_percentage.append(100 * float(item[1])/train_df.shape[0])\n",
    "\n",
    "for item in test_counts.items():\n",
    "    test_names.append(lithology_keys[item[0]])\n",
    "    test_percentage.append(100 * float(item[1])/test_df.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "\n",
    "ax[0].set_title('Train set')\n",
    "ax[0].bar(x=np.arange(len(train_names)), height=train_percentage)\n",
    "ax[0].set_xticks(np.arange(len(train_names)))\n",
    "ax[0].set_xticklabels(train_names, rotation=45)\n",
    "\n",
    "ax[1].set_title('Hidden test set')\n",
    "ax[1].bar(x=np.arange(len(test_names)), height=test_percentage)\n",
    "ax[1].set_xticks(np.arange(len(test_names)))\n",
    "ax[1].set_xticklabels(test_names, rotation=45)\n",
    "\n",
    "fig.supylabel('Lithology presence (\\%)')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiXPNDG7-Fvs"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FORCE(Dataset):\n",
    "    def __init__(self, dataframe, window_size = 50):\n",
    "        # Convert dataframe to NumPy array\n",
    "        self.data_array = dataframe.drop(columns=['WELL']).values\n",
    "        self.groups = dataframe['WELL'].values\n",
    "        self.window_size = window_size\n",
    "        self.group_indices = self.compute_group_indices()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.group_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        group_idx, data_idx = self.group_indices[idx]\n",
    "        sequence_ = self.data_array[data_idx:data_idx+self.window_size]\n",
    "        sequence = sequence_[:,:-1]\n",
    "        label = sequence_[:,-1]\n",
    "\n",
    "        sequence = (sequence - sequence.mean())/sequence.std()\n",
    "\n",
    "        return torch.from_numpy(sequence).to(torch.float32), torch.from_numpy(label).to(torch.long)\n",
    "\n",
    "    def compute_group_indices(self):\n",
    "        unique_groups, group_counts = np.unique(self.groups, return_counts=True)\n",
    "        group_indices = []\n",
    "        start_idx = 0\n",
    "        for group, count in zip(unique_groups, group_counts):\n",
    "            end_idx = start_idx + count - self.window_size - 1\n",
    "            indices = [(i, idx) for i, idx in enumerate(range(start_idx, end_idx))]\n",
    "            group_indices.extend(indices)\n",
    "            start_idx = end_idx\n",
    "        return group_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-n2uQQU-Fvt",
    "outputId": "30208505-21a0-44d9-d2f5-24d1883cc20a"
   },
   "outputs": [],
   "source": [
    "window_size = 50\n",
    "train_dataset = FORCE(train_df, window_size)\n",
    "test_dataset = FORCE(test_df, window_size)\n",
    "\n",
    "print('Número de dados de treino:', len(train_dataset))\n",
    "print('Número de dados de teste:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzwlRIUh-Fvt",
    "outputId": "fa3cac97-4ce7-4860-d644-99371f0cc071"
   },
   "outputs": [],
   "source": [
    "X, y = train_dataset[0]\n",
    "print('Dimensão das features:', X.shape)\n",
    "print('Dimensão das anotações:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-7Q9KPLUQSE"
   },
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35yoYCvwUFaS"
   },
   "source": [
    "2. Implemente o modelo `LithologyTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwhNSl77-Fvt"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\" Código baseado de https://pytorch.org/tutorials/beginner/transformer_tutorial.html \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)  # fazendo com que \"pe\" seja um buffer (variável não treinável)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1]]\n",
    "        return x\n",
    "\n",
    "class LithologyTransformer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, d_model, nhead,\n",
    "                 dim_feedforward, norm_first, num_encoder_layers, num_decoder_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_embedding = nn.Linear(input_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(1+output_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
    "                                          dim_feedforward, activation='gelu', batch_first=True, norm_first=norm_first)\n",
    "\n",
    "        self.classifier = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.positional_encoding(self.encoder_embedding(src))\n",
    "        tgt = self.positional_encoding(self.decoder_embedding(tgt))\n",
    "\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.shape[1]).to(device)\n",
    "        outputs = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "\n",
    "        return self.classifier(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0ZC9Zy9-Fvt",
    "outputId": "2017d61b-1f64-49e7-ddca-85a2e0320b38"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "d_model = 8\n",
    "batch_size = 1024\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True, shuffle=False)\n",
    "\n",
    "model = LithologyTransformer(input_size=4, output_size=12, d_model=d_model, nhead=4, dim_feedforward=16,\n",
    "                             norm_first=False, num_encoder_layers=1, num_decoder_layers=1)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad == True)\n",
    "print('Número de parâmetros do nosso modelo:', n_params)\n",
    "\n",
    "print('\\nComponentes do modelo final:')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ik8VoOu6-Fvt"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Shiftando o target para a direita (inserção do token <sos>)\n",
    "        sos = torch.full((batch_size, 1), fill_value=12).to(device)\n",
    "        tgt = torch.cat([sos, y], dim=1)\n",
    "\n",
    "        outputs = model(X, tgt)\n",
    "        outputs = outputs[:, :-1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs.transpose(1, 2), y)  # a loss function espera que a saída do modelo seja (batch_size, out_size, seq_lengh)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Batch [{batch+1}/{len(dataloader)}] -> batch loss: {loss.item():.5f}')\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f18934890476496da8a9c51de5093dd0"
     ]
    },
    "id": "2Nh6NzxR-Fvt",
    "outputId": "41842698-a2c5-486e-848d-f800a5a48c7d"
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = train_epoch(model, train_dataloader, optimizer, criterion)\n",
    "\n",
    "    print(f'Epoch [{epoch}/{num_epochs}] -> mean epoch loss: {epoch_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IynsZDLr-Fvu"
   },
   "source": [
    "### Avaliando a performance da rede em dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DW_xdueC-Fvu"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    predictions = torch.zeros(len(dataloader), batch_size, window_size).to(device)\n",
    "    labels = torch.zeros_like(predictions)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            preds = torch.full((batch_size, 1), fill_value=12).to(device)\n",
    "            for i in range(X.shape[1]):\n",
    "                outputs = model(X, preds)\n",
    "                last_pred = outputs.argmax(dim=-1)[:, -1].unsqueeze(1)\n",
    "\n",
    "                preds = torch.cat([preds, last_pred], dim=1)\n",
    "\n",
    "            loss = criterion(outputs.transpose(1, 2), y)  # a loss function espera que a saída do modelo seja (batch_size, out_size, seq_lengh)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions[batch] = outputs.argmax(dim=-1)\n",
    "            labels[batch] = y\n",
    "\n",
    "    total_loss /= len(dataloader)\n",
    "    return total_loss, predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "028a159ca52a4575b9dcac44bd4ba7cc"
     ]
    },
    "id": "i7pMqUwc-Fvu",
    "outputId": "51109a74-7d8f-4e9a-f3a8-0749884733f3"
   },
   "outputs": [],
   "source": [
    "total_loss, predictions, labels = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "print(f'Mean hidden test loss: {total_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIif8Nez-Fvu",
    "outputId": "8316c685-484d-4083-dce1-b538639855a5"
   },
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy, Precision, Recall, ConfusionMatrix\n",
    "\n",
    "acc = Accuracy(task = 'multiclass', num_classes = 12).to(device)\n",
    "prec = Precision(task = 'multiclass', average='macro', num_classes = 12).to(device)\n",
    "recall = Recall(task = 'multiclass', average='macro', num_classes = 12).to(device)\n",
    "\n",
    "print(f'Acurácia: {(100*acc(labels, predictions)):.2f}%')\n",
    "print(f'Precisão: {(100*prec(labels, predictions)):.2f}%')\n",
    "print(f'Recall: {(100*recall(labels, predictions)):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlnVul-F-Fvu"
   },
   "source": [
    "Computando uma matriz de confusão para verificarmos a qualidade do nosso modelo nos dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcDJBLQ0-Fvu",
    "outputId": "7c77593c-a8a7-4de9-f94f-838f6a19b95d"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix = ConfusionMatrix(task = 'multiclass', num_classes = 12).to(device)\n",
    "cm = confusion_matrix(labels, predictions).cpu()\n",
    "cm = (cm.float() / cm.sum(axis=1)[:, np.newaxis]).nan_to_num()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt='.2f', xticklabels=lithology_keys.values(), yticklabels=lithology_keys.values())\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show(block=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
