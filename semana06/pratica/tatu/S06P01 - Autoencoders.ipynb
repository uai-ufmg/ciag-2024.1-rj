{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6X5dpwxWudb"
   },
   "source": [
    "# Autoencoders - Aula Prática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JnJrF6xW0du"
   },
   "source": [
    "## Configurações\n",
    "\n",
    "Importando módulos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2_e6wi4Uwco"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.v2 as T\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from math import ceil, sqrt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cx6mX-6IW1ix"
   },
   "source": [
    "## Introdução\n",
    "\n",
    "Autoencoders são modelos que aprendem a representação de dados de entrada. Eles são compostos por duas partes: um encoder e um decoder. O encoder é responsável por mapear a entrada para um espaço latente, enquanto o decoder é responsável por mapear o espaço latente para a saída. A ideia é que o espaço latente seja uma representação mais compacta e significativa dos dados de entrada.\n",
    "\n",
    "Nesta aula prática, vamos implementar um autoencoder para reconstruir imagens do dataset MNIST e entender como esse tipo de modelo funciona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Vamos começar carregando o dataset MNIST. O dataset é composto por imagens de dígitos escritos à mão, com dimensões 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_path = '/pgeoprj2/ciag2024/dados'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(dataset, n=16, figsize=(6, 6)):\n",
    "    cols = ceil(sqrt(n))\n",
    "    rows = ceil(n / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i == 0:\n",
    "            print(f\"image shape: {dataset[i][0].shape}\")\n",
    "        image, _, label = dataset[i]\n",
    "        ax.imshow(image.squeeze(), cmap=\"gray\")  # Squeeze removes the channel dimension\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Como queremos que o autoencoder aprenda a reconstruir a imagem de entrada,\n",
    "reformulamos o dataset MNIST para que ele retorne a mesma imagem como entrada e saída.\n",
    "\"\"\"\n",
    "class AutoencoderMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, input_transform=None, target_transform=None, download=False, return_labels=False):\n",
    "        self.dataset = datasets.MNIST(root=root, train=train, transform=transform, download=download)\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "        self.return_labels = return_labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, label = self.dataset[idx]\n",
    "        y = x.clone()\n",
    "\n",
    "        if self.input_transform:\n",
    "            x = self.input_transform(x)\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "\n",
    "        return (x, y, label) if self.return_labels else (x, y)\n",
    "    \n",
    "    def start_return_labels(self):\n",
    "        self.return_labels = True\n",
    "    \n",
    "    def stop_return_labels(self):\n",
    "        self.return_labels = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmoDSajsUybO"
   },
   "outputs": [],
   "source": [
    "mnist_train_ds = AutoencoderMNIST(\n",
    "    mnist_path,\n",
    "    train=True,\n",
    "    transform=T.Compose([\n",
    "        T.ToImage(),\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "    ]),\n",
    "    download=False,\n",
    "    return_labels=True\n",
    ")\n",
    "mnist_test_ds = AutoencoderMNIST(\n",
    "    mnist_path,\n",
    "    train=False,\n",
    "    transform=T.Compose([\n",
    "        T.ToImage(),\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Tamanho do dataset de (treino, teste):\", (len(mnist_train_ds), len(mnist_test_ds))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "qfl9PnbqVDxx",
    "outputId": "03dc022a-11b4-4f70-a714-f0dc97ee4605"
   },
   "outputs": [],
   "source": [
    "plot_dataset(mnist_train_ds, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementando o autoencoder\n",
    "\n",
    "Vamos implementar um autoencoder convolucional simples, com 3 camadas convolucionais no encoder e 3 camadas convolucionais no decoder. A arquitetura do autoencoder é a seguinte:\n",
    "\n",
    "- Encoder:\n",
    "    - Conv2D com 16 filtros, kernel de tamanho 3x3, função de ativação ReLU\n",
    "    - MaxPooling2D com pool size 2x2\n",
    "    - Conv2D com 8 filtros, kernel de tamanho 3x3, função de ativação ReLU\n",
    "    - MaxPooling2D com pool size 2x2\n",
    "    - Conv2D com 2 filtros, kernel de tamanho 3x3, função de ativação ReLU\n",
    "    - Camada Linear que mapeia os features maps para o espaço latente\n",
    "\n",
    "- Decoder:\n",
    "    - Camada Linear que mapeia o espaço latente para os features maps\n",
    "    - Conv2D com 128 filtros, kernel de tamanho 3x3, função de ativação ReLU\n",
    "    - UpSampling2D com size 2x2\n",
    "    - Conv2D com 64 filtros, kernel de tamanho 3x3, função de ativação ReLU\n",
    "    - UpSampling2D com size 2x2\n",
    "    - Conv2D com 32 filtros, kernel de tamanho 3x3, função de ativação ReLU\n",
    "    - Conv2D com 1 filtro, kernel de tamanho 3x3, função de ativação sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tamanho do espaço latente é 2 para que possamos visualizar a representação dos dados no espaço latente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FY6YKp4YPXZB"
   },
   "outputs": [],
   "source": [
    "class ConvolutionalEncoder(nn.Module):\n",
    "    def __init__(self, latent_size=2, n_channels=3):\n",
    "        super(ConvolutionalEncoder, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # reduzimos o tamanho da imagem\n",
    "        # considerando input (n_channels, 28, 28)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 16, 3, padding=1), # (16, 28, 28)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2), # (16, 14, 14)\n",
    "            nn.Conv2d(16, 8, 3, padding=1), # (8, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),  # (8, 7, 7)\n",
    "            nn.Conv2d(8, 2, 3, padding=1), # (2, 7, 7)\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # extraimos as features da imagem reduzida\n",
    "        self.flatten = nn.Flatten() # (2 * 7 * 7,) = (98,)\n",
    "\n",
    "        # finalmente fazemos o encoding da nossa imagem em um vetor n dimensional\n",
    "        self.encoder = nn.Linear(98, self.latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8aUMrij6SNtd"
   },
   "outputs": [],
   "source": [
    "class ConvolutionalDecoder(nn.Module):\n",
    "    def __init__(self, latent_size=2, n_channels=3):\n",
    "        super(ConvolutionalDecoder, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # decodificamos a representação latente para uma imagem 7x7\n",
    "        self.decoder = nn.Linear(self.latent_size, 98)\n",
    "        self.unflatten = nn.Unflatten(1, (2, 7, 7)) # (2, 7, 7)\n",
    "\n",
    "        # voltamos a imagem ao tamanho original\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(2, 8, 3, padding=1), # (8, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2), # (8, 14, 14)\n",
    "            nn.Conv2d(8, 16, 3, padding=1), # (16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2), # (16, 28, 28)\n",
    "            nn.Conv2d(16, n_channels, 3, padding=1), # (n_channels, 28, 28)\n",
    "            nn.Sigmoid(), # sigmoid na última camada para que o output esteja no intervalo [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.conv_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TYs8vqYOT3S9"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_size=2, n_channels=3):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = ConvolutionalEncoder(latent_size, n_channels)\n",
    "        self.decoder = ConvolutionalDecoder(latent_size, n_channels)\n",
    "\n",
    "    def forward(self, x, return_latent=False):\n",
    "        latent = self.encoder(x)\n",
    "        x = self.decoder(latent)\n",
    "\n",
    "        return (x, latent) if return_latent else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(latent_size=2, n_channels=1)\n",
    "autoencoder.cuda()\n",
    "summary(autoencoder, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o autoencoder\n",
    "\n",
    "Vamos treinar o autoencoder para reconstruir imagens do dataset MNIST. O autoencoder será treinado por 20 épocas, com batch size 128 e otimizador Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ot-l2kzIVxoV",
    "outputId": "92352a33-63fb-403e-9951-2e066b1ccfa2"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader=None, device='cpu', epochs=32):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i, (inputs, targets) in (pbar := tqdm(enumerate(train_loader), total=len(train_loader), unit='batch')):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Zero the gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # A loss é computada contra o input pois queremos reconstruir a imagem de entrada\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss /= inputs.size(0)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}/{epochs} - Loss: {train_loss/(i+1):.4f}\")\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_fn(outputs, targets)\n",
    "                    loss /= inputs.size(0)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}: Train Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_ds.stop_return_labels()\n",
    "mnist_test_ds.stop_return_labels()\n",
    "train_dl = DataLoader(mnist_train_ds, batch_size=128, shuffle=True)\n",
    "test_dl = DataLoader(mnist_test_ds, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(latent_size=2, n_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVPRhneBWjkn"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train(autoencoder, optimizer, criterion, train_dl, test_dl, device=device, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando os resultados\n",
    "\n",
    "Nessa etapa, vamos analisar os resultados obtidos pelo autoencoder. Vamos visualizar as imagens de entrada e as imagens reconstruídas pelo autoencoder. Além disso, vamos visualizar a representação dos dados no espaço latente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructions(model, dataset, n=8, device='cpu', figsize=(12, 6), plot_label=False):\n",
    "    fig, axes = plt.subplots(3 if plot_label else 2, n, figsize=figsize)\n",
    "    images = torch.cat([dataset[i][0].unsqueeze(0) for i in range(n)], dim=0).to(device)\n",
    "    labels = torch.cat([dataset[i][1].unsqueeze(0) for i in range(n)], dim=0)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructions = model(images)\n",
    "\n",
    "    for i, (ax, im, rec, lbl) in enumerate(zip(range(n), images, reconstructions, labels)):\n",
    "        im = im.cpu().squeeze()\n",
    "        rec = rec.cpu().squeeze()\n",
    "        lbl = lbl.cpu().squeeze()\n",
    "\n",
    "        if plot_label:\n",
    "            axes[0][ax].imshow(lbl, cmap=\"gray\")\n",
    "            axes[0][ax].axis(\"off\")\n",
    "            axes[1][ax].imshow(im, cmap=\"gray\")\n",
    "            axes[1][ax].axis(\"off\")\n",
    "            axes[2][ax].imshow(rec, cmap=\"gray\")\n",
    "            axes[2][ax].axis(\"off\")\n",
    "        else:\n",
    "            axes[0][ax].imshow(im, cmap=\"gray\")\n",
    "            axes[0][ax].axis(\"off\")\n",
    "            axes[1][ax].imshow(rec, cmap=\"gray\")\n",
    "            axes[1][ax].axis(\"off\")\n",
    "\n",
    "        if i == n//2:\n",
    "            if plot_label:\n",
    "                axes[0][ax].set_title(\"Original\")\n",
    "                axes[1][ax].set_title(\"Sample\")\n",
    "                axes[2][ax].set_title(\"Reconstruction\")\n",
    "            else:\n",
    "                axes[0][ax].set_title(\"Sample\")\n",
    "                axes[1][ax].set_title(\"Reconstruction\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstructions(autoencoder, mnist_test_ds, n=9, device=device, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space(encoder, dataset, n_classes=10, batch_size=128, device='cpu', reduce_dims=None):\n",
    "    # Get the latent representations for all test data\n",
    "    latent_representations = []\n",
    "    labels_agg = []\n",
    "    classes = list(range(n_classes))\n",
    "\n",
    "    dataset.start_return_labels()\n",
    "    test_dl_with_labels = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    encoder.to(device)\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, _, labels in test_dl_with_labels:\n",
    "            images = images.to(device)\n",
    "            latent_vectors = encoder(images)\n",
    "            latent_representations.append(latent_vectors.cpu().numpy())\n",
    "            labels_agg.append(labels.numpy())\n",
    "\n",
    "    # Concatenate all latent representations\n",
    "    latent_representations = np.concatenate(latent_representations, axis=0)\n",
    "    labels_agg = np.concatenate(labels_agg, axis=0)\n",
    "    \n",
    "    if reduce_dims is not None:\n",
    "        if reduce_dims == 'pca':\n",
    "            pca = PCA(n_components=2, random_state=42)\n",
    "            latent_representations = pca.fit_transform(latent_representations)\n",
    "        elif reduce_dims == 'tsne':\n",
    "            tsne = TSNE(n_components=2, random_state=42)\n",
    "            latent_representations = tsne.fit_transform(latent_representations)\n",
    "\n",
    "    # Plot the latent representations\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cmap = plt.get_cmap('jet', 10)\n",
    "    for c in classes:\n",
    "        indices = np.where(labels_agg == c)\n",
    "        plt.scatter(\n",
    "            latent_representations[indices, 0],\n",
    "            latent_representations[indices, 1],\n",
    "            s=3,\n",
    "            label=f\"{c}\",\n",
    "            c=[cmap(c)],\n",
    "            alpha=0.5,\n",
    "        )\n",
    "    # add the labels mapping\n",
    "    plt.legend(markerscale=5)\n",
    "    plt.title('Latent Representations of Test Data')\n",
    "    plt.xlabel('Latent Dimension 1')\n",
    "    plt.ylabel('Latent Dimension 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(autoencoder.encoder, mnist_test_ds, reduce_dims=None, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que os digitos estão organizados no espaço latente de acordo com suas características. Por exemplo, os digitos 0 e 1 estão distantes, enquanto os digitos 3 e 8 estão juntos. Isso mostra que o autoencoder aprendeu a representação dos dados de forma significativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1 - Denoising Autoencoder\n",
    "\n",
    "Tendo em vista que o autoencoder é capaz de aprender a representação dos dados, podemos utilizá-lo para remover ruídos das imagens. Implemente um denoising autoencoder, que recebe imagens com ruídos como entrada e tenta reconstruir a imagem original.\n",
    "\n",
    "<details>\n",
    "<summary>Dica:</summary>\n",
    "Você só vai precisar de mexer no dataset de entrada, adicionando ruídos às imagens.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2 - Removendo Marcas d'água\n",
    "\n",
    "Agora, vamos aplicar o denoising autoencoder para remover marcas d'água de imagens. Para isso, vamos adicionar marcas d'água às imagens do dataset MNIST e treinar o denoising autoencoder para remover essas marcas d'água."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Watermark(object):\n",
    "    def __init__(self, watermark, clamp=True):\n",
    "        self.watermark = watermark\n",
    "        self.w_width, self.w_height = watermark.shape[-2:]\n",
    "        self.clamp = clamp\n",
    "\n",
    "    def __call__(self, image):\n",
    "        width, height = image.shape[-2:]\n",
    "        x_offset = torch.randint(0, width - self.w_width, (1,)).item()\n",
    "        y_offset = torch.randint(0, height - self.w_height, (1,)).item()\n",
    "        im_patch = image[..., x_offset:x_offset+self.w_width, y_offset:y_offset+self.w_height]\n",
    "        im_patch += self.watermark.unsqueeze(0)\n",
    "        if self.clamp:\n",
    "            image = torch.clamp(image, 0, 1)\n",
    "        return image\n",
    "\n",
    "# Se quiser, pode alterar o que será exibido na marca d'água\n",
    "ciag_watermark = [\n",
    "    [1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2 - Autoencoder com regularização\n",
    "\n",
    "Implemente um autoencoder com regularização no espaço latente. Adicione uma regularização L1 ou L2 no espaço latente e treine o autoencoder.\n",
    "\n",
    "<details>\n",
    "<summary>Dica:</summary>\n",
    "Altere a função de treino para aceitar mais um parâmetro, que será a regularização no espaço latente.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
