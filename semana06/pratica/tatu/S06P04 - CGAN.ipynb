{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CGAN - Aula Prática\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurações\n",
    "\n",
    "Importando módulos necessários e baixando os datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder, CIFAR10\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.v2.functional as K\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "GANs (Generative Adversarial Networks) são uma arquitetura de rede neural que tem como objetivo gerar novos dados a partir de um conjunto de dados de treinamento. A ideia é que a rede neural seja capaz de aprender a distribuição dos dados de treinamento e, a partir disso, gerar novos dados que sejam semelhantes aos dados de treinamento. Neste notebook, vamos implementar uma GAN para gerar imagens de personagens de anime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "O dataset que vamos utilizar é o [CiFAR10](https://pytorch.org/vision/main/generated/torchvision.datasets.CIFAR10.html) que é composto por 60.000 imagens coloridas de 32x32 pixels divididas em 10 classes. As classes são: 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship' e 'truck'.\n",
    "\n",
    "Esse dataset é muito utilizado para treinar modelos de redes neurais convolucionais, mas também pode ser utilizado para treinar GANs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_path = '/pgeoprj2/ciag2024/dados'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot samples and metadata of a dataset\n",
    "def plot_dataset(\n",
    "    dataset,\n",
    "    n_rows=4,\n",
    "    n_cols=4,\n",
    "    figsize=(6, 6),\n",
    "    denormalize=False,\n",
    "    label_decoder=lambda x: [x],\n",
    "):\n",
    "    print(f'Number of samples: {len(dataset)}')\n",
    "    print(f'Sample shape: {dataset[0][0].shape}')\n",
    "\n",
    "    samples = np.random.randint(0, len(dataset), size=(n_rows, n_cols))\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            image, label = dataset[samples[i, j]]\n",
    "            label = label_decoder(label)[0]\n",
    "            if denormalize:\n",
    "                image = K.normalize(image, -1, 2)\n",
    "                image = image.clamp(0, 1)\n",
    "            image = image.permute(1, 2, 0)\n",
    "            im = axes[i, j].imshow(image)\n",
    "            if image.shape[2] == 1:\n",
    "                im.set_cmap('gray')\n",
    "            axes[i, j].set_title(label)\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = CIFAR10(\n",
    "    root=cifar10_path,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToImage(),\n",
    "            transforms.ToDtype(torch.float32, scale=True),\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLabelMapper(object):\n",
    "    def __init__(self, dataset):\n",
    "        self.mapping = {\n",
    "            i: label for i, label in enumerate(dataset.classes)\n",
    "        }\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(list(self.mapping.values()))\n",
    "        # make sure the classes are in order\n",
    "        self.label_encoder.classes_ = np.array(list(self.mapping.values()))\n",
    "\n",
    "    def __getitem__(self, label):\n",
    "        return self.mapping[label]\n",
    "\n",
    "    def __call__(self, label):\n",
    "        return self.label_encoder.inverse_transform(\n",
    "            [label] if isinstance(label, int) else label\n",
    "        )\n",
    "\n",
    "\n",
    "label_mapper = DatasetLabelMapper(cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(\n",
    "    cifar10,\n",
    "    n_cols=6,\n",
    "    n_rows=6,\n",
    "    denormalize=True,\n",
    "    figsize=(8, 9),\n",
    "    label_decoder=label_mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo\n",
    "\n",
    "A CGAN (Conditional Generative Adversarial Network) é uma extensão da GAN que permite a geração de imagens condicionadas a uma label. A ideia é que a rede geradora receba uma label como entrada e gere uma imagem correspondente a essa label. A rede discriminadora, por sua vez, recebe a imagem gerada e a label correspondente e tem como objetivo distinguir entre imagens reais e imagens geradas.\n",
    "\n",
    "A arquitetura da CGAN é semelhante à arquitetura da GAN, mas com a adição de uma label como entrada. A label é concatenada com a entrada da rede geradora e com a entrada da rede discriminadora. Como podemos ver na imagem abaixo:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"CGAN\" style=\"width:70%\" src=\"data:image/jpeg;base64,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\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerador\n",
    "\n",
    "O gerador é uma rede neural que recebe um vetor de valores aleatórios como entrada (a semente da geração) e gera uma imagem como saída. A ideia é que o gerador aprenda a mapear vetores de números aleatórios para imagens que sejam semelhantes às imagens de treinamento. O gerador é treinado para enganar o discriminador, ou seja, para gerar imagens que o discriminador classifica como reais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(\n",
    "    generator,\n",
    "    num_images=64,\n",
    "    num_classes=10,\n",
    "    latent_dim=128,\n",
    "    figsize=(6, 6),\n",
    "    device='cpu',\n",
    "    denormalize=False,\n",
    "    label_decoder=lambda x: x,\n",
    "):\n",
    "\n",
    "    # generator.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        labels = torch.randint(0, num_classes, (num_images,)).to(device)\n",
    "        noise = torch.randn(num_images, latent_dim, 1, 1).to(device)\n",
    "        generated_images = generator(labels, noise).cpu()\n",
    "\n",
    "    labels = label_decoder(labels.cpu())\n",
    "\n",
    "    generated_images = generated_images.permute(0, 2, 3, 1)\n",
    "    if denormalize:\n",
    "        generated_images = K.normalize(generated_images, -1, 2)  # Normalize to (0, 1)\n",
    "\n",
    "    n_rows = int(math.sqrt(num_images))\n",
    "    n_cols = int(math.sqrt(num_images))\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            im = axes[i, j].imshow(generated_images[i * n_cols + j])\n",
    "            if generated_images[i * n_cols + j].shape[-1] == 1:\n",
    "                im.set_cmap('gray')\n",
    "            axes[i, j].set_title(labels[i * n_cols + j].item())\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size=128, n_classes=10, output_dim=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # latent_size is the size of the input noise vector\n",
    "        self.latent_size = latent_size\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(n_classes, latent_size), \n",
    "            nn.Unflatten(1, (latent_size, 1, 1))\n",
    "        )\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_size*2, 512, kernel_size=4, stride=1, bias=False),  # (512, 4, 4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),  # (256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),  # (128, 16, 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),  # (64, 32, 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(128, output_dim, kernel_size=3, stride=1, padding=1, bias=False),  # (output_dim, 32, 32)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, y, x=None, batch_size=1):\n",
    "        if x is None:\n",
    "            device = next(self.parameters()).device\n",
    "            x = torch.randn(batch_size, self.latent_size, 1, 1, device=device)\n",
    "        elif batch_size > 1:\n",
    "            print('Warning: changing batch_size with x is not supported, the batch size will be the size of x')\n",
    "        \n",
    "        y = self.embedding(y)\n",
    "        \n",
    "        return self.net(torch.cat([x, y], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(output_dim=3)\n",
    "generator.to('cuda')\n",
    "labels = torch.randint(0, 10, (256,), device='cuda')\n",
    "noise = torch.randn(256, 128, 1, 1, device='cuda')\n",
    "generator(labels, batch_size=256).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_generated_images(\n",
    "    generator,\n",
    "    device='cuda',\n",
    "    num_images=36,\n",
    "    denormalize=True,\n",
    "    figsize=(8, 9),\n",
    "    label_decoder=label_mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminador\n",
    "\n",
    "O discriminador é uma rede neural que recebe uma imagem como entrada e classifica se a imagem é real (vinda do conjunto de treinamento) ou falsa (gerada). O discriminador é treinado para distinguir entre imagens reais e falsas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, n_classes=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(n_classes, 32*32),\n",
    "            nn.Unflatten(1, (1, 32, 32))\n",
    "        )\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # +1 for the label channel\n",
    "            nn.Conv2d(in_channels+1, 64, kernel_size=4, stride=2, padding=1, bias=False), # (64, 16, 16)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),  # (128, 8, 8)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),  # (256, 4, 4)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 1, kernel_size=4, bias=False),  # (1, 1, 1)\n",
    "            nn.Flatten(),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = self.embedding(y)\n",
    "        return self.net(torch.cat([x, y], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "discriminator.to('cuda')\n",
    "discriminator(\n",
    "    torch.randn(256, 3, 32, 32, device='cuda'), torch.randint(0, 10, (256,), device='cuda')\n",
    ").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo\n",
    "\n",
    "Vamos treinar o modelo para reconstruir imagens do dataset. O modelo será treinado por 32 épocas, com batch size 128 e otimizador Adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    g_optimizer,\n",
    "    d_optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    num_classes=10,\n",
    "    epochs=24,\n",
    "    plot_frequency=None,\n",
    "    label_mapper=lambda x: x,\n",
    "):\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    g_losses_train = []\n",
    "    d_losses_train = []\n",
    "    d_accuracies_train = []\n",
    "    real_scores_train = []\n",
    "    fake_scores_train = []\n",
    "\n",
    "    if val_loader is not None:\n",
    "        g_losses_val = []\n",
    "        d_losses_val = []\n",
    "        d_accuracies_val = []\n",
    "        real_scores_val = []\n",
    "        fake_scores_val = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        g_loss_train = 0.0\n",
    "        d_loss_train = 0.0\n",
    "        d_batch_accuracy = 0.0\n",
    "        real_score = 0.0\n",
    "        fake_score = 0.0\n",
    "\n",
    "        for i, (images, labels) in (\n",
    "            pbar := tqdm(enumerate(train_loader), total=len(train_loader), unit='batch')\n",
    "        ):\n",
    "            real_images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            batch_size = real_images.size(0)\n",
    "\n",
    "            # Train discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "\n",
    "            real_outputs = discriminator(real_images, labels)\n",
    "            real_labels = torch.ones_like(real_outputs, device=device)\n",
    "            real_loss = criterion(real_outputs, real_labels)\n",
    "\n",
    "            noise = torch.randn(batch_size, generator.latent_size, 1, 1, device=device)\n",
    "            fake_images = generator(labels, noise)\n",
    "            fake_outputs = discriminator(fake_images.detach(), labels)\n",
    "            fake_labels = torch.zeros_like(fake_outputs, device=device)\n",
    "            fake_loss = criterion(fake_outputs, fake_labels)\n",
    "\n",
    "            batch_accuracy = (\n",
    "                torch.mean(real_outputs >= 0.5, dtype=torch.float32)\n",
    "                + torch.mean(fake_outputs < 0.5, dtype=torch.float32)\n",
    "            ) / 2\n",
    "            d_batch_accuracy += batch_accuracy.item()\n",
    "\n",
    "            d_loss = real_loss + fake_loss\n",
    "            d_loss_train += d_loss.item()\n",
    "            real_score += real_outputs.mean().item()\n",
    "            fake_score += fake_outputs.mean().item()\n",
    "\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Train generator\n",
    "            g_optimizer.zero_grad()\n",
    "\n",
    "            noise = torch.randn(batch_size, generator.latent_size, 1, 1, device=device)\n",
    "            fake_images = generator(labels, noise)\n",
    "            fake_outputs = discriminator(fake_images, labels)\n",
    "\n",
    "            g_loss = criterion(fake_outputs, real_labels)\n",
    "            g_loss_train += g_loss.item()\n",
    "\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            pbar.set_description(\n",
    "                f'Epoch {epoch+1}/{epochs} G_Loss: {g_loss_train/((i+1)):.4f} D_Loss: {d_loss_train/(i+1):.4f}'\n",
    "            )\n",
    "\n",
    "        g_loss_train /= len(train_loader)\n",
    "        d_loss_train /= len(train_loader)\n",
    "        d_accuracy_train = d_batch_accuracy / (len(train_loader))\n",
    "        real_score /= len(train_loader)\n",
    "        fake_score /= len(train_loader)\n",
    "\n",
    "        g_losses_train.append(g_loss_train)\n",
    "        d_losses_train.append(d_loss_train)\n",
    "        d_accuracies_train.append(d_accuracy_train)\n",
    "        real_scores_train.append(real_score)\n",
    "        fake_scores_train.append(fake_score)\n",
    "\n",
    "        if val_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                generator.eval()\n",
    "                discriminator.eval()\n",
    "\n",
    "                g_loss_val = 0.0\n",
    "                d_loss_val = 0.0\n",
    "                d_batch_accuracy = 0.0\n",
    "                real_score = 0.0\n",
    "                fake_score = 0.0\n",
    "\n",
    "                for images, labels in val_loader:\n",
    "                    real_images = images.to(device, non_blocking=True)\n",
    "                    labels = labels.to(device, non_blocking=True)\n",
    "                    batch_size = real_images.size(0)\n",
    "\n",
    "                    # Evaluate discriminator\n",
    "\n",
    "                    real_outputs = discriminator(real_images, labels)\n",
    "                    real_labels = torch.ones_like(real_outputs, device=device)\n",
    "                    real_loss = criterion(real_outputs, real_labels)\n",
    "\n",
    "                    # Generate fake images\n",
    "                    noise = torch.randn(\n",
    "                        batch_size, generator.latent_size, 1, 1, device=device\n",
    "                    )\n",
    "                    fake_images = generator(labels, noise)\n",
    "\n",
    "                    fake_outputs = discriminator(fake_images, labels)\n",
    "                    fake_labels = torch.zeros_like(fake_outputs, device=device)\n",
    "                    fake_loss = criterion(fake_outputs, fake_labels)\n",
    "\n",
    "                    batch_accuracy = (\n",
    "                        torch.mean(real_outputs >= 0.5, dtype=torch.float32)\n",
    "                        + torch.mean(fake_outputs < 0.5, dtype=torch.float32)\n",
    "                    ) / 2\n",
    "                    d_batch_accuracy += batch_accuracy.item()\n",
    "\n",
    "                    d_loss = real_loss + fake_loss\n",
    "                    d_loss_val += d_loss.item()\n",
    "                    real_score += real_outputs.mean().item()\n",
    "                    fake_score += 1 - fake_outputs.mean().item()\n",
    "\n",
    "                    # Evaluate generator\n",
    "\n",
    "                    # reuse fake_images and fake_outputs to save computation\n",
    "                    g_loss = criterion(fake_outputs, real_labels)\n",
    "\n",
    "                    g_loss_val += g_loss.item()\n",
    "\n",
    "                g_loss_val /= len(val_loader)\n",
    "                d_loss_val /= len(val_loader)\n",
    "                d_accuracy_val = d_batch_accuracy / len(val_loader)\n",
    "                real_score /= len(val_loader)\n",
    "                fake_score /= len(val_loader)\n",
    "\n",
    "                g_losses_val.append(g_loss_val)\n",
    "                d_losses_val.append(d_loss_val)\n",
    "                d_accuracies_val.append(d_accuracy_val)\n",
    "                real_scores_val.append(real_score)\n",
    "                fake_scores_val.append(fake_score)\n",
    "        print(\n",
    "            f'[Epoch {epoch+1}/{epochs}]\\n\\tG_Trn_Loss: {g_loss_train:.4f}\\tD_Trn_Loss: {d_loss_train:.4f}\\tD_Trn_Real: {real_score:.4f}\\tD_Trn_Fake: {fake_score:.4f}\\tD_Trn_Acc: {d_accuracy_train*100:.2f}%',\n",
    "            (\n",
    "                ''\n",
    "                if val_loader is None\n",
    "                else f'\\n\\tG_Val_Loss: {g_loss_val:.4f}\\tD_Val_Loss: {d_loss_val:.4f}\\tD_Val_Real: {real_score:.4f}\\tD_Val_Fake: {fake_score:.4f}\\tD_Val_Acc: {d_accuracy_val*100:.2f}%'\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if plot_frequency is not None and (epoch + 1) % plot_frequency == 0:\n",
    "            plot_generated_images(\n",
    "                generator, num_images=25, num_classes=num_classes, device=device, denormalize=True, label_decoder=label_mapper, figsize=(6, 7)\n",
    "            )\n",
    "\n",
    "    train_stats = {\n",
    "        'g_losses': g_losses_train,\n",
    "        'd_losses': d_losses_train,\n",
    "        'd_accuracies': d_accuracies_train,\n",
    "        'real_scores': real_scores_train,\n",
    "        'fake_scores': fake_scores_train,\n",
    "    }\n",
    "\n",
    "    if val_loader is not None:\n",
    "        val_stats = {\n",
    "            'g_losses': g_losses_val,\n",
    "            'd_losses': d_losses_val,\n",
    "            'd_accuracies': d_accuracies_val,\n",
    "            'real_scores': real_scores_val,\n",
    "            'fake_scores': fake_scores_val,\n",
    "        }\n",
    "\n",
    "        return train_stats, val_stats\n",
    "\n",
    "    return train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10(\n",
    "    root=cifar10_path,\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToImage(),\n",
    "            transforms.ToDtype(torch.float32, scale=True),\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "val_dataset = CIFAR10(\n",
    "    root=cifar10_path,\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToImage(),\n",
    "            transforms.ToDtype(torch.float32, scale=True),\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=128, shuffle=True, num_workers=1, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=128, shuffle=False, num_workers=1, pin_memory=True\n",
    ")\n",
    "\n",
    "plot_dataset(train_dataset, n_cols=6, n_rows=6, denormalize=True, figsize=(8, 9), label_decoder=label_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 64\n",
    "latent_dim = 128\n",
    "num_classes = 10\n",
    "dataset_dim = 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "generator = Generator(latent_dim, output_dim=dataset_dim, n_classes=num_classes)\n",
    "discriminator = Discriminator(in_channels=dataset_dim, n_classes=num_classes)\n",
    "\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(\n",
    "    discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999)\n",
    ")\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats, val_stats = train(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    g_optimizer,\n",
    "    d_optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    epochs=epochs,\n",
    "    plot_frequency=4,\n",
    "    label_mapper=label_mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_generated_images(\n",
    "    generator,\n",
    "    device=device,\n",
    "    num_images=36,\n",
    "    denormalize=True,\n",
    "    figsize=(8, 9),\n",
    "    label_decoder=label_mapper,\n",
    "    num_classes=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 1\n",
    "\n",
    "Vamos explorar a função do embedding no gerador. Experimente alterar o tamanho do embedding e o treine com o dataset CIFAR 10. Experimente também remover o embedding do gerador substituindo-o por um OneHotEncoding.\n",
    "\n",
    "<details>\n",
    "<summary>Dica:</summary>\n",
    "\n",
    "Inclua um parâmetro `embedding_size` no construtor do gerador. Utilize o parâmetro para definir o tamanho do embedding.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingGenerator(nn.Module):\n",
    "    def __init__(self, latent_size=128, embedding_size=128, n_classes=10, output_dim=3, arch='small'):\n",
    "        super(EmbeddingGenerator, self).__init__()\n",
    "\n",
    "        # latent_size is the size of the input noise vector\n",
    "        self.latent_size = latent_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # TODO: preencha a camada de embedding\n",
    "        self.embedding = nn.Sequential(\n",
    "            ...\n",
    "        )\n",
    "\n",
    "        # TODO: preencha a rede neural com a dimensão correta\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(..., 512, kernel_size=4, stride=1, bias=False),  # (512, 4, 4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),  # (256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),  # (128, 16, 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),  # (64, 32, 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "        ) \n",
    "        \n",
    "        # Ainda não se preocupe com essa parte, será utilizada no exercício 2\n",
    "        if arch == 'large':\n",
    "            ...  # (output_dim, 64, 64)\n",
    "        else:\n",
    "            self.net.append(nn.Conv2d(64, output_dim, kernel_size=3, stride=1, padding=1, bias=False))  # (output_dim, 32, 32)\n",
    "        \n",
    "        self.net.append(nn.Tanh())\n",
    "\n",
    "    def forward(self, y, x=None, batch_size=1):\n",
    "        if x is None:\n",
    "            device = next(self.parameters()).device\n",
    "            x = torch.randn(batch_size, self.latent_size, 1, 1, device=device)\n",
    "        elif batch_size > 1:\n",
    "            print('Warning: changing batch_size with x is not supported, the batch size will be the size of x')\n",
    "        \n",
    "        y = self.embedding(y)\n",
    "        \n",
    "        return self.net(torch.cat([x, y], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, n_classes=10, embedding_size=128, arch='small'):\n",
    "        super(EmbeddingDiscriminator, self).__init__()\n",
    "\n",
    "        # TODO: preencha a camada de embedding\n",
    "        self.embedding = nn.Sequential(\n",
    "            ...\n",
    "        )\n",
    "\n",
    "        # TODO: preencha a rede neural com a dimensionalidade correta\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(..., 64, kernel_size=4, stride=2, padding=1, bias=False), # (64, 32, 32) / (64, 16, 16)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),  # (128, 16, 16) / (128, 8, 8)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),  # (256, 8, 8) / (256, 4, 4)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        # Ainda não se preocupe com essa parte, será utilizada no exercício 2\n",
    "        if arch == 'large':\n",
    "            ... # (1, 1, 1)\n",
    "        else:\n",
    "            self.net.append(nn.Conv2d(256, 1, kernel_size=4, bias=False)) # (1, 1, 1)\n",
    "        \n",
    "        self.net.append(nn.Flatten())\n",
    "        self.net.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = self.embedding(y)\n",
    "        return self.net(torch.cat([x, y], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: preencha os hiperparâmetros\n",
    "# Dica: Teste tamanhos de embedding diferentes para ver o resultado\n",
    "epochs = ...\n",
    "latent_dim = ...\n",
    "num_classes = ...\n",
    "dataset_dim = ...\n",
    "embedding_size = ...\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "generator = EmbeddingGenerator(latent_dim, n_classes=num_classes, embedding_size=embedding_size, output_dim=dataset_dim, arch='small')\n",
    "discriminator = EmbeddingDiscriminator(in_channels=dataset_dim, n_classes=num_classes, embedding_size=embedding_size, arch='small')\n",
    "\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(\n",
    "    discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999)\n",
    ")\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats, val_stats = train(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    g_optimizer,\n",
    "    d_optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    epochs=epochs,\n",
    "    plot_frequency=4,\n",
    "    label_mapper=label_mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_generated_images(\n",
    "    generator,\n",
    "    device=device,\n",
    "    num_images=36,\n",
    "    denormalize=True,\n",
    "    figsize=(8, 9),\n",
    "    label_decoder=label_mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotGenerator(nn.Module):\n",
    "    def __init__(self, latent_size=128, n_classes=10, output_dim=3, arch='small'):\n",
    "        super(OneHotGenerator, self).__init__()\n",
    "\n",
    "        # latent_size is the size of the input noise vector\n",
    "        self.latent_size = latent_size\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # TODO: preencha a camada de embedding\n",
    "        # Lembre-se que essa camada deve retornar um vetor de n_classesx1x1 para ser concatenado com o vetor de ruído\n",
    "        self.embedding = nn.Sequential(\n",
    "            ... # (n_classes, 1, 1)\n",
    "        )\n",
    "\n",
    "        # TODO: preencha a rede neural com a dimensão correta\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(..., 512, kernel_size=4, stride=1, bias=False),  # (512, 4, 4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),  # (256, 8, 8)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),  # (128, 16, 16)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),  # (64, 32, 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "        ) \n",
    "        \n",
    "        # Ainda não se preocupe com essa parte, será utilizada no exercício 2\n",
    "        if arch == 'large':\n",
    "            ...  # (output_dim, 64, 64)\n",
    "        else:\n",
    "            self.net.append(nn.Conv2d(64, output_dim, kernel_size=3, stride=1, padding=1, bias=False))  # (output_dim, 32, 32)\n",
    "        \n",
    "        self.net.append(nn.Tanh())\n",
    "    \n",
    "    def forward(self, y, x=None, batch_size=1):\n",
    "        if x is None:\n",
    "            device = next(self.parameters()).device\n",
    "            x = torch.randn(batch_size, self.latent_size, 1, 1, device=device)\n",
    "        elif batch_size > 1:\n",
    "            print('Warning: changing batch_size with x is not supported, the batch size will be the size of x')\n",
    "\n",
    "        # TODO: Converta o vetor de classes em um vetor one-hot\n",
    "        y = ... # (batch_size, n_classes)\n",
    "        y = self.embedding(y) # (batch_size, n_classes, 1, 1)\n",
    "        \n",
    "        return self.net(torch.cat([x, y], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, n_classes=10, arch='small'):\n",
    "        super(OneHotDiscriminator, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # TODO: preencha a camada de embedding\n",
    "        # Lembre-se que essa camada deve retornar um vetor de 1xNxN para ser concatenado com a imagem\n",
    "        self.embedding = nn.Sequential(\n",
    "            ... # (1, N, N)\n",
    "        )\n",
    "\n",
    "        # TODO: preencha a rede neural com a dimensionalidade correta\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(..., 64, kernel_size=4, stride=2, padding=1, bias=False), # (64, 32, 32) / (64, 16, 16)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),  # (128, 16, 16) / (128, 8, 8)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),  # (256, 8, 8) / (256, 4, 4)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        # Ainda não se preocupe com essa parte, será utilizada no exercício 2\n",
    "        if arch == 'large':\n",
    "            ... # (1, 1, 1)\n",
    "        else:\n",
    "            self.net.append(nn.Conv2d(256, 1, kernel_size=4, bias=False)) # (1, 1, 1)\n",
    "        \n",
    "        self.net.append(nn.Flatten())\n",
    "        self.net.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # TODO: Converta o vetor de classes em um vetor one-hot\n",
    "        y = ... # (batch_size, n_classes)\n",
    "        y = self.embedding(y) # (batch_size, 1, N, N)\n",
    "\n",
    "        return self.net(torch.cat([x, y], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: preencha os hiperparâmetros\n",
    "epochs = ...\n",
    "latent_dim = ...\n",
    "num_classes = ...\n",
    "dataset_dim = ...\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "generator = OneHotGenerator(latent_dim, n_classes=num_classes, output_dim=dataset_dim, arch='small')\n",
    "discriminator = OneHotDiscriminator(in_channels=dataset_dim, n_classes=num_classes, arch='small')\n",
    "\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(\n",
    "    discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999)\n",
    ")\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats, val_stats = train(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    g_optimizer,\n",
    "    d_optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    epochs=epochs,\n",
    "    plot_frequency=4,\n",
    "    label_mapper=label_mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_generated_images(\n",
    "    generator,\n",
    "    device=device,\n",
    "    num_images=36,\n",
    "    denormalize=True,\n",
    "    figsize=(8, 9),\n",
    "    label_decoder=label_mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 2\n",
    "\n",
    "Treine uma GAN no dataset [Stanford Dogs Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/). O dataset contém imagens de Cachorros de 120 raças diferentes e suas respectivas anotações.\n",
    "\n",
    "<details>\n",
    "<summary>Dica:</summary>\n",
    "\n",
    "Utilize a classe ImageFolder do torchvision.datasets para carregar o dataset. O nome das pastas deve ser o nome da raça do cachorro.\n",
    "\n",
    "As imagens do dataset possuem tamanhos diferentes, você pode redimensionar as imagens para um tamanho fixo utilizando a classe Resize do torchvision.transforms.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs_path = '/pgeoprj2/ciag2024/dados/GAN/dogs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "O dataset contém imagens de cachorros de 120 raças diferentes. As imagens estão organizadas em pastas, onde o nome da pasta é o nome da raça do cachorro. Vamos importar o dataset e visualizar algumas imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Crie um dataset de imagens de cachorros\n",
    "# Lembre-se de normalizar as imagens para o intervalo [-1, 1]\n",
    "# O tamanho das imagens deve ser 64x64\n",
    "dogs_dataset = ImageFolder(\n",
    "    ...\n",
    ")\n",
    "\n",
    "dogs_dataset.classes = [c.split('-')[-1].replace('_', ' ') for c in dogs_dataset.classes]\n",
    "dogs_mapper = DatasetLabelMapper(dogs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(dogs_dataset, n_cols=6, n_rows=6, denormalize=True, figsize=(12, 12), label_decoder=dogs_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Crie um DataLoader para o dataset de cachorros\n",
    "# Separe o dataset em treino e validação, com 80% das imagens para treino\n",
    "train_dataset, val_dataset = random_split(\n",
    "    ...\n",
    ")\n",
    "\n",
    "# TODO: Crie os dataloaders para o treino e validação\n",
    "train_loader = DataLoader(\n",
    "    ...\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    ...\n",
    ")\n",
    "\n",
    "plot_dataset(train_dataset, n_cols=6, n_rows=6, denormalize=True, figsize=(12, 12), label_decoder=dogs_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importante:** Antes de seguir para o treinamento, retorne ao exercício 1 e altere as classes do gerador e discriminador para gerar e discriminar uma imagem 64x64, respectivamente. Já existe um parâmetro `arch` no construtor que pode ser utilizado para definir a arquitetura da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: preencha os hiperparâmetros\n",
    "epochs = ...\n",
    "latent_dim = ...\n",
    "num_classes = ...\n",
    "dataset_dim = ...\n",
    "embedding_size = ...\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "generator = EmbeddingGenerator(latent_dim, output_dim=dataset_dim, embedding_size=embedding_size, n_classes=num_classes, arch='large')\n",
    "discriminator = EmbeddingDiscriminator(in_channels=dataset_dim, embedding_size=embedding_size, n_classes=num_classes, arch='large')\n",
    "\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(\n",
    "    discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999)\n",
    ")\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_stats, d_stats = train(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    g_optimizer,\n",
    "    d_optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    num_classes=num_classes,\n",
    "    epochs=epochs,\n",
    "    plot_frequency=4,\n",
    "    label_mapper=dogs_mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_generated_images(\n",
    "    generator, device=device, num_images=36, denormalize=True, figsize=(12, 12), label_decoder=dogs_mapper, num_classes=num_classes\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCCTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
