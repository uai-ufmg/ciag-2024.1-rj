{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1wVZ_qST7u2FftTEd_5Qg6h_O_lxIHmJg","timestamp":1736428264489}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7d8a6f1c93fa47b18d53c22d29e96bc9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bec471fe1cd0420fa46a158ff1b2ae06","IPY_MODEL_ced95bf3a6ca4de7b23876c29600e9ec","IPY_MODEL_5dc5a7d6b21f4fae8fd16d1400555726"],"layout":"IPY_MODEL_aaa3c39a981a460d898903a402a60a7c"}},"bec471fe1cd0420fa46a158ff1b2ae06":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a18e291a5c44a5986323e7710254ee1","placeholder":"​","style":"IPY_MODEL_032a1fd15d9e4f079b618c9967fcaa8e","value":"100%"}},"ced95bf3a6ca4de7b23876c29600e9ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_35b548f417374b5fb98204be2a49a9c8","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e5c237801df4ab38f2a5feb59d3c00f","value":170498071}},"5dc5a7d6b21f4fae8fd16d1400555726":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd9064db10804a1a963babc4904bb301","placeholder":"​","style":"IPY_MODEL_7220aa0a65eb47d08dfbb8c501b03a00","value":" 170498071/170498071 [00:01&lt;00:00, 98940756.35it/s]"}},"aaa3c39a981a460d898903a402a60a7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a18e291a5c44a5986323e7710254ee1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"032a1fd15d9e4f079b618c9967fcaa8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35b548f417374b5fb98204be2a49a9c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e5c237801df4ab38f2a5feb59d3c00f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd9064db10804a1a963babc4904bb301":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7220aa0a65eb47d08dfbb8c501b03a00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"QRV_7PdLIXhh"},"source":["# Redes Neurais Convolucionais\n","\n","Vamos agora implementar a rede [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), uma das redes que trouxeram todo esse interesse para a área de *deep learning*.\n","\n","\n","\n"]},{"cell_type":"markdown","source":["## Configuração do ambiente"],"metadata":{"id":"--WTZYnbpWwD"}},{"cell_type":"code","metadata":{"id":"EAgvY5eBIc1s"},"source":["import time, os, sys, numpy as np\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch import optim\n","from torchsummary import summary\n","\n","import time, os, sys, numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","n = torch.cuda.device_count()\n","devices_ids= list(range(n))"],"metadata":{"id":"aJm-oZ3jo_0h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Carregamento da base da dados\n","\n","A função `load_data_cifar10` carrega e prepara o dataset CIFAR-10 para treinamento e teste.\n","\n","CIFAR-10 é um conjunto de dados amplamente utilizado em tarefas de visão computacional e aprendizado de máquina, especialmente em problemas de classificação de imagens. Ele consiste em imagens coloridas de dimensão 32 x 32, separadas em 10 classes."],"metadata":{"id":"QOQeMbN-piBi"}},{"cell_type":"code","metadata":{"id":"Xru0TyJXIjGp"},"source":["def load_data_cifar10(batch_size, resize=None, root=os.path.join('~', '.pytorch', 'datasets', 'cifar10')):\n","    root = os.path.expanduser(root)\n","\n","    transformer = []\n","    if resize:\n","        transformer += [torchvision.transforms.Resize(resize)]\n","    transformer += [torchvision.transforms.ToTensor()]\n","    transformer = torchvision.transforms.Compose(transformer)\n","\n","    cifar10_train = torchvision.datasets.CIFAR10(root=root, train=True, download=True, transform=transformer)\n","    cifar10_test = torchvision.datasets.CIFAR10(root=root, train=False, download=True, transform=transformer)\n","    num_workers = 0 if sys.platform.startswith('win32') else 4\n","\n","    train_iter = torch.utils.data.DataLoader(cifar10_train,\n","                                            batch_size, shuffle=True,\n","                                            num_workers=num_workers)\n","\n","    test_iter = torch.utils.data.DataLoader(cifar10_test,\n","                                            batch_size, shuffle=False,\n","                                            num_workers=num_workers)\n","    return train_iter, test_iter"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Funções auxiliares\n","\n","* `evaluate_accuracy` calcula a acurácia de um modelo em um dataset.\n","\n","* `train_validate` implemneta o treinamento e validação de uma rede."],"metadata":{"id":"y437uL7Vs7oJ"}},{"cell_type":"code","source":["def evaluate_accuracy(data_iter, net, loss):\n","    acc_sum, n, l = torch.Tensor([0]), 0, 0\n","    net.eval()\n","\n","    with torch.no_grad():\n","      for X, y in data_iter:\n","          X, y = X.to(device), y.to(device)\n","          y_hat = net(X)\n","          l += loss(y_hat, y).sum()\n","          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n","          n += y.size()[0]\n","\n","    return acc_sum.item() / n, l.item() / len(data_iter)\n","\n","def train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs):\n","    print('training on', device)\n","\n","    for epoch in range(num_epochs):\n","        net.train()\n","        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n","\n","        for X, y in train_iter:\n","            X, y = X.to(device), y.to(device)\n","            y_hat = net(X)\n","\n","            trainer.zero_grad()\n","            l = loss(y_hat, y).sum()\n","\n","            l.backward()\n","            trainer.step()\n","\n","            train_l_sum += l.item()\n","            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n","            n += y.size()[0]\n","\n","        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n","\n","        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n","              'test acc %.3f, time %.1f sec'\n","              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss,\n","                 test_acc, time.time() - start))"],"metadata":{"id":"Nr17G9Xfs9F3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TO6YizcSIpiH"},"source":["## AlexNet\n","\n","Agora já temos todo o conhecimento necessário para implementar nossa primeira arquitetura moderna.\n","\n","Vamos implementar a [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), uma das arquiteturas mais famosas dessa nova onda de rede neurais.\n","\n","<p align=\"center\">\n","  <img width=700 src=\"https://www.researchgate.net/profile/Jaime_Gallego2/publication/318168077/figure/fig1/AS:578190894927872@1514862859810/AlexNet-CNN-architecture-layers.png\">\n","</p>\n","\n","<p align=\"center\">\n","  <img width=700 src=\"https://miro.medium.com/max/700/1*vXBvV_Unz3JAxytc5iSeoQ.png\">\n","</p>"]},{"cell_type":"markdown","source":["### Atividade\n","\n","Implementa sua rede neural baseada na AlexNet.\n","\n","Lembre-se que, após cada camada de convolução e linear, há uma ativação não linear ReLU.\n","\n","**Dica:** utilize blocos *sequential* para diminuir a complexidade da função de forward."],"metadata":{"id":"22r6pQe1tNS0"}},{"cell_type":"code","source":["class AlexNet(nn.Module):\n","    def __init__(self, input_channels, classes=10, **kwargs):\n","        super(AlexNet, self).__init__(**kwargs)\n","\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(input_channels, 96, kernel_size=11, stride=4, padding=0),\n","            nn.ReLU(),\n","        )\n","\n","        self.maxpool1 = nn.MaxPool2d(kernel_size = 3, stride = 2)\n","\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n","            nn.ReLU(),\n","        )\n","\n","        self.maxpool2 = nn.MaxPool2d(kernel_size = 3, stride = 2)\n","\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","        )\n","\n","        self.conv4 = nn.Sequential(\n","            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","        )\n","\n","        self.conv5 = nn.Sequential(\n","            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","        )\n","\n","        self.maxpool5 = nn.MaxPool2d(kernel_size = 3, stride = 2)\n","\n","        self.flat = nn.Flatten()\n","\n","        self.fc6 = nn.Sequential(\n","            nn.Linear(9216, 4096),\n","            nn.ReLU()\n","        )\n","\n","        self.fc7 = nn.Sequential(\n","            nn.Linear(4096, 4096),\n","            nn.ReLU()\n","        )\n","\n","        self.fc8 = nn.Linear(4096, classes)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.maxpool1(x)\n","\n","        x = self.conv2(x)\n","        x = self.maxpool2(x)\n","\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.conv5(x)\n","        x = self.maxpool5(x)\n","        x = self.flat(x)\n","        x = self.fc6(x)\n","        x = self.fc7(x)\n","        out = self.fc8(x)\n","\n","        return out"],"metadata":{"id":"4fZfulS9u_iT"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LMWfNHpvIoRR","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7d8a6f1c93fa47b18d53c22d29e96bc9","bec471fe1cd0420fa46a158ff1b2ae06","ced95bf3a6ca4de7b23876c29600e9ec","5dc5a7d6b21f4fae8fd16d1400555726","aaa3c39a981a460d898903a402a60a7c","3a18e291a5c44a5986323e7710254ee1","032a1fd15d9e4f079b618c9967fcaa8e","35b548f417374b5fb98204be2a49a9c8","8e5c237801df4ab38f2a5feb59d3c00f","bd9064db10804a1a963babc4904bb301","7220aa0a65eb47d08dfbb8c501b03a00"]},"outputId":"a9082d99-b9d9-45d1-f69c-153c587db645"},"source":["num_epochs, lr, batch_size, weight_decay = 20, 0.001, 100, 0.0001\n","\n","net = AlexNet(input_channels=3, classes=10)\n","net.to(device)\n","print(summary(net,(3,227,227)))\n","\n","loss = nn.CrossEntropyLoss()\n","\n","train_iter, test_iter = load_data_cifar10(batch_size, resize=227)\n","\n","trainer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 96, 55, 55]          34,944\n","              ReLU-2           [-1, 96, 55, 55]               0\n","         MaxPool2d-3           [-1, 96, 27, 27]               0\n","            Conv2d-4          [-1, 256, 27, 27]         614,656\n","              ReLU-5          [-1, 256, 27, 27]               0\n","         MaxPool2d-6          [-1, 256, 13, 13]               0\n","            Conv2d-7          [-1, 384, 13, 13]         885,120\n","              ReLU-8          [-1, 384, 13, 13]               0\n","            Conv2d-9          [-1, 384, 13, 13]       1,327,488\n","             ReLU-10          [-1, 384, 13, 13]               0\n","           Conv2d-11          [-1, 256, 13, 13]         884,992\n","             ReLU-12          [-1, 256, 13, 13]               0\n","        MaxPool2d-13            [-1, 256, 6, 6]               0\n","          Flatten-14                 [-1, 9216]               0\n","           Linear-15                 [-1, 4096]      37,752,832\n","             ReLU-16                 [-1, 4096]               0\n","           Linear-17                 [-1, 4096]      16,781,312\n","             ReLU-18                 [-1, 4096]               0\n","           Linear-19                   [-1, 10]          40,970\n","================================================================\n","Total params: 58,322,314\n","Trainable params: 58,322,314\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.59\n","Forward/backward pass size (MB): 11.05\n","Params size (MB): 222.48\n","Estimated Total Size (MB): 234.12\n","----------------------------------------------------------------\n","None\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/.pytorch/datasets/cifar10/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d8a6f1c93fa47b18d53c22d29e96bc9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting /root/.pytorch/datasets/cifar10/cifar-10-python.tar.gz to /root/.pytorch/datasets/cifar10\n","Files already downloaded and verified\n","training on cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["epoch 1, train loss 1.8529, train acc 0.310, test loss 1.5753, test acc 0.427, time 107.1 sec\n","epoch 2, train loss 1.3988, train acc 0.492, test loss 1.2647, test acc 0.545, time 104.6 sec\n","epoch 3, train loss 1.1834, train acc 0.579, test loss 1.0976, test acc 0.609, time 103.7 sec\n","epoch 4, train loss 1.0185, train acc 0.639, test loss 1.0133, test acc 0.648, time 105.3 sec\n","epoch 5, train loss 0.8846, train acc 0.688, test loss 0.9714, test acc 0.664, time 103.8 sec\n","epoch 6, train loss 0.7719, train acc 0.728, test loss 0.8920, test acc 0.693, time 102.8 sec\n","epoch 7, train loss 0.6790, train acc 0.762, test loss 0.9016, test acc 0.694, time 105.4 sec\n","epoch 8, train loss 0.5863, train acc 0.795, test loss 0.9019, test acc 0.695, time 103.8 sec\n","epoch 9, train loss 0.5033, train acc 0.823, test loss 0.9189, test acc 0.708, time 104.5 sec\n","epoch 10, train loss 0.4177, train acc 0.852, test loss 1.0099, test acc 0.702, time 103.1 sec\n","epoch 11, train loss 0.3408, train acc 0.880, test loss 0.9964, test acc 0.709, time 107.1 sec\n","epoch 12, train loss 0.2861, train acc 0.899, test loss 1.1230, test acc 0.700, time 104.6 sec\n","epoch 13, train loss 0.2397, train acc 0.915, test loss 1.2597, test acc 0.706, time 102.8 sec\n","epoch 14, train loss 0.2123, train acc 0.924, test loss 1.2172, test acc 0.703, time 106.7 sec\n","epoch 15, train loss 0.1792, train acc 0.937, test loss 1.3746, test acc 0.693, time 102.5 sec\n","epoch 16, train loss 0.1671, train acc 0.941, test loss 1.3957, test acc 0.690, time 108.6 sec\n","epoch 17, train loss 0.1588, train acc 0.945, test loss 1.4183, test acc 0.704, time 103.8 sec\n","epoch 18, train loss 0.1417, train acc 0.951, test loss 1.5371, test acc 0.699, time 103.3 sec\n","epoch 19, train loss 0.1358, train acc 0.954, test loss 1.4713, test acc 0.696, time 103.0 sec\n","epoch 20, train loss 0.1302, train acc 0.956, test loss 1.6082, test acc 0.694, time 103.2 sec\n"]}]}]}