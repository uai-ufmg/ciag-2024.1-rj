{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIolQ6y6Tjmr"
   },
   "source": [
    "# AlexNet com recursos limitados\n",
    "\n",
    "Antes implementamos a [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) sem considerar a questão de quantidade de parâmetros. Nesta prática, vamos focar nesse quesito.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lagex56X1jZo"
   },
   "source": [
    "## Configuração do ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eUe7ps69Tmib"
   },
   "outputs": [],
   "source": [
    "import time, os, sys, numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import time, os, sys, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "diUAOEsCuHc9"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n = torch.cuda.device_count()\n",
    "devices_ids= list(range(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Y_kcRk7_Xyq"
   },
   "source": [
    "## Carregamento das bases de dados\n",
    "\n",
    "A função `load_data_cifar10` carrega e prepara o dataset CIFAR-10 para treinamento e teste.\n",
    "\n",
    "A função `load_data_fashion_mnist` faz o mesmo para o dataset Fashion MNIST. Esse conjunto de dados apresenta imagens em tons de cinza com dimensão de 28 x 28, e também é dividido em 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pP43CoSP_p1Z"
   },
   "outputs": [],
   "source": [
    "def load_data_cifar10(batch_size, resize=None):\n",
    "    root = '/pgeoprj2/ciag2024/dados/cifar/'\n",
    "\n",
    "    transformer = []\n",
    "    if resize:\n",
    "        transformer += [torchvision.transforms.Resize(resize)]\n",
    "    transformer += [torchvision.transforms.ToTensor()]\n",
    "    transformer = torchvision.transforms.Compose(transformer)\n",
    "\n",
    "    cifar10_train = torchvision.datasets.CIFAR10(root=root, train=True, download=True, transform=transformer)\n",
    "    cifar10_test = torchvision.datasets.CIFAR10(root=root, train=False, download=True, transform=transformer)\n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 1\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(cifar10_train,\n",
    "                                            batch_size, shuffle=True,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "    test_iter = torch.utils.data.DataLoader(cifar10_test,\n",
    "                                            batch_size, shuffle=False,\n",
    "                                            num_workers=num_workers)\n",
    "    return train_iter, test_iter\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join('~', '.pytorch', 'datasets', 'fashion-mnist')):\n",
    "    root = os.path.expanduser(root)\n",
    "\n",
    "    transformer = []\n",
    "    if resize:\n",
    "        transformer += [torchvision.transforms.Resize(resize)]\n",
    "    transformer += [torchvision.transforms.ToTensor()]\n",
    "    transformer = torchvision.transforms.Compose(transformer)\n",
    "\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True,download=True,transform=transformer)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False,download=True,transform=transformer)\n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 1\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train,\n",
    "                                            batch_size, shuffle=True,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test,\n",
    "                                            batch_size, shuffle=False,\n",
    "                                            num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLXqfJTt_WOH"
   },
   "source": [
    "## Funções auxiliares\n",
    "\n",
    "* `_get_batch` retorna as features e os labels de um batch\n",
    "\n",
    "* `evaluate_accuracy` calcula a acurácia de um modelo em um dataset.\n",
    "\n",
    "* `train_validate` implemneta o treinamento e validação de uma rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZE0X3AQzTsvK"
   },
   "outputs": [],
   "source": [
    "def _get_batch(batch):\n",
    "    features, labels = batch\n",
    "\n",
    "    if labels.type() != features.type():\n",
    "        labels = labels.type(features.type())\n",
    "\n",
    "    return (torch.nn.DataParallel(features, device_ids=devices_ids),\n",
    "            torch.nn.DataParallel(labels, device_ids=devices_ids), features.shape[0])\n",
    "\n",
    "def evaluate_accuracy(data_iter, net, loss):\n",
    "    acc_sum, n, l = torch.Tensor([0]), 0, 0\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for X, y in data_iter:\n",
    "          X, y = X.to(device), y.to(device)\n",
    "          y_hat = net(X)\n",
    "          l += loss(y_hat, y).sum()\n",
    "          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "          n += y.size()[0]\n",
    "\n",
    "    return acc_sum.item() / n, l.item() / len(data_iter)\n",
    "\n",
    "def train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs):\n",
    "    print('training on', device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "\n",
    "            trainer.zero_grad()\n",
    "            l = loss(y_hat, y).sum()\n",
    "\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.size()[0]\n",
    "\n",
    "        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n",
    "\n",
    "        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n",
    "              'test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss,\n",
    "                 test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYVw56ASTyHk"
   },
   "source": [
    "## AlexNet\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=700 src=\"https://miro.medium.com/max/700/1*vXBvV_Unz3JAxytc5iSeoQ.png\">\n",
    "</p>\n",
    "\n",
    "A AlexNet foi uma arquitetura muito famosa. Entretanto, ela possui muitos parâmetros.\n",
    "Essa arquitetura, para classificar 10 classes, tem um total de **58.312.736** parâmetros, como mostrado na tabela abaixo.\n",
    "\n",
    "**Camada** | **Calc Parâmetros** | **Total Parâmetros**\n",
    "--- | ---: | ---:\n",
    "Convolução 1 | 11\\*11\\*3\\*96 | 34.848\n",
    "Convolução 2 | 5\\*5\\*96\\*256 | 614.400\n",
    "Convolução 3 | 3\\*3\\*256\\*384 | 884.736\n",
    "Convolução 4 | 3\\*3\\*384\\*384 | 1.327.104\n",
    "Convolução 5 | 3\\*3\\*384\\*256 | 884.736\n",
    "FC 6 | 9216*4096 | 37.748.736\n",
    "FC 7 | 4096*4096 | 16.777.216\n",
    "FC 8 | 4096*10 | 40.960\n",
    "**Total** | | **58.312.736**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeM01oG9uYQC"
   },
   "source": [
    "### Rede original (para efeito de comparação)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X2vZ5m01scbJ"
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, input_channels, classes=10, **kwargs):\n",
    "        super(AlexNet, self).__init__(**kwargs)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(4096, classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763,
     "referenced_widgets": [
      "f8ead91fd97442239c016907f9203e2f",
      "fdb9a9e0c8cf44f78ee828aa3e03b702",
      "13770343f0c241e88828def84607b421",
      "a784808ee2414b088b15ee69d50a81c7",
      "1ad121cce90348d9916a16c35ad84c98",
      "c8f9fb7ded4945aa9496d821aacf7ebe",
      "6e16609b351442e79f4a6c805f0aec19",
      "bcb4d05a6ab448e797e9b01a05b6b3e3",
      "7e60e6758e3d41ee8e06b8112b4e635a",
      "64b53ad69dfa408eb0d2e5e53cd59488",
      "2140fc1531de4a39a882620bda0eca12"
     ]
    },
    "executionInfo": {
     "elapsed": 17771,
     "status": "ok",
     "timestamp": 1648206799845,
     "user": {
      "displayName": "Cristiano Rodrigues",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjMpntylvnODnhz5axnHmx8kfTnSNd4u5XPR_4z=s64",
      "userId": "08854749787306865774"
     },
     "user_tz": 180
    },
    "id": "rOPKGrvwtEL3",
    "outputId": "61eac7de-df26-4c0f-e341-de562aa23505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 55, 55]          34,944\n",
      "              ReLU-2           [-1, 96, 55, 55]               0\n",
      "         MaxPool2d-3           [-1, 96, 27, 27]               0\n",
      "            Conv2d-4          [-1, 256, 27, 27]         614,656\n",
      "              ReLU-5          [-1, 256, 27, 27]               0\n",
      "         MaxPool2d-6          [-1, 256, 13, 13]               0\n",
      "            Conv2d-7          [-1, 384, 13, 13]         885,120\n",
      "              ReLU-8          [-1, 384, 13, 13]               0\n",
      "            Conv2d-9          [-1, 384, 13, 13]       1,327,488\n",
      "             ReLU-10          [-1, 384, 13, 13]               0\n",
      "           Conv2d-11          [-1, 256, 13, 13]         884,992\n",
      "             ReLU-12          [-1, 256, 13, 13]               0\n",
      "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
      "          Flatten-14                 [-1, 9216]               0\n",
      "           Linear-15                 [-1, 4096]      37,752,832\n",
      "             ReLU-16                 [-1, 4096]               0\n",
      "          Dropout-17                 [-1, 4096]               0\n",
      "           Linear-18                 [-1, 4096]      16,781,312\n",
      "             ReLU-19                 [-1, 4096]               0\n",
      "          Dropout-20                 [-1, 4096]               0\n",
      "           Linear-21                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 58,322,314\n",
      "Trainable params: 58,322,314\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 11.11\n",
      "Params size (MB): 222.48\n",
      "Estimated Total Size (MB): 234.18\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "training on cuda\n",
      "epoch 1, train loss 2.0774, train acc 0.223, test loss 1.8350, test acc 0.323, time 58.9 sec\n",
      "epoch 2, train loss 1.6486, train acc 0.393, test loss 1.4508, test acc 0.459, time 63.5 sec\n",
      "epoch 3, train loss 1.4581, train acc 0.468, test loss 1.3188, test acc 0.526, time 64.3 sec\n",
      "epoch 4, train loss 1.3464, train acc 0.512, test loss 1.2503, test acc 0.544, time 63.6 sec\n",
      "epoch 5, train loss 1.2467, train acc 0.556, test loss 1.2021, test acc 0.576, time 63.8 sec\n",
      "epoch 6, train loss 1.1681, train acc 0.585, test loss 1.1124, test acc 0.609, time 64.1 sec\n",
      "epoch 7, train loss 1.0888, train acc 0.615, test loss 1.1157, test acc 0.602, time 63.5 sec\n",
      "epoch 8, train loss 1.0301, train acc 0.636, test loss 1.0274, test acc 0.641, time 63.7 sec\n",
      "epoch 9, train loss 0.9758, train acc 0.658, test loss 0.9814, test acc 0.657, time 63.6 sec\n",
      "epoch 10, train loss 0.9288, train acc 0.675, test loss 0.9697, test acc 0.657, time 63.8 sec\n",
      "epoch 11, train loss 0.8912, train acc 0.688, test loss 0.9042, test acc 0.680, time 63.8 sec\n",
      "epoch 12, train loss 0.8515, train acc 0.700, test loss 0.9127, test acc 0.682, time 63.8 sec\n",
      "epoch 13, train loss 0.8166, train acc 0.711, test loss 0.8734, test acc 0.696, time 63.6 sec\n",
      "epoch 14, train loss 0.7825, train acc 0.725, test loss 0.8716, test acc 0.700, time 63.9 sec\n",
      "epoch 15, train loss 0.7622, train acc 0.733, test loss 0.8972, test acc 0.693, time 63.7 sec\n",
      "epoch 16, train loss 0.7414, train acc 0.740, test loss 0.9110, test acc 0.691, time 63.9 sec\n",
      "epoch 17, train loss 0.7144, train acc 0.748, test loss 0.8491, test acc 0.714, time 63.8 sec\n",
      "epoch 18, train loss 0.6838, train acc 0.760, test loss 0.8747, test acc 0.701, time 63.7 sec\n",
      "epoch 19, train loss 0.6680, train acc 0.765, test loss 0.8770, test acc 0.703, time 63.8 sec\n",
      "epoch 20, train loss 0.6491, train acc 0.772, test loss 0.8742, test acc 0.708, time 63.7 sec\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr, batch_size, weight_decay = 20, 0.001, 100, 0.0001\n",
    "\n",
    "net = AlexNet(input_channels=3, classes=10)\n",
    "net.to(device)\n",
    "print(summary(net,(3,227,227)))\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train_iter, test_iter = load_data_cifar10(batch_size, resize=227)\n",
    "\n",
    "trainer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7fP5bn2ts-b"
   },
   "source": [
    "### Atividade - Arquitetura usando batch normalization e convolução dilatada\n",
    "\n",
    "O objetivo nessa prática é adaptar a rede neural proposta, baseada na [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), e incluir [*batch normalization*](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) e camadas de [convolução dilatada](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.htmll).\n",
    "\n",
    "> Inclua uma operação de batch normalization em cada uma das camadas convolucionais e nas duas primeiras transformações lineares.\n",
    "\n",
    "- Atente-se para especificar se o batch normalization é 1D ([nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)) ou 2D ([nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)).\n",
    "- Normalmente, aplicamos a camada de [*batch normalization*](https://arxiv.org/abs/1502.03167) entre a transformação e a função de ativação em uma camada densa.\n",
    "- Para camadas convolucionais, o [*batch normalization*](https://arxiv.org/abs/1502.03167) ocorre após o cálculo da convolução e antes da aplicação da função de ativação.\n",
    "- Nas funções de batch normalization, o parâmetro num_features será a quantidade de canais de saída da convolução, se a normalização estiver sendo aplicada em uma camada convolucional ou será a quantidade de features de saída da camada linear, se linear.\n",
    "\n",
    "> Inclua a convolução dilatada na camada conv2 e conv5 por meio do parâmetro dilation da função Conv2d.\n",
    "\n",
    "- Para a camada conv2, use uma taxa de dilatação de 2\n",
    "- Para a camada conv5, use uma taxa de dilatação de 4\n",
    "- Neste caso, como usamos filtros dilatados em duas camadas (com dilatação aumentante, ou seja, dilatação 2 seguida da dilatação 4), remova (ou comente com #) duas camadas convolucionais (conv3 e conv4), já que o *receptive field* se mantem similar dessa forma.\n",
    "\n",
    "> Ao final, observe que mesmo com uma quantidade menor de parâmetros, a nova rede alcança uma acurácia similar ou melhor que a rede original vista na prática passada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DFGfNGRPttP3"
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, input_channels, classes=10, **kwargs):\n",
    "        super(AlexNet, self).__init__(**kwargs)\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(num_features=96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2, dilation=2),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm2d(num_features=384),\n",
    "            # nn.ReLU(),\n",
    "\n",
    "            # nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm2d(num_features=384),\n",
    "            # nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, dilation=4),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(1024, 4096),\n",
    "            nn.BatchNorm1d(num_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.BatchNorm1d(num_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(4096, classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 193558,
     "status": "error",
     "timestamp": 1648206999775,
     "user": {
      "displayName": "Cristiano Rodrigues",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjMpntylvnODnhz5axnHmx8kfTnSNd4u5XPR_4z=s64",
      "userId": "08854749787306865774"
     },
     "user_tz": 180
    },
    "id": "HvmmttCCt3Uq",
    "outputId": "2d18eaec-a923-4465-e31d-db08943ded57",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 55, 55]          34,944\n",
      "       BatchNorm2d-2           [-1, 96, 55, 55]             192\n",
      "              ReLU-3           [-1, 96, 55, 55]               0\n",
      "         MaxPool2d-4           [-1, 96, 27, 27]               0\n",
      "            Conv2d-5          [-1, 256, 23, 23]         614,656\n",
      "       BatchNorm2d-6          [-1, 256, 23, 23]             512\n",
      "              ReLU-7          [-1, 256, 23, 23]               0\n",
      "         MaxPool2d-8          [-1, 256, 11, 11]               0\n",
      "            Conv2d-9            [-1, 256, 5, 5]         590,080\n",
      "      BatchNorm2d-10            [-1, 256, 5, 5]             512\n",
      "             ReLU-11            [-1, 256, 5, 5]               0\n",
      "        MaxPool2d-12            [-1, 256, 2, 2]               0\n",
      "          Flatten-13                 [-1, 1024]               0\n",
      "           Linear-14                 [-1, 4096]       4,198,400\n",
      "      BatchNorm1d-15                 [-1, 4096]           8,192\n",
      "             ReLU-16                 [-1, 4096]               0\n",
      "          Dropout-17                 [-1, 4096]               0\n",
      "           Linear-18                 [-1, 4096]      16,781,312\n",
      "      BatchNorm1d-19                 [-1, 4096]           8,192\n",
      "             ReLU-20                 [-1, 4096]               0\n",
      "          Dropout-21                 [-1, 4096]               0\n",
      "           Linear-22                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 22,277,962\n",
      "Trainable params: 22,277,962\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 10.93\n",
      "Params size (MB): 84.98\n",
      "Estimated Total Size (MB): 96.50\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "training on cuda\n",
      "epoch 1, train loss 1.4380, train acc 0.500, test loss 1.4296, test acc 0.533, time 54.6 sec\n",
      "epoch 2, train loss 1.0367, train acc 0.644, test loss 1.0061, test acc 0.653, time 54.5 sec\n",
      "epoch 3, train loss 0.8545, train acc 0.707, test loss 1.0081, test acc 0.665, time 54.6 sec\n",
      "epoch 4, train loss 0.7422, train acc 0.743, test loss 0.7982, test acc 0.726, time 55.7 sec\n",
      "epoch 5, train loss 0.6440, train acc 0.777, test loss 0.9351, test acc 0.692, time 56.3 sec\n",
      "epoch 6, train loss 0.5714, train acc 0.804, test loss 0.8711, test acc 0.721, time 55.8 sec\n",
      "epoch 7, train loss 0.4990, train acc 0.827, test loss 0.8101, test acc 0.735, time 54.5 sec\n",
      "epoch 8, train loss 0.4468, train acc 0.846, test loss 0.8930, test acc 0.721, time 54.5 sec\n",
      "epoch 9, train loss 0.3899, train acc 0.866, test loss 0.8562, test acc 0.736, time 55.8 sec\n",
      "epoch 10, train loss 0.3481, train acc 0.879, test loss 0.9970, test acc 0.715, time 54.5 sec\n",
      "epoch 11, train loss 0.3039, train acc 0.894, test loss 0.8529, test acc 0.741, time 55.8 sec\n",
      "epoch 12, train loss 0.2721, train acc 0.906, test loss 0.9304, test acc 0.738, time 55.6 sec\n",
      "epoch 13, train loss 0.2430, train acc 0.915, test loss 0.9387, test acc 0.740, time 56.5 sec\n",
      "epoch 14, train loss 0.2175, train acc 0.925, test loss 0.8658, test acc 0.765, time 54.5 sec\n",
      "epoch 15, train loss 0.2040, train acc 0.930, test loss 0.9391, test acc 0.749, time 55.6 sec\n",
      "epoch 16, train loss 0.1831, train acc 0.936, test loss 0.8856, test acc 0.766, time 54.6 sec\n",
      "epoch 17, train loss 0.1714, train acc 0.941, test loss 0.8884, test acc 0.770, time 54.5 sec\n",
      "epoch 18, train loss 0.1604, train acc 0.946, test loss 0.9750, test acc 0.757, time 54.5 sec\n",
      "epoch 19, train loss 0.1519, train acc 0.948, test loss 0.9431, test acc 0.754, time 54.7 sec\n",
      "epoch 20, train loss 0.1455, train acc 0.949, test loss 1.0307, test acc 0.740, time 54.9 sec\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr, batch_size, weight_decay = 20, 0.001, 100, 0.0001\n",
    "\n",
    "net = AlexNet(input_channels=3, classes=10)\n",
    "net.to(device)\n",
    "print(summary(net,(3,227,227)))\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train_iter, test_iter = load_data_cifar10(batch_size, resize=227)\n",
    "# train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=227)\n",
    "\n",
    "trainer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYvMiLCsDFGm"
   },
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1I-ucKf5ovRwuX6Bg20klqexZZ6aDjiE-",
     "timestamp": 1736428483688
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13770343f0c241e88828def84607b421": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bcb4d05a6ab448e797e9b01a05b6b3e3",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e60e6758e3d41ee8e06b8112b4e635a",
      "value": 170498071
     }
    },
    "1ad121cce90348d9916a16c35ad84c98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2140fc1531de4a39a882620bda0eca12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64b53ad69dfa408eb0d2e5e53cd59488": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e16609b351442e79f4a6c805f0aec19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e60e6758e3d41ee8e06b8112b4e635a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a784808ee2414b088b15ee69d50a81c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64b53ad69dfa408eb0d2e5e53cd59488",
      "placeholder": "​",
      "style": "IPY_MODEL_2140fc1531de4a39a882620bda0eca12",
      "value": " 170499072/? [00:02&lt;00:00, 75404415.18it/s]"
     }
    },
    "bcb4d05a6ab448e797e9b01a05b6b3e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8f9fb7ded4945aa9496d821aacf7ebe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8ead91fd97442239c016907f9203e2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fdb9a9e0c8cf44f78ee828aa3e03b702",
       "IPY_MODEL_13770343f0c241e88828def84607b421",
       "IPY_MODEL_a784808ee2414b088b15ee69d50a81c7"
      ],
      "layout": "IPY_MODEL_1ad121cce90348d9916a16c35ad84c98"
     }
    },
    "fdb9a9e0c8cf44f78ee828aa3e03b702": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8f9fb7ded4945aa9496d821aacf7ebe",
      "placeholder": "​",
      "style": "IPY_MODEL_6e16609b351442e79f4a6c805f0aec19",
      "value": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
