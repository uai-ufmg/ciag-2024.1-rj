{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1I-ucKf5ovRwuX6Bg20klqexZZ6aDjiE-","timestamp":1736428483688}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f8ead91fd97442239c016907f9203e2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fdb9a9e0c8cf44f78ee828aa3e03b702","IPY_MODEL_13770343f0c241e88828def84607b421","IPY_MODEL_a784808ee2414b088b15ee69d50a81c7"],"layout":"IPY_MODEL_1ad121cce90348d9916a16c35ad84c98"}},"fdb9a9e0c8cf44f78ee828aa3e03b702":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8f9fb7ded4945aa9496d821aacf7ebe","placeholder":"​","style":"IPY_MODEL_6e16609b351442e79f4a6c805f0aec19","value":""}},"13770343f0c241e88828def84607b421":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcb4d05a6ab448e797e9b01a05b6b3e3","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e60e6758e3d41ee8e06b8112b4e635a","value":170498071}},"a784808ee2414b088b15ee69d50a81c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64b53ad69dfa408eb0d2e5e53cd59488","placeholder":"​","style":"IPY_MODEL_2140fc1531de4a39a882620bda0eca12","value":" 170499072/? [00:02&lt;00:00, 75404415.18it/s]"}},"1ad121cce90348d9916a16c35ad84c98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8f9fb7ded4945aa9496d821aacf7ebe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e16609b351442e79f4a6c805f0aec19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bcb4d05a6ab448e797e9b01a05b6b3e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e60e6758e3d41ee8e06b8112b4e635a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64b53ad69dfa408eb0d2e5e53cd59488":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2140fc1531de4a39a882620bda0eca12":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"nIolQ6y6Tjmr"},"source":["# AlexNet com recursos limitados\n","\n","Antes implementamos a [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) sem considerar a questão de quantidade de parâmetros. Nesta prática, vamos focar nesse quesito.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["## Configuração do ambiente"],"metadata":{"id":"lagex56X1jZo"}},{"cell_type":"code","metadata":{"id":"eUe7ps69Tmib"},"source":["import time, os, sys, numpy as np\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch import optim\n","from torchsummary import summary\n","\n","import time, os, sys, numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","n = torch.cuda.device_count()\n","devices_ids= list(range(n))"],"metadata":{"id":"diUAOEsCuHc9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Carregamento das bases de dados\n","\n","A função `load_data_cifar10` carrega e prepara o dataset CIFAR-10 para treinamento e teste.\n","\n","A função `load_data_fashion_mnist` faz o mesmo para o dataset Fashion MNIST. Esse conjunto de dados apresenta imagens em tons de cinza com dimensão de 28 x 28, e também é dividido em 10 classes."],"metadata":{"id":"1Y_kcRk7_Xyq"}},{"cell_type":"code","source":["def load_data_cifar10(batch_size, resize=None, root=os.path.join('~', '.pytorch', 'datasets', 'cifar10')):\n","    root = os.path.expanduser(root)\n","\n","    transformer = []\n","    if resize:\n","        transformer += [torchvision.transforms.Resize(resize)]\n","    transformer += [torchvision.transforms.ToTensor()]\n","    transformer = torchvision.transforms.Compose(transformer)\n","\n","    cifar10_train = torchvision.datasets.CIFAR10(root=root, train=True, download=True, transform=transformer)\n","    cifar10_test = torchvision.datasets.CIFAR10(root=root, train=False, download=True, transform=transformer)\n","    num_workers = 0 if sys.platform.startswith('win32') else 4\n","\n","    train_iter = torch.utils.data.DataLoader(cifar10_train,\n","                                            batch_size, shuffle=True,\n","                                            num_workers=num_workers)\n","\n","    test_iter = torch.utils.data.DataLoader(cifar10_test,\n","                                            batch_size, shuffle=False,\n","                                            num_workers=num_workers)\n","    return train_iter, test_iter\n","\n","def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join('~', '.pytorch', 'datasets', 'fashion-mnist')):\n","    root = os.path.expanduser(root)\n","\n","    transformer = []\n","    if resize:\n","        transformer += [torchvision.transforms.Resize(resize)]\n","    transformer += [torchvision.transforms.ToTensor()]\n","    transformer = torchvision.transforms.Compose(transformer)\n","\n","    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True,download=True,transform=transformer)\n","    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False,download=True,transform=transformer)\n","    num_workers = 0 if sys.platform.startswith('win32') else 4\n","\n","    train_iter = torch.utils.data.DataLoader(mnist_train,\n","                                            batch_size, shuffle=True,\n","                                            num_workers=num_workers)\n","\n","    test_iter = torch.utils.data.DataLoader(mnist_test,\n","                                            batch_size, shuffle=False,\n","                                            num_workers=num_workers)\n","    return train_iter, test_iter"],"metadata":{"id":"pP43CoSP_p1Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Funções auxiliares\n","\n","* `_get_batch` retorna as features e os labels de um batch\n","\n","* `evaluate_accuracy` calcula a acurácia de um modelo em um dataset.\n","\n","* `train_validate` implemneta o treinamento e validação de uma rede."],"metadata":{"id":"gLXqfJTt_WOH"}},{"cell_type":"code","metadata":{"id":"ZE0X3AQzTsvK"},"source":["def _get_batch(batch):\n","    features, labels = batch\n","\n","    if labels.type() != features.type():\n","        labels = labels.type(features.type())\n","\n","    return (torch.nn.DataParallel(features, device_ids=devices_ids),\n","            torch.nn.DataParallel(labels, device_ids=devices_ids), features.shape[0])\n","\n","def evaluate_accuracy(data_iter, net, loss):\n","    acc_sum, n, l = torch.Tensor([0]), 0, 0\n","    net.eval()\n","\n","    with torch.no_grad():\n","      for X, y in data_iter:\n","          X, y = X.to(device), y.to(device)\n","          y_hat = net(X)\n","          l += loss(y_hat, y).sum()\n","          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n","          n += y.size()[0]\n","\n","    return acc_sum.item() / n, l.item() / len(data_iter)\n","\n","def train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs):\n","    print('training on', device)\n","\n","    for epoch in range(num_epochs):\n","        net.train()\n","        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n","\n","        for X, y in train_iter:\n","            X, y = X.to(device), y.to(device)\n","            y_hat = net(X)\n","\n","            trainer.zero_grad()\n","            l = loss(y_hat, y).sum()\n","\n","            l.backward()\n","            trainer.step()\n","\n","            train_l_sum += l.item()\n","            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n","            n += y.size()[0]\n","\n","        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n","\n","        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n","              'test acc %.3f, time %.1f sec'\n","              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss,\n","                 test_acc, time.time() - start))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QYVw56ASTyHk"},"source":["## AlexNet\n","\n","<p align=\"center\">\n","  <img width=700 src=\"https://miro.medium.com/max/700/1*vXBvV_Unz3JAxytc5iSeoQ.png\">\n","</p>\n","\n","A AlexNet foi uma arquitetura muito famosa. Entretanto, ela possui muitos parâmetros.\n","Essa arquitetura, para classificar 10 classes, tem um total de **58.312.736** parâmetros, como mostrado na tabela abaixo.\n","\n","**Camada** | **Calc Parâmetros** | **Total Parâmetros**\n","--- | ---: | ---:\n","Convolução 1 | 11\\*11\\*3\\*96 | 34.848\n","Convolução 2 | 5\\*5\\*96\\*256 | 614.400\n","Convolução 3 | 3\\*3\\*256\\*384 | 884.736\n","Convolução 4 | 3\\*3\\*384\\*384 | 1.327.104\n","Convolução 5 | 3\\*3\\*384\\*256 | 884.736\n","FC 6 | 9216*4096 | 37.748.736\n","FC 7 | 4096*4096 | 16.777.216\n","FC 8 | 4096*10 | 40.960\n","**Total** | | **58.312.736**\n"]},{"cell_type":"markdown","source":["### Rede original (para efeito de comparação)"],"metadata":{"id":"TeM01oG9uYQC"}},{"cell_type":"code","source":["class AlexNet(nn.Module):\n","    def __init__(self, input_channels, classes=10, **kwargs):\n","        super(AlexNet, self).__init__(**kwargs)\n","        self.features = nn.Sequential(\n","            nn.Conv2d(in_channels=input_channels, out_channels=96, kernel_size=11, stride=4, padding=0),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n","\n","            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n","\n","            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","\n","            nn.Linear(9216, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","\n","            nn.Linear(4096, classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.classifier(x)\n","        return x"],"metadata":{"id":"X2vZ5m01scbJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs, lr, batch_size, weight_decay = 20, 0.001, 100, 0.0001\n","\n","net = AlexNet(input_channels=3, classes=10)\n","net.to(device)\n","print(summary(net,(3,227,227)))\n","\n","loss = nn.CrossEntropyLoss()\n","\n","train_iter, test_iter = load_data_cifar10(batch_size, resize=227)\n","\n","trainer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":763,"referenced_widgets":["f8ead91fd97442239c016907f9203e2f","fdb9a9e0c8cf44f78ee828aa3e03b702","13770343f0c241e88828def84607b421","a784808ee2414b088b15ee69d50a81c7","1ad121cce90348d9916a16c35ad84c98","c8f9fb7ded4945aa9496d821aacf7ebe","6e16609b351442e79f4a6c805f0aec19","bcb4d05a6ab448e797e9b01a05b6b3e3","7e60e6758e3d41ee8e06b8112b4e635a","64b53ad69dfa408eb0d2e5e53cd59488","2140fc1531de4a39a882620bda0eca12"]},"id":"rOPKGrvwtEL3","outputId":"61eac7de-df26-4c0f-e341-de562aa23505","executionInfo":{"status":"ok","timestamp":1648206799845,"user_tz":180,"elapsed":17771,"user":{"displayName":"Cristiano Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMpntylvnODnhz5axnHmx8kfTnSNd4u5XPR_4z=s64","userId":"08854749787306865774"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 96, 55, 55]          34,944\n","              ReLU-2           [-1, 96, 55, 55]               0\n","         MaxPool2d-3           [-1, 96, 27, 27]               0\n","            Conv2d-4          [-1, 256, 27, 27]         614,656\n","              ReLU-5          [-1, 256, 27, 27]               0\n","         MaxPool2d-6          [-1, 256, 13, 13]               0\n","            Conv2d-7          [-1, 384, 13, 13]         885,120\n","              ReLU-8          [-1, 384, 13, 13]               0\n","            Conv2d-9          [-1, 384, 13, 13]       1,327,488\n","             ReLU-10          [-1, 384, 13, 13]               0\n","           Conv2d-11          [-1, 256, 13, 13]         884,992\n","             ReLU-12          [-1, 256, 13, 13]               0\n","        MaxPool2d-13            [-1, 256, 6, 6]               0\n","          Flatten-14                 [-1, 9216]               0\n","           Linear-15                 [-1, 4096]      37,752,832\n","             ReLU-16                 [-1, 4096]               0\n","          Dropout-17                 [-1, 4096]               0\n","           Linear-18                 [-1, 4096]      16,781,312\n","             ReLU-19                 [-1, 4096]               0\n","          Dropout-20                 [-1, 4096]               0\n","           Linear-21                   [-1, 10]          40,970\n","================================================================\n","Total params: 58,322,314\n","Trainable params: 58,322,314\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.59\n","Forward/backward pass size (MB): 11.11\n","Params size (MB): 222.48\n","Estimated Total Size (MB): 234.18\n","----------------------------------------------------------------\n","None\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/.pytorch/datasets/fashion-mnist/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8ead91fd97442239c016907f9203e2f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting /root/.pytorch/datasets/fashion-mnist/cifar-10-python.tar.gz to /root/.pytorch/datasets/fashion-mnist\n","Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"markdown","source":["### Atividade - Arquitetura usando batch normalization e convolução dilatada\n","\n","O objetivo nessa prática é adaptar a rede neural proposta, baseada na [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), e incluir [*batch normalization*](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) e camadas de [convolução dilatada](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.htmll).\n","\n","> Inclua uma operação de batch normalization em cada uma das camadas convolucionais e nas duas primeiras transformações lineares.\n","\n","- Atente-se para especificar se o batch normalization é 1D ([nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)) ou 2D ([nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)).\n","- Normalmente, aplicamos a camada de [*batch normalization*](https://arxiv.org/abs/1502.03167) entre a transformação e a função de ativação em uma camada densa.\n","- Para camadas convolucionais, o [*batch normalization*](https://arxiv.org/abs/1502.03167) ocorre após o cálculo da convolução e antes da aplicação da função de ativação.\n","- Nas funções de batch normalization, o parâmetro num_features será a quantidade de canais de saída da convolução, se a normalização estiver sendo aplicada em uma camada convolucional ou será a quantidade de features de saída da camada linear, se linear.\n","\n","> Inclua a convolução dilatada na camada conv2 e conv5 por meio do parâmetro dilation da função Conv2d.\n","\n","- Para a camada conv2, use uma taxa de dilatação de 2\n","- Para a camada conv5, use uma taxa de dilatação de 4\n","- Neste caso, como usamos filtros dilatados em duas camadas (com dilatação aumentante, ou seja, dilatação 2 seguida da dilatação 4), remova (ou comente com #) duas camadas convolucionais (conv3 e conv4), já que o *receptive field* se mantem similar dessa forma.\n","\n","> Ao final, observe que mesmo com uma quantidade menor de parâmetros, a nova rede alcança uma acurácia similar ou melhor que a rede original vista na prática passada."],"metadata":{"id":"e7fP5bn2ts-b"}},{"cell_type":"code","source":["class AlexNet(nn.Module):\n","    def __init__(self, input_channels, classes=10, **kwargs):\n","        super(AlexNet, self).__init__(**kwargs)\n","\n","        self.features = nn.Sequential(\n","            nn.Conv2d(in_channels=input_channels, out_channels=96, kernel_size=11, stride=4, padding=0),\n","            nn.BatchNorm2d(num_features=96),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n","\n","            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2, dilation=2),\n","            nn.BatchNorm2d(num_features=256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n","\n","            # nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n","            # nn.BatchNorm2d(num_features=384),\n","            # nn.ReLU(),\n","\n","            # nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n","            # nn.BatchNorm2d(num_features=384),\n","            # nn.ReLU(),\n","\n","            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, dilation=4),\n","            nn.BatchNorm2d(num_features=256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","\n","            nn.Linear(1024, 4096),\n","            nn.BatchNorm1d(num_features=4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","\n","            nn.Linear(4096, 4096),\n","            nn.BatchNorm1d(num_features=4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","\n","            nn.Linear(4096, classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.classifier(x)\n","        return x"],"metadata":{"id":"DFGfNGRPttP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs, lr, batch_size, weight_decay = 20, 0.001, 100, 0.0001\n","\n","net = AlexNet(input_channels=3, classes=10)\n","net.to(device)\n","print(summary(net,(3,227,227)))\n","\n","loss = nn.CrossEntropyLoss()\n","\n","train_iter, test_iter = load_data_cifar10(batch_size, resize=227)\n","# train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=227)\n","\n","trainer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"],"metadata":{"id":"HvmmttCCt3Uq","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1648206999775,"user_tz":180,"elapsed":193558,"user":{"displayName":"Cristiano Rodrigues","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMpntylvnODnhz5axnHmx8kfTnSNd4u5XPR_4z=s64","userId":"08854749787306865774"}},"outputId":"2d18eaec-a923-4465-e31d-db08943ded57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 96, 55, 55]          34,944\n","       BatchNorm2d-2           [-1, 96, 55, 55]             192\n","              ReLU-3           [-1, 96, 55, 55]               0\n","         MaxPool2d-4           [-1, 96, 27, 27]               0\n","            Conv2d-5          [-1, 256, 23, 23]         614,656\n","       BatchNorm2d-6          [-1, 256, 23, 23]             512\n","              ReLU-7          [-1, 256, 23, 23]               0\n","         MaxPool2d-8          [-1, 256, 11, 11]               0\n","            Conv2d-9          [-1, 384, 11, 11]         885,120\n","      BatchNorm2d-10          [-1, 384, 11, 11]             768\n","             ReLU-11          [-1, 384, 11, 11]               0\n","           Conv2d-12          [-1, 384, 11, 11]       1,327,488\n","      BatchNorm2d-13          [-1, 384, 11, 11]             768\n","             ReLU-14          [-1, 384, 11, 11]               0\n","           Conv2d-15            [-1, 256, 5, 5]         884,992\n","      BatchNorm2d-16            [-1, 256, 5, 5]             512\n","             ReLU-17            [-1, 256, 5, 5]               0\n","        MaxPool2d-18            [-1, 256, 2, 2]               0\n","          Flatten-19                 [-1, 1024]               0\n","           Linear-20                 [-1, 4096]       4,198,400\n","      BatchNorm1d-21                 [-1, 4096]           8,192\n","             ReLU-22                 [-1, 4096]               0\n","          Dropout-23                 [-1, 4096]               0\n","           Linear-24                 [-1, 4096]      16,781,312\n","      BatchNorm1d-25                 [-1, 4096]           8,192\n","             ReLU-26                 [-1, 4096]               0\n","          Dropout-27                 [-1, 4096]               0\n","           Linear-28                   [-1, 10]          40,970\n","================================================================\n","Total params: 24,787,018\n","Trainable params: 24,787,018\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.59\n","Forward/backward pass size (MB): 13.06\n","Params size (MB): 94.55\n","Estimated Total Size (MB): 108.20\n","----------------------------------------------------------------\n","None\n","Files already downloaded and verified\n","Files already downloaded and verified\n","training on cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-ca6d911d3567>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# treinamento e validação via Pytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-526e2f6614f2>\u001b[0m in \u001b[0;36mtrain_validate\u001b[0;34m(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mtrain_l_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mtrain_acc_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["."],"metadata":{"id":"uYvMiLCsDFGm"}}]}