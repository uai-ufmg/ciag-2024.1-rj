{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQi1bQq1MTNh"
   },
   "source": [
    "# Segmentação Semântica - Rocks\n",
    "## Configuração\n",
    "\n",
    "Importando módulos necessários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCr1F07lJ3lN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io\n",
    "from sklearn import metrics\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Rodando em:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição de funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(*models):\n",
    "    for model in models:\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "                    \n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                module.weight.data.fill_(1)\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "\n",
    "def evaluate(preds, labels):\n",
    "    f1_list = []\n",
    "    iou_list = []\n",
    "\n",
    "    for i in tqdm(range(len(preds)), desc='Metrics'):\n",
    "        f1 = metrics.f1_score(labels[i].flatten(), preds[i].flatten())\n",
    "        iou = metrics.jaccard_score(labels[i].flatten(), preds[i].flatten())\n",
    "\n",
    "        f1_list.append(f1)\n",
    "        iou_list.append(iou)\n",
    "\n",
    "    f1_list = np.asarray(f1_list)\n",
    "    iou_list = np.asarray(iou_list)\n",
    "\n",
    "    return f1_list, iou_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "Neste notebook faremos alguns experimentos com o Dataset [DRP-Benchmarks](https://www.sciencedirect.com/science/article/pii/S0098300412003147) sendo utilizados as bases de Bereau e Grosmont. Em seguida treinaremos uma UNet para realizar a segmentação semântica das rochas nas imagens extraidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqNqprCdmdQ4"
   },
   "source": [
    "# Dataloaders Customizados\n",
    "\n",
    "Os frameworks de Deep Learning modernos (i.e. MXNet e Pytorch) permitem a criação de dataloaders customizados ao se sobrescreverem classes desses frameworks. Esse tipo de dataloader é especialmente útil no caso de tarefas diferentes das de classificação que temos visto até agora (i.e. segmentação e detecção de imagens, processamento de áudio, processamento de linguagem natural, etc), nas quais os labels podem ser mais esparsos ou densos que rótulos de classificação.\n",
    "\n",
    "Usando como base as classes [*Dataloader*](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) e [*Dataset*](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) do subpacote [*data*](https://pytorch.org/docs/stable/data.html) do Pytorch, podemos customizar a leitura dos dados ao mesmo tempo em que paralelizamos a leitura das amostras dos nossos batches. A paralelização da leitura de amostras em várias [threads](https://www.tutorialspoint.com/python/python_multithreading) torna o uso da GPU o mais eficiente possível, já que não é necessário deixar a GPU esperando pelo carregamento de novas amostras para compor um batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJImDO6sMTVX"
   },
   "outputs": [],
   "source": [
    "class RockDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            is_train: bool,\n",
    "            crop_size: int = 256,\n",
    "            num_classes: int = 2,\n",
    "            datapath: str = '/pgeoprj2/ciag2024/dados/drp-benchmarks/images/grosmont'\n",
    "        ):\n",
    "        \n",
    "        self.is_train = is_train\n",
    "        self.crop_size = crop_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.img_path = os.path.join(datapath, 'tif')\n",
    "        self.mask_path = os.path.join(datapath, 'segmented-kongju.raw')\n",
    "\n",
    "        self.image_filenames, self.mask_vol = self.make_dataset()\n",
    "\n",
    "        if len(self.image_filenames) == 0:\n",
    "            raise RuntimeError('Found 0 images, please check the data set.')\n",
    "\n",
    "    def make_dataset(self):\n",
    "        if self.is_train:\n",
    "            mask_vol = np.fromfile(open(self.mask_path, 'rb'), dtype=np.int8).reshape(1024, 1024, 1024)\n",
    "            mask_vol = mask_vol.transpose((0, 2, 1))[:-64]\n",
    "        else:\n",
    "            mask_vol = np.fromfile(open(self.mask_path, 'rb'), dtype=np.int8).reshape(1024, 1024, 1024)\n",
    "            mask_vol = mask_vol.transpose((0, 2, 1))[-64:]\n",
    "\n",
    "        if self.is_train:\n",
    "            image_filenames = sorted([f for f in os.listdir(self.img_path) if os.path.isfile(os.path.join(self.img_path, f))])[:-64]\n",
    "        else:\n",
    "            image_filenames = sorted([f for f in os.listdir(self.img_path) if os.path.isfile(os.path.join(self.img_path, f))])[-64:]\n",
    "\n",
    "        return image_filenames, mask_vol\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.image_filenames[idx]\n",
    "        img_path = os.path.join(self.img_path, filename)\n",
    "\n",
    "        # Lendo os dados\n",
    "        img = io.imread(img_path)\n",
    "        mask = self.mask_vol[idx]\n",
    "\n",
    "        # Fazendo o casting apropriado\n",
    "        img = img.astype(np.float32)\n",
    "        mask = mask.astype(np.int64)\n",
    "\n",
    "        # Z-Score Normalization\n",
    "        img = (img - img.mean()) / img.std()\n",
    "\n",
    "        # Realizando crops (treino = random, teste = fixo)\n",
    "        if self.is_train:\n",
    "            randh, randw = np.random.randint(0, 1024 - self.crop_size, size=2)\n",
    "\n",
    "            img = img[randh:randh+self.crop_size, randw:randw+self.crop_size]\n",
    "            mask = mask[randh:randh+self.crop_size, randw:randw+self.crop_size]\n",
    "\n",
    "            # Adicionando dimensão para canal\n",
    "            img = np.expand_dims(img, axis=0)  # (H, W) -> (1, H, W)\n",
    "            \n",
    "        else:\n",
    "            img = np.array([\n",
    "                img[0:256, 0:256],    img[256:512, 0:256],    img[512:768, 0:256],    img[768:1024, 0:256],\n",
    "                img[0:256, 256:512],  img[256:512, 256:512],  img[512:768, 256:512],  img[768:1024, 256:512],\n",
    "                img[0:256, 512:768],  img[256:512, 512:768],  img[512:768, 512:768],  img[768:1024, 512:768],\n",
    "                img[0:256, 768:1024], img[256:512, 768:1024], img[512:768, 768:1024], img[768:1024, 768:1024],\n",
    "            ])\n",
    "\n",
    "            mask = np.array([\n",
    "                mask[0:256, 0:256],    mask[256:512, 0:256],    mask[512:768, 0:256],    mask[768:1024, 0:256],\n",
    "                mask[0:256, 256:512],  mask[256:512, 256:512],  mask[512:768, 256:512],  mask[768:1024, 256:512],\n",
    "                mask[0:256, 512:768],  mask[256:512, 512:768],  mask[512:768, 512:768],  mask[768:1024, 512:768],\n",
    "                mask[0:256, 768:1024], mask[256:512, 768:1024], mask[512:768, 768:1024], mask[768:1024, 768:1024],\n",
    "            ])\n",
    "\n",
    "            # Adicionando dimensão para canal\n",
    "            img = np.expand_dims(img, axis=1)  # (16, H, W) -> (16, 1, H, W)\n",
    "\n",
    "            img = np.array(img, dtype=np.float32)\n",
    "            mask = np.array(mask, dtype=np.int64)\n",
    "        \n",
    "        img = torch.from_numpy(img)\n",
    "        mask = torch.from_numpy(mask)\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando datasets e dataloaders\n",
    "batch_size = 16\n",
    "num_workers = 2\n",
    "\n",
    "train_dataset = RockDataset(is_train=True)\n",
    "valid_dataset = RockDataset(is_train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=None, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "print('Número de instâncias de treino:', len(train_dataset))\n",
    "print('Número de instâncias de validação:', len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "qR8JAGoDjA4p",
    "outputId": "f5cbb136-8f57-4346-806f-ea422fc46f28"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i, (img, label) in enumerate(train_dataset):\n",
    "    if i >= 4:\n",
    "        break\n",
    "\n",
    "    ax[0][i].imshow(img[0].numpy(), cmap=plt.get_cmap('gray'))\n",
    "    # ax[0][i].imshow(lab.numpy(), 'Greens_r', interpolation='nearest', alpha=0.3)  # Descomente para ver o overlay das labels\n",
    "    ax[0][i].set_yticks([])\n",
    "    ax[0][i].set_xticks([])\n",
    "\n",
    "    ax[1][i].imshow(label.numpy(), cmap=plt.get_cmap('gray'))\n",
    "    ax[1][i].set_yticks([])\n",
    "    ax[1][i].set_xticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgFCtKuV53Ri"
   },
   "source": [
    "# Atividade Prática: Implementando pipeline de segmentação\n",
    "\n",
    "O dataset contém apenas duas classes e um canal de input, tendo os dataloaders já pré-definidos acima. Para essa tarefa vamos usar uma arquitetura `UNet`.\n",
    "\n",
    "Os passos a serem seguidos são os seguintes:\n",
    "1.   Definir arquitetura de uma rede de segmentação.\n",
    "2.   Definir uma loss.\n",
    "3.   Instanciar um otimizador.\n",
    "4.   Implementar funções de treino e teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDlbcvpuZ1OI"
   },
   "source": [
    "Primeiramente vamos implementar a arquitetura da UNet. Para isso, vamos dividir a arquitetura em blocos _encoder_ e blocos _decoder_. Os __encoders_ são usados na parte inicial da rede, e os _decoders_ na parte final.\n",
    "\n",
    "Cada bloco _encoder_ possui duas camadas de convolução com `kernel_size=3` e `padding=1`, de forma com que essas convoluções não alterem as dimensões espaciais (altura $H$ e largura $W$) da entrada. Apenas no final de cada bloco _encoder_ temos um `max_pooling` para reduzir as dimensões espaciais da entrada (ela terá metade da altura e largura).\n",
    "\n",
    "Cada bloco _decoder_ possui duas camadas de convolução, da mesma forma dos _encoders_ (sem alterar dimensões espaciais). No final do bloco, para aumentar a dimensão espacial da entrada, ele possui uma convolução transposta com `kernel_size=2` e `stride=2` para dobrar a altura e largura da entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wE0Z3zgHTorq",
    "outputId": "ecdf3d38-9c28-4e00-970a-18906f9d3937"
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        # [TODO] Cada bloco do encoder será composto por:\n",
    "        #   1) Conv2d com in_channels e out_channels como entrada e saída;\n",
    "        #   2) Batch norm;\n",
    "        #   3) ReLU;\n",
    "        #   4) Conv2d replicando out_channels como entrada e saída;\n",
    "        #   5) Batch norm;\n",
    "        #   6) ReLU;\n",
    "        #   7) MaxPooling2d com kernel 2x2 e stride de 2.\n",
    "        \n",
    "        self.encode = nn.Sequential(\n",
    "            # implemente sua solução aqui\n",
    "        )\n",
    "    \n",
    "    # TODO: implemente o forward\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, middle_channels: int, out_channels: int):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        # [TODO] cada bloco do decoder será composto por:\n",
    "        #   1) Dropout de canais;\n",
    "        #   2) Conv2d tendo in_channels e middle_channels como entrada e saída;\n",
    "        #   3) Batch Norm;\n",
    "        #   4) Conv2d replicando middle_channels como entrada e saída;\n",
    "        #   5) Batch Norm;\n",
    "        #   6) ReLU;\n",
    "        #   7) Conv2d transposta, tendo como saída out_channels com kernel 2x2 e stride de 2.\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "            # implemente sua solução aqui\n",
    "        )\n",
    "    \n",
    "    # TODO: implemente o forward\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_classes: int = 2):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # [TODO] Reutilize os blocos encoder e decoder para construir a UNet.\n",
    "        #        Experimente diferentes formas de organização desses blocos (ex.: com e sem dropout entre eles).\n",
    "        #        Obs.: Não se esqueça da última camada de classificação (faça ela convolucional)\n",
    "\n",
    "        # implemente sua solução aqui\n",
    "        \n",
    "        # Inicialização dos pesos\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: implemente o forward nos blocos de encoder.\n",
    "        \n",
    "        # implemente sua solução aqui\n",
    "\n",
    "        # TODO: implemente o forward dos blocos de decoder.\n",
    "        #       Lembre-se da característica fundamental da UNet nesse passo!\n",
    "\n",
    "        # implemente sua solução aqui\n",
    "\n",
    "        # TODO: implemente o forward na última camada (predição de classes)\n",
    "\n",
    "        # implemente sua solução aqui\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_channels=1)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kuo6esmKQOCL"
   },
   "outputs": [],
   "source": [
    "# Configurando o otimizador\n",
    "weight_decay = 5e-4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8p2oLwCQFzX"
   },
   "outputs": [],
   "source": [
    "# TODO: escolha qual a loss mais apropriada para esse caso\n",
    "criterion = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2XCR_Q_WH_B"
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, criterion, optimizer):\n",
    "    tic = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    model.train()\n",
    "    for i, batch in (pbar := tqdm(enumerate(train_dataloader), total=len(train_dataloader), unit='batch')):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # TODO: implemente um step do treinamento dessa rede\n",
    "        # Use outputs e loss para salvar os outputs do modelo e loss\n",
    "        outputs = ...\n",
    "        loss = ...\n",
    "\n",
    "        # Obtendo as predições (argmax)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        # Salvando as predições e labels para computar métricas para toda época.\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        # Atualizando o display da loss\n",
    "        train_losses.append(loss.item())\n",
    "        pbar.set_description(f\"Train loss: {np.mean(train_losses):.4f}\")\n",
    "\n",
    "    # Computando métricas da época\n",
    "    f1, iou = evaluate(all_preds, all_labels)\n",
    "    tac = time.time()\n",
    "\n",
    "    print('[train], [loss %.4f +/- %.4f], [iou %.4f +/- %.4f], [f1 %.4f +/- %.4f], [time %.2f]' % (\n",
    "        np.mean(train_losses), np.std(train_losses), iou.mean(), iou.std(), f1.mean(), f1.std(), (tac - tic)))\n",
    "\n",
    "\n",
    "def validate(model, valid_dataloader, criterion, epoch):\n",
    "    tic = time.time()\n",
    "    display_images_freq = 1\n",
    "\n",
    "    valid_losses = []\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in (pbar := tqdm(enumerate(valid_dataloader), total=len(valid_dataloader), unit='batch')):\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # TODO: implemente um step de validação.\n",
    "            # Use outputs e loss para salvar a saída da rede e loss\n",
    "\n",
    "            outputs = ...\n",
    "            loss = ...\n",
    "\n",
    "            # Obtendo as predições (argmax)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            # Salvando as predições e labels para computar métricas para toda época.\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "            # Atualizando o display da loss\n",
    "            valid_losses.append(loss.item())\n",
    "            pbar.set_description(f\"Valid loss: {np.mean(valid_losses):.4f}\")\n",
    "\n",
    "            # Exibindo alguns exemplos\n",
    "            if i == 0 and epoch % display_images_freq == 0:\n",
    "                fig, ax = plt.subplots(2, 3, figsize=(10, 7))\n",
    "                perm = np.random.permutation(inputs.size(0))\n",
    "\n",
    "                for p in range(2):\n",
    "                    ax[p, 0].imshow(inputs[perm[p], 0].detach().cpu().numpy())\n",
    "                    ax[p, 0].set_yticks([])\n",
    "                    ax[p, 0].set_xticks([])\n",
    "                    ax[p, 0].set_title('Image')\n",
    "\n",
    "                    ax[p, 1].imshow(labels[perm[p]].detach().cpu().numpy())\n",
    "                    ax[p, 1].set_yticks([])\n",
    "                    ax[p, 1].set_xticks([])\n",
    "                    ax[p, 1].set_title('Label')\n",
    "\n",
    "                    ax[p, 2].imshow(preds[perm[p]].detach().cpu().numpy())\n",
    "                    ax[p, 2].set_yticks([])\n",
    "                    ax[p, 2].set_xticks([])\n",
    "                    ax[p, 2].set_title('Prediction')\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "    # Computando métricas da época\n",
    "    f1, iou = evaluate(all_preds, all_labels)\n",
    "    tac = time.time()\n",
    "\n",
    "    print('[test], [loss %.4f +/- %.4f], [iou %.4f +/- %.4f], [f1 %.4f +/- %.4f], [time %.2f]' % (\n",
    "        np.mean(valid_losses), np.std(valid_losses), iou.mean(), iou.std(), f1.mean(), f1.std(), (tac - tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "b75050a788af44f6be84d12e6772f78b",
      "4990a7cee7414253b4ddc70542d33d8b",
      "07a157257d774801b2468d91631b03cc",
      "bff5818361a145a3a77eb81195cc68fc",
      "26d1294388534668852abb7dbf31e961",
      "03fde8ba86014ec4839e0b25db1649f4",
      "57c8604fd2bb4c45b18f261b1ac669c7",
      "e5b3f337809243fdb95929ecbc0af4d3",
      "b0a704f44bc6423489a9900e128ce96a",
      "9314810bab31414c8c91d73ffc49fc62",
      "31ea708e6dda4d83b58382073979fde3"
     ]
    },
    "id": "19ywBEJ-NdWy",
    "outputId": "33f26c78-208a-4938-967f-f3146927012c"
   },
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f' ========== Epoch {epoch} ========== ')\n",
    "\n",
    "    train(model, train_dataloader, criterion, optimizer)\n",
    "    validate(model, valid_dataloader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03fde8ba86014ec4839e0b25db1649f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07a157257d774801b2468d91631b03cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5b3f337809243fdb95929ecbc0af4d3",
      "max": 120,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b0a704f44bc6423489a9900e128ce96a",
      "value": 120
     }
    },
    "26d1294388534668852abb7dbf31e961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31ea708e6dda4d83b58382073979fde3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4990a7cee7414253b4ddc70542d33d8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03fde8ba86014ec4839e0b25db1649f4",
      "placeholder": "​",
      "style": "IPY_MODEL_57c8604fd2bb4c45b18f261b1ac669c7",
      "value": "Loss: 0.2266: 100%"
     }
    },
    "57c8604fd2bb4c45b18f261b1ac669c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9314810bab31414c8c91d73ffc49fc62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0a704f44bc6423489a9900e128ce96a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b75050a788af44f6be84d12e6772f78b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4990a7cee7414253b4ddc70542d33d8b",
       "IPY_MODEL_07a157257d774801b2468d91631b03cc",
       "IPY_MODEL_bff5818361a145a3a77eb81195cc68fc"
      ],
      "layout": "IPY_MODEL_26d1294388534668852abb7dbf31e961"
     }
    },
    "bff5818361a145a3a77eb81195cc68fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9314810bab31414c8c91d73ffc49fc62",
      "placeholder": "​",
      "style": "IPY_MODEL_31ea708e6dda4d83b58382073979fde3",
      "value": " 120/120 [00:23&lt;00:00,  5.61batch/s]"
     }
    },
    "e5b3f337809243fdb95929ecbc0af4d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
