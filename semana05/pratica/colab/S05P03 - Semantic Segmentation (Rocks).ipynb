{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQi1bQq1MTNh"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY5VBxjD4gkJ",
        "outputId": "1dd4485a-d98d-4320-ba26-64e4e5b6b412"
      },
      "outputs": [],
      "source": [
        "# Downloading the Digital Rock Physics Benchmarks dataset.\n",
        "!git clone https://github.com/fkrzikalla/drp-benchmarks\n",
        "\n",
        "# Removing unnecessary files.\n",
        "!rm -r drp-benchmarks/images/fontainebleau\n",
        "!rm -r drp-benchmarks/images/spherepack\n",
        "\n",
        "!rm drp-benchmarks/images/*/segmented-kongju.mat\n",
        "!rm drp-benchmarks/images/*/segmented-stanford.raw.gz\n",
        "!rm drp-benchmarks/images/*/segmented-vsg.raw.gz\n",
        "\n",
        "# Uncompressing raw segmentations.\n",
        "!gunzip drp-benchmarks/images/berea/segmented-kongju.raw.gz\n",
        "!gunzip drp-benchmarks/images/grosmont/segmented-kongju.raw.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcktC5EZOa4p",
        "outputId": "22eecdae-8362-4c58-eb54-ac3871af9638"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nCr1F07lJ3lN"
      },
      "outputs": [],
      "source": [
        "# Basic imports.\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "from skimage import io\n",
        "from skimage import transform\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from torchinfo import summary\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZHHFWXeif2A",
        "outputId": "d3fc309d-c17f-470c-c8ab-6fd69a01d9f8"
      },
      "outputs": [],
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 10,                    # Number of epochs.\n",
        "    'dataset': 'grosmont',              # Dataset. (bereau or grosmont)\n",
        "    'n_classes': 2,                     # Number of classes in segmentation task.\n",
        "    'lr': 1e-4,                         # Learning rate.\n",
        "    'weight_decay': 5e-4,               # L2 penalty.\n",
        "    'momentum': 0.9,                    # Momentum.\n",
        "    'num_workers': 2,                   # Number of workers on data loader.\n",
        "    'batch_size': 8,                    # Mini-batch size.\n",
        "    'crop_size': 256,                   # Random crop size.\n",
        "    'show_freq': 1,                     # Frequency for showing predictions.\n",
        "    'root': 'drp-benchmarks/images'     # root directory for the images\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9EemBZ7ISIYP"
      },
      "outputs": [],
      "source": [
        "# Random initialization for weights and biases.\n",
        "def initialize_weights(*models):\n",
        "    for model in models:\n",
        "        for module in model.modules():\n",
        "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "                nn.init.kaiming_normal_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "            elif isinstance(module, nn.BatchNorm2d):\n",
        "                module.weight.data.fill_(1)\n",
        "                module.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fpwMOIO2uleJ"
      },
      "outputs": [],
      "source": [
        "def evaluate(prds, labs, num_classes):\n",
        "\n",
        "    iou_list = []\n",
        "    f1_list = []\n",
        "\n",
        "    for prd, lab in zip(prds, labs):\n",
        "\n",
        "        iou = metrics.jaccard_score(lab.ravel(), prd.ravel())\n",
        "        f1 = metrics.f1_score(lab.ravel(), prd.ravel())\n",
        "\n",
        "        iou_list.append(iou)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "    iou_list = np.asarray(iou_list)\n",
        "    f1_list = np.asarray(f1_list)\n",
        "\n",
        "    return iou_list, f1_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introdução\n",
        "\n",
        "Neste notebook faremos alguns experimentos com o Dataset [DRP-Benchmarks](https://www.sciencedirect.com/science/article/pii/S0098300412003147) sendo utilizados as bases de Bereau e Grosmont. Em seguida treinaremos uma UNet para realizar a segmentação semântica das rochas nas imagens extraidas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqNqprCdmdQ4"
      },
      "source": [
        "# Dataloaders Customizados\n",
        "\n",
        "Os frameworks de Deep Learning modernos (i.e. MXNet e Pytorch) permitem a criação de dataloaders customizados ao se sobrescreverem classes desses frameworks. Esse tipo de dataloader é especialmente útil no caso de tarefas diferentes das de classificação que temos visto até agora (i.e. segmentação e detecção de imagens, processamento de áudio, processamento de linguagem, natural, etc), nas quais os labels podem ser mais esparsos ou densos que rótulos de classificação.\n",
        "\n",
        "Usando como base as classes [*Dataloader*](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) e [*Dataset*](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) do subpacote [*data*](https://pytorch.org/docs/stable/data.html) do Pytorch, podemos customizar a leitura dos dados ao mesmo tempo em que paralelizamos a leitura das amostras dos nossos batches. A paralelização da leitura de amostras em várias [threads](https://www.tutorialspoint.com/python/python_multithreading) torna o uso da GPU o mais eficiente possível, já que não é necessário deixar a GPU esperando pelo carregamento de novas amostras para compor um batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TJImDO6sMTVX"
      },
      "outputs": [],
      "source": [
        "# Class that reads the DRP Benchmarks dataset.\n",
        "class RockDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, root, mode, dataset, crop_size=256, num_classes=2):\n",
        "\n",
        "        # Initializing variables.\n",
        "        self.root = root\n",
        "        self.mode = mode\n",
        "        self.dataset = dataset\n",
        "        self.crop_size = crop_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Creating list of image files.\n",
        "        self.imgs = self.make_dataset()\n",
        "\n",
        "        # Check for consistency in list.\n",
        "        if len(self.imgs) == 0:\n",
        "\n",
        "            raise (RuntimeError('Found 0 images, please check the data set.'))\n",
        "\n",
        "    def make_dataset(self):\n",
        "\n",
        "        # Joining input paths.\n",
        "        self.img_path = os.path.join(self.root,\n",
        "                                     self.dataset,\n",
        "                                     'tif')\n",
        "        self.msk_path = os.path.join(self.root,\n",
        "                                     self.dataset,\n",
        "                                     'segmented-kongju.raw')\n",
        "\n",
        "        # Preload mask volume.\n",
        "        transpose_sequence = [0, 2, 1]\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self.msk_vol = np.fromfile(open(self.msk_path, 'rb'),\n",
        "                                       dtype=np.int8).reshape(1024, 1024, 1024)\n",
        "            self.msk_vol = self.msk_vol.transpose(*transpose_sequence)[:-64]\n",
        "\n",
        "        elif self.mode == 'test':\n",
        "            self.msk_vol = np.fromfile(open(self.msk_path, 'rb'),\n",
        "                                       dtype=np.int8).reshape(1024, 1024, 1024)\n",
        "            self.msk_vol = self.msk_vol.transpose(*transpose_sequence)[-64:]\n",
        "\n",
        "        # Reading paths from file.\n",
        "        if self.mode == 'train':\n",
        "            items = sorted([f for f in os.listdir(self.img_path) if os.path.isfile(os.path.join(self.img_path, f))])[:-64]\n",
        "        elif self.mode == 'test':\n",
        "            items = sorted([f for f in os.listdir(self.img_path) if os.path.isfile(os.path.join(self.img_path, f))])[-64:]\n",
        "\n",
        "        # Returning list.\n",
        "        return items\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Reading items from list.\n",
        "        f = self.imgs[index]\n",
        "\n",
        "        img_path = os.path.join(self.img_path, f)\n",
        "\n",
        "        # Reading images.\n",
        "        img = io.imread(img_path)\n",
        "        msk = self.msk_vol[index]\n",
        "\n",
        "        # Casting images to the appropriate dtypes.\n",
        "        img = img.astype(np.float32)\n",
        "        msk = msk.astype(np.int64)\n",
        "\n",
        "        # Normalization (z-score).\n",
        "        img = (img - img.mean()) / img.std()\n",
        "\n",
        "        # Random cropping images.\n",
        "        if self.mode == 'train':\n",
        "\n",
        "            rand_hw = np.random.randint(0, 1024 - self.crop_size, size=2)\n",
        "\n",
        "            img = img[rand_hw[0]:(rand_hw[0] + self.crop_size),\n",
        "                      rand_hw[1]:(rand_hw[1] + self.crop_size)]\n",
        "            msk = msk[rand_hw[0]:(rand_hw[0] + self.crop_size),\n",
        "                      rand_hw[1]:(rand_hw[1] + self.crop_size)]\n",
        "\n",
        "            # Fixing channel dimension.\n",
        "            img = np.expand_dims(img, axis=0) # From (H, W) to (C=1, H, W).\n",
        "\n",
        "        if self.mode == 'test':\n",
        "\n",
        "            img = [img[0:256, 0:256],    img[256:512, 0:256],    img[512:768, 0:256],    img[768:1024, 0:256],\n",
        "                   img[0:256, 256:512],  img[256:512, 256:512],  img[512:768, 256:512],  img[768:1024, 256:512],\n",
        "                   img[0:256, 512:768],  img[256:512, 512:768],  img[512:768, 512:768],  img[768:1024, 512:768],\n",
        "                   img[0:256, 768:1024], img[256:512, 768:1024], img[512:768, 768:1024], img[768:1024, 768:1024],\n",
        "                  ]\n",
        "            msk = [msk[0:256, 0:256],    msk[256:512, 0:256],    msk[512:768, 0:256],    msk[768:1024, 0:256],\n",
        "                   msk[0:256, 256:512],  msk[256:512, 256:512],  msk[512:768, 256:512],  msk[768:1024, 256:512],\n",
        "                   msk[0:256, 512:768],  msk[256:512, 512:768],  msk[512:768, 512:768],  msk[768:1024, 512:768],\n",
        "                   msk[0:256, 768:1024], msk[256:512, 768:1024], msk[512:768, 768:1024], msk[768:1024, 768:1024],\n",
        "                  ]\n",
        "\n",
        "            img = np.asarray(img, dtype=np.float32)\n",
        "            msk = np.asarray(msk, dtype=np.int64)\n",
        "\n",
        "        # Turning numpy arrays to tensors.\n",
        "        img = torch.from_numpy(img)\n",
        "        msk = torch.from_numpy(msk)\n",
        "\n",
        "        # Returning to iterator.\n",
        "        return img, msk\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.imgs)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# Instantiating dataloaders.\n",
        "root = args['root']\n",
        "\n",
        "# Setting datasets.\n",
        "train_set = RockDataset(root,\n",
        "                        'train',\n",
        "                        args['dataset'],\n",
        "                        args['crop_size'])\n",
        "test_set = RockDataset(root,\n",
        "                       'test',\n",
        "                       args['dataset'],\n",
        "                       args['crop_size'])\n",
        "\n",
        "# Setting dataloaders.\n",
        "train_loader = DataLoader(train_set,\n",
        "                          batch_size=args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         batch_size=1,\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "qR8JAGoDjA4p",
        "outputId": "f5cbb136-8f57-4346-806f-ea422fc46f28"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 4, figsize=(16, 8))\n",
        "for i, batch_data in enumerate(train_set):\n",
        "\n",
        "    if i >= 4:\n",
        "        break\n",
        "\n",
        "    img, lab = batch_data\n",
        "\n",
        "\n",
        "    ax[0][i].imshow(img[0].numpy(), cmap=plt.get_cmap('gray'))\n",
        "    # ax[0][i].imshow(lab.numpy(), 'Greens_r', interpolation='nearest', alpha=0.3) # Uncomment to see overlay of labels.\n",
        "    ax[0][i].set_yticks([])\n",
        "    ax[0][i].set_xticks([])\n",
        "\n",
        "    ax[1][i].imshow(lab.numpy(), cmap=plt.get_cmap('gray'))\n",
        "    ax[1][i].set_yticks([])\n",
        "    ax[1][i].set_xticks([])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgFCtKuV53Ri"
      },
      "source": [
        "# Atividade Prática: Implementando pipeline de segmentação\n",
        "\n",
        "Usar as arquiteturas de segmentação estudadas durante essa semana e adaptá-la para o dataset de rochas. O dataset contém apenas duas classes e um canal de input, tendo o dataloader já pré-definido a cima. Para essa tarefa vamos usar uma arquitetura UNet.\n",
        "\n",
        "Os passos a serem seguidos são os seguintes:\n",
        "\n",
        "1.   Definir arquitetura de uma rede de segmentação.\n",
        "2.   Definir uma loss.\n",
        "3.   Instanciar um otimizador.\n",
        "4.   Implementar funções de *train()* e *test()*.\n",
        "5.   Implementar for que itera sobre as epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDlbcvpuZ1OI"
      },
      "source": [
        "Primeiramente vamos implementar a arquitetura da UNet. Para isso, vamos dividir a arquitetura em blocos \"encoder\" e blocos \"decoder\". Os encoders são usados na parte inicial da rede, e os decoders na parte final.\n",
        "\n",
        "Cada bloco encoder possui duas camadas de convolução com `kernel_size=3` e `padding=1`, de forma com que essas convoluções não alterem as dimensões espaciais (altura $H$ e largura $W$) da entrada. Apenas no final de cada bloco encoder temos um `max_pooling` para reduzir as dimensões espaciais da entrada (ela terá metade da altura e largura).\n",
        "\n",
        "Cada bloco decoder possui duas camadas de convolução, da mesma forma dos encoders (sem alterar dimensões espaciais). No final do bloco, para aumentar a dimensão espacial da entrada, ele possui uma convolução transposta com `kernel_size=2` e `stride=2` para dobrar a altura e largura da entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE0Z3zgHTorq",
        "outputId": "ecdf3d38-9c28-4e00-970a-18906f9d3937"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "\n",
        "        super(EncoderBlock, self).__init__()\n",
        "\n",
        "        # TODO: each encoder block be composed of:\n",
        "        #        1) 2d conv inputting in_channels and outputting out_channels;\n",
        "        #        2) batch norm;\n",
        "        #        3) ReLU;\n",
        "        #        4) 2d conv outputting out_channels;\n",
        "        #        5) batch norm;\n",
        "        #        6) ReLU;\n",
        "        #        7) max pooling 2d with kernel 2x2 and stride of 2.\n",
        "        self.encode = nn.Sequential(\n",
        "            ...\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # TODO: implement forward on self.encode.\n",
        "        return ...\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, middle_channels, out_channels):\n",
        "\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        # TODO: each decoder block be composed of:\n",
        "        #        1) channel dropout;\n",
        "        #        2) 2d conv inputting in_channels and outputting\n",
        "        #           middle_channels;\n",
        "        #        3) batch norm;\n",
        "        #        4) 2d conv outputting middle_channels;\n",
        "        #        5) batch norm;\n",
        "        #        6) ReLU;\n",
        "        #        7) transposed 2d conv outputting out_channels with kernel 2x2,\n",
        "        #           stride of 2, padding and output_padding of 0.\n",
        "        self.decode = nn.Sequential(\n",
        "            ...\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # TODO: implement forward on self.decode.\n",
        "        return ...\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # TODO: this U-Net will be composed of 3 Encoder Blocks and 3\n",
        "        #        Decoder Blocks:\n",
        "        #        1) Encoder Block inputting input_channels and outputting 32\n",
        "        #           channels;\n",
        "        #        2) Encoder Block outputting 64 channels;\n",
        "        #        3) Encoder Block 128 channels;\n",
        "        #        4) Decoder Block inputting 128 channels, 256 middle channels\n",
        "        #           and outputting 128 channels;\n",
        "        #        5) Decoder Block inputting 256 channels, 128 middle channels\n",
        "        #           and outputting 64 channels;\n",
        "        #        6) Decoder Block inputting 128 channels, 64 middle channels\n",
        "        #           and outputting 32 channels;\n",
        "        #        7) channel dropout;\n",
        "        #        8) 2d conv block (conv + batch norm + ReLU) inputting 64 channels\n",
        "        #           and outputting 32 channels;\n",
        "        #        9) 2d conv block (conv + batch norm + ReLU) inputting 32 channels\n",
        "        #           and outputting 32 channels;\n",
        "        #        10) 2d conv block with kernel 1x1 outputting num_classes\n",
        "        #            channels. This is the pixel classification layer.\n",
        "        pass\n",
        "\n",
        "        # Random initialization for weights.\n",
        "        initialize_weights(self)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # TODO: implement forward on Encoder Blocks.\n",
        "        ...\n",
        "\n",
        "        # TODO: implement forward on Decoder Blocks. Each Decoder receives the\n",
        "        #        output of the last layer concatenated with the output of an\n",
        "        #        Encoder Block with compatible size. See U-Net architectural\n",
        "        #        scheme above.\n",
        "        ...\n",
        "\n",
        "        # TODO: implement forward on the final 2 conv blocks and final 1x1\n",
        "        #        conv to predict classes.\n",
        "        ...\n",
        "\n",
        "        # TODO: return output.\n",
        "        return ...\n",
        "\n",
        "net = UNet(1, num_classes=args['n_classes']).to(args['device'])\n",
        "\n",
        "summary(net, (1, 1, 256, 256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Kuo6esmKQOCL"
      },
      "outputs": [],
      "source": [
        "# Setting optimizer.\n",
        "optimizer = optim.Adam([p for p in list(net.parameters()) if p.requires_grad],\n",
        "                       lr=args['lr'],\n",
        "                       weight_decay=args['weight_decay'],\n",
        "                       betas=(args['momentum'], 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w8p2oLwCQFzX"
      },
      "outputs": [],
      "source": [
        "# TODO: Setting loss.\n",
        "criterion = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e2XCR_Q_WH_B"
      },
      "outputs": [],
      "source": [
        "# Training function.\n",
        "def train(train_loader, net, criterion, optimizer, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "\n",
        "    # Setting network for training mode.\n",
        "    net.train()\n",
        "\n",
        "    # Average Meter for batch loss.\n",
        "    train_loss = []\n",
        "\n",
        "    # Lists for whole epoch loss.\n",
        "    labs_all, prds_all = [], []\n",
        "\n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in (pbar := tqdm(enumerate(train_loader), total=len(train_loader), unit='batch')):\n",
        "        # Obtaining images and labels for batch.\n",
        "        inps, labs = batch_data\n",
        "\n",
        "        # TODO: Casting to cuda variables (inps and labs).\n",
        "        inps = ...\n",
        "        labs = ...\n",
        "\n",
        "        # TODO: Clearing the gradients of optimizer.\n",
        "        ...\n",
        "\n",
        "        # TODO: Forwarding.\n",
        "        outs = ...\n",
        "\n",
        "        # TODO: Computing loss.\n",
        "        loss = ...\n",
        "\n",
        "        # TODO: Computing backpropagation and taking optimizer step.\n",
        "        ...\n",
        "\n",
        "        # Obtaining predictions.\n",
        "        prds = outs.data.max(1)[1].squeeze_(1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Appending images for epoch loss calculation.\n",
        "        labs_all.append(labs.detach().data.squeeze(0).cpu().numpy())\n",
        "        prds_all.append(prds)\n",
        "\n",
        "        # Updating loss meter.\n",
        "        train_loss = np.append(train_loss, loss.data.item())\n",
        "\n",
        "        pbar.set_description(f\"Loss: {train_loss.mean():.4f}\")\n",
        "\n",
        "    # Computing error metrics for whole epoch.\n",
        "    iou, f1 = evaluate(prds_all, labs_all, args['n_classes'])\n",
        "    toc = time.time()\n",
        "\n",
        "    # Printing training epoch loss and metrics.\n",
        "    # print('-------------------------------------------------------------------')\n",
        "    print('[train], [loss %.4f +/- %.4f], [iou %.4f +/- %.4f], [f1 %.4f +/- %.4f], [time %.2f]' % (\n",
        "        train_loss.mean(), train_loss.std(), iou.mean(), iou.std(), f1.mean(), f1.std(), (toc - tic)))\n",
        "    # print('-------------------------------------------------------------------')\n",
        "\n",
        "def test(test_loader, net, criterion, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "\n",
        "    # Setting network for evaluation mode.\n",
        "    net.eval()\n",
        "\n",
        "    # Average Meter for batch loss.\n",
        "    test_loss = []\n",
        "\n",
        "    # Lists for whole epoch loss.\n",
        "    labs_all, prds_all = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Iterating over batches.\n",
        "        for i, batch_data in enumerate(test_loader):\n",
        "\n",
        "            # Obtaining images and labels for batch.\n",
        "            inps, labs = batch_data\n",
        "\n",
        "            inps = inps.view(inps.size(1), 1, inps.size(2), inps.size(3))\n",
        "            labs = labs.view(labs.size(1), labs.size(2), labs.size(3))\n",
        "\n",
        "            # TODO: Casting to cuda variables.\n",
        "            inps = ...\n",
        "            labs = ...\n",
        "\n",
        "            # TODO: Forwarding.\n",
        "            outs = ...\n",
        "\n",
        "            # TODO: Computing loss.\n",
        "            loss = ...\n",
        "\n",
        "            # Obtaining predictions.\n",
        "            prds = outs.data.max(1)[1].squeeze_(1).squeeze(0).cpu().numpy()\n",
        "\n",
        "            # Appending images for epoch loss calculation.\n",
        "            labs_all.append(labs.detach().data.squeeze(0).cpu().numpy())\n",
        "            prds_all.append(prds)\n",
        "\n",
        "            # Updating loss meter.\n",
        "            test_loss = np.append(test_loss, loss.data.item())\n",
        "\n",
        "            # Showing some example results\n",
        "            if i == 0 and epoch % args['show_freq'] == 0:\n",
        "\n",
        "                fig, ax = plt.subplots(2, 3, figsize=(10, 7))\n",
        "\n",
        "                perm = np.random.permutation(inps.size(0))\n",
        "\n",
        "                for p in range(2):\n",
        "\n",
        "                    ax[p, 0].imshow(inps[perm[p], 0].detach().cpu().numpy().squeeze())\n",
        "                    ax[p, 0].set_yticks([])\n",
        "                    ax[p, 0].set_xticks([])\n",
        "                    ax[p, 0].set_title('Image')\n",
        "\n",
        "                    ax[p, 1].imshow(labs[perm[p]].detach().data.squeeze(0).cpu().numpy().squeeze())\n",
        "                    ax[p, 1].set_yticks([])\n",
        "                    ax[p, 1].set_xticks([])\n",
        "                    ax[p, 1].set_title('Label')\n",
        "\n",
        "                    ax[p, 2].imshow(prds[perm[p]].squeeze())\n",
        "                    ax[p, 2].set_yticks([])\n",
        "                    ax[p, 2].set_xticks([])\n",
        "                    ax[p, 2].set_title('Prediction')\n",
        "\n",
        "\n",
        "    # Computing error metrics for whole epoch.\n",
        "    iou, f1 = evaluate(prds_all, labs_all, args['n_classes'])\n",
        "    toc = time.time()\n",
        "\n",
        "    # Printing test epoch loss and metrics.\n",
        "    # print('-------------------------------------------------------------------')\n",
        "    print('[test], [loss %.4f +/- %.4f], [iou %.4f +/- %.4f], [f1 %.4f +/- %.4f], [time %.2f]' % (\n",
        "        test_loss.mean(), test_loss.std(), iou.mean(), iou.std(), f1.mean(), f1.std(), (toc - tic)))\n",
        "\n",
        "    plt.show()\n",
        "    # print('-------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "b75050a788af44f6be84d12e6772f78b",
            "4990a7cee7414253b4ddc70542d33d8b",
            "07a157257d774801b2468d91631b03cc",
            "bff5818361a145a3a77eb81195cc68fc",
            "26d1294388534668852abb7dbf31e961",
            "03fde8ba86014ec4839e0b25db1649f4",
            "57c8604fd2bb4c45b18f261b1ac669c7",
            "e5b3f337809243fdb95929ecbc0af4d3",
            "b0a704f44bc6423489a9900e128ce96a",
            "9314810bab31414c8c91d73ffc49fc62",
            "31ea708e6dda4d83b58382073979fde3"
          ]
        },
        "id": "19ywBEJ-NdWy",
        "outputId": "33f26c78-208a-4938-967f-f3146927012c"
      },
      "outputs": [],
      "source": [
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "    print(f' ========== Epoch {epoch} ========== ')\n",
        "\n",
        "    # Training function.\n",
        "    train(train_loader, net, criterion, optimizer, epoch)\n",
        "\n",
        "    # Computing test loss and metrics.\n",
        "    test(test_loader, net, criterion, epoch)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "TCCTorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03fde8ba86014ec4839e0b25db1649f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07a157257d774801b2468d91631b03cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5b3f337809243fdb95929ecbc0af4d3",
            "max": 120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0a704f44bc6423489a9900e128ce96a",
            "value": 120
          }
        },
        "26d1294388534668852abb7dbf31e961": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31ea708e6dda4d83b58382073979fde3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4990a7cee7414253b4ddc70542d33d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03fde8ba86014ec4839e0b25db1649f4",
            "placeholder": "​",
            "style": "IPY_MODEL_57c8604fd2bb4c45b18f261b1ac669c7",
            "value": "Loss: 0.2266: 100%"
          }
        },
        "57c8604fd2bb4c45b18f261b1ac669c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9314810bab31414c8c91d73ffc49fc62": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0a704f44bc6423489a9900e128ce96a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b75050a788af44f6be84d12e6772f78b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4990a7cee7414253b4ddc70542d33d8b",
              "IPY_MODEL_07a157257d774801b2468d91631b03cc",
              "IPY_MODEL_bff5818361a145a3a77eb81195cc68fc"
            ],
            "layout": "IPY_MODEL_26d1294388534668852abb7dbf31e961"
          }
        },
        "bff5818361a145a3a77eb81195cc68fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9314810bab31414c8c91d73ffc49fc62",
            "placeholder": "​",
            "style": "IPY_MODEL_31ea708e6dda4d83b58382073979fde3",
            "value": " 120/120 [00:23&lt;00:00,  5.61batch/s]"
          }
        },
        "e5b3f337809243fdb95929ecbc0af4d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
