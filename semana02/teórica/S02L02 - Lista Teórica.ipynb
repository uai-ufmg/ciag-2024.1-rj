{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lista Teórica 02-02: Redes Neurais e Backpropagation\n",
        "\n",
        "> - Redes Neurais\n",
        "> - Backpropagation"
      ],
      "metadata": {
        "id": "mp8u8W4Z9BUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercício 1\n",
        "\n",
        "Considere uma rede com uma camada interna, em que tanto ela como a camada\n",
        "de sa ́ıda possuem apenas um  ́unico n ́o. Ambas a funçõees de ativação são a sigmóide. Seja y a saída. Assuma que função custo ́e dada por $\\frac{(y − z)^\n",
        "2}{2}$, onde z  ́e a classificação do dado univariado x. Use backpropagation para calcular o gradiente da fun ção custo com respeito aos quatro pesos da rede."
      ],
      "metadata": {
        "id": "Qsr7RJsE9ULs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Responda Aqui_"
      ],
      "metadata": {
        "id": "x3Z_i7vm95AA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercício 2\n",
        "\n",
        "Considere uma rede neural com input de duas features x1 e x2, e com duas\n",
        "camadas internas, cada uma com dois nós. Assuma que os pesos estão distribúıdos de forma que os nós em cima nas camadas aplicam a sigmóide na soma dos seus inputs, e que os nós embaixo aplicam a função tangente hiperb́olica em seus inputs. O nó de saída aplica a ReLU na soma dos dois inputs. Desenhe esta rede. Escreva a saída dessa rede neural como uma\n",
        "função de x1 e x2 de forma fechada."
      ],
      "metadata": {
        "id": "XH_M_hKE97JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Responda Aqui_"
      ],
      "metadata": {
        "id": "lwFJrFwZ-SD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercício 3\n",
        "\n",
        "Vimos como usar uma rede de perceptrons com uma camada interna para\n",
        "construir a fun ̧c ̃ao XOR. Queremos novamente construir esta função, mas nos dois nós da camada interna temos a função de ativação ReLU, e no nó de saída a função de ativação\n",
        "f(x) = x. Caso o XOR dê 0, queremos que saída seja 0, e se o XOR dá 1, queremos que a saída seja > 0. Construa a rede, exibindo explicitamente a escolha dos pesos."
      ],
      "metadata": {
        "id": "7ZYJtspT-Tv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Responda Aqui_"
      ],
      "metadata": {
        "id": "-FN62dl3-nU_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9QRAA46P94m6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}