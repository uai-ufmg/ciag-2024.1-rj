{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "UPScLKnJ9B66"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_classification, make_multilabel_classification\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "N--6xMnk9Vby"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introdução\n",
        "\n",
        "Esse notebook tem como intuito destacar os diferentes tipos de tareas de classificação, exemplificando quais funções de perda do pytorch devemos usar em cada caso. Aqui, temos duas tabelas que sumarizam as informações mais abaixo"
      ],
      "metadata": {
        "id": "8VbnG2RWVYBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Tipo de Classificação | Funções de Perda              | Formato da Label | Formato da Saída do Modelo | Ativação           |\n",
        "|------------------------|------------------------------|------------------|----------------------------|--------------------|\n",
        "| Binária                | `BCELoss`, `BCEWithLogitsLoss` | `[B]` ou `[B, 1]` | `[B, 1]`                    | `Sigmoid`          |\n",
        "| Multi-Classe           | `CrossEntropyLoss`, `NLLLoss` | `[B]`            | `[B, C]`                    | `Softmax`          |\n",
        "| Multi-Label            | `BCELoss`, `BCEWithLogitsLoss` | `[B, C]`         | `[B, C]`                    | `Sigmoid`          |\n"
      ],
      "metadata": {
        "id": "l2EKWnVtVN9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Função de Perda          | Tarefa            | Ativação Interna | Precisa Aplicar Ativação Antes? | Ativação Aplicada (se necessário) |\n",
        "|--------------------------|-------------------|------------------|----------------------------------|------------------------------------|\n",
        "| `BCELoss`               | Binária/Multi-Label | Não              | Sim                              | `Sigmoid`                          |\n",
        "| `BCEWithLogitsLoss`      | Binária/Multi-Label | Sim              | Não                              | Nenhuma                            |\n",
        "| `CrossEntropyLoss`       | Multi-Classe       | Sim              | Não                              | Nenhuma                            |\n",
        "| `NLLLoss`                | Multi-Classe       | Não              | Sim                              | `Softmax`                          |\n"
      ],
      "metadata": {
        "id": "cod4mSfFVWH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classificação Binária"
      ],
      "metadata": {
        "id": "H0t8unP8_ws-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classificação binária é aquela em que temos DUAS classes, a 0 e a 1. Nesses casos, a saída de nossa rede será tipicamente um número, que indica a probabilidade da imagem pertencer à classe 1.\n",
        "\n",
        "Um exemplo de classifcação binária e a classifcação de imagens em fotos de gatos, ou fotos de cachorros, em um conjunto de imagens em que todas as fotos tem exatamente um desses animais.\n",
        "\n",
        "Para a classificação binária, a saída de nossa rede deve conter uma noção probabilística da entrada pertencer à classe 1, ou seja, ela deve ser um número entre 0 e 1. Para isso, tipicamente usamos a *sigmoid* como a ativação de nossa rede.\n",
        "\n",
        "Podemos usar 2 funções de perda nesse caso:\n",
        "\n",
        "- A BCELoss;\n",
        "- A BCEWithLogitsLoss;\n",
        "\n",
        "> obs: a BCEWithLogitsLoss já faz o calculo da sigmoid internamente, de forma que, ao usá-la, sua rede não deve ter ativação na camada final."
      ],
      "metadata": {
        "id": "hXUrYbIiOvHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_samples = 300\n",
        "num_classes = 2\n",
        "num_features = 5\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=num_samples,\n",
        "    n_features=num_features,\n",
        "    n_informative=4,\n",
        "    n_redundant=0,\n",
        "    n_classes=num_classes,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float)\n",
        "\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "w4xDBQYq_z_u"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BCEWithLogitsLoss"
      ],
      "metadata": {
        "id": "Y5n78BtAQF4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temos dois casos usando a BCEWithLogitsLoss para classificação binária:\n",
        "- No primeiro, e mais comum, nossas labels e saídas da rede tem o formato: [B, 1]\n",
        "- No segundo, as labels e saídas tem o formato: [B]\n",
        "\n",
        "> aqui, B é o batch size\n",
        "\n"
      ],
      "metadata": {
        "id": "6RaAonTrQHUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleMLP(input_dim=num_features, hidden_dim=16, output_dim=1)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "model.train()\n",
        "losses = []\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_y = batch_y.unsqueeze(1) # Adicionando uma dimensão para que a label seja do formato [B, 1]\n",
        "\n",
        "        outputs = model(batch_x)\n",
        "\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    epoch_loss /= len(dataloader)\n",
        "    losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(f'outputs shape: {outputs.shape} || X example {outputs[0].detach()}')\n",
        "print(f'y shape: {batch_y.shape} || y example {batch_y[0]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ7765g0_1x7",
        "outputId": "ced5d817-a179-44fb-a404-fc58f9c2fa66"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 0.6474\n",
            "Epoch [2/10] - Loss: 0.5402\n",
            "Epoch [3/10] - Loss: 0.4754\n",
            "Epoch [4/10] - Loss: 0.4203\n",
            "Epoch [5/10] - Loss: 0.3652\n",
            "Epoch [6/10] - Loss: 0.3475\n",
            "Epoch [7/10] - Loss: 0.3085\n",
            "Epoch [8/10] - Loss: 0.2704\n",
            "Epoch [9/10] - Loss: 0.2659\n",
            "Epoch [10/10] - Loss: 0.2340\n",
            "outputs shape: torch.Size([12, 1]) || X example tensor([-1.0494])\n",
            "y shape: torch.Size([12, 1]) || y example tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleMLP(input_dim=num_features, hidden_dim=16, output_dim=1)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()  # For multi-class\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "model.train()\n",
        "losses = []\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_y = batch_y\n",
        "\n",
        "        outputs = model(batch_x)\n",
        "        outputs = outputs.squeeze(1) # Tirando uma dimensão para que a saída seja do formato [B]\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    epoch_loss /= len(dataloader)\n",
        "    losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(f'outputs shape: {outputs.shape} || X example {outputs[0].detach()}')\n",
        "print(f'y shape: {batch_y.shape} || y example {batch_y[0]}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCCg9vayA_fY",
        "outputId": "d726bbe7-ca64-4f97-cc2f-0d1664945e80"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 0.6566\n",
            "Epoch [2/10] - Loss: 0.5551\n",
            "Epoch [3/10] - Loss: 0.4756\n",
            "Epoch [4/10] - Loss: 0.4006\n",
            "Epoch [5/10] - Loss: 0.3618\n",
            "Epoch [6/10] - Loss: 0.3145\n",
            "Epoch [7/10] - Loss: 0.2881\n",
            "Epoch [8/10] - Loss: 0.2648\n",
            "Epoch [9/10] - Loss: 0.2560\n",
            "Epoch [10/10] - Loss: 0.2576\n",
            "outputs shape: torch.Size([12]) || X example 1.2056090831756592\n",
            "y shape: torch.Size([12]) || y example 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BCELoss"
      ],
      "metadata": {
        "id": "ORH1b3-bQpDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para a classificação binária, podemos também usar a BCELoss, embora o uso da versão com logits seja preferível. A BCELoss funciona da mesma forma que a BCEWithLogitsLoss, com a diferença que aqui não é feito o cálculo da sigmoid."
      ],
      "metadata": {
        "id": "uD4H7gzMQqtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleMLP(input_dim=num_features, hidden_dim=16, output_dim=1)\n",
        "\n",
        "criterion = nn.BCELoss()  # For multi-class\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "model.train()\n",
        "losses = []\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_y = batch_y.unsqueeze(1) # Adicionando uma dimensão para que o formato da label seja [B, 1]\n",
        "\n",
        "        outputs = torch.sigmoid(model(batch_x)) # Tirando a sigmoid da saída da rede. Tipicamente isso estaria dentro do modelo\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    epoch_loss /= len(dataloader)\n",
        "    losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "\n",
        "print(f'outputs shape: {outputs.shape} || X example {outputs[0].detach()}')\n",
        "print(f'y shape: {batch_y.shape} || y example {batch_y[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liSe1mWqBwzM",
        "outputId": "733b2e69-23f8-41a9-87aa-f7e725ba4946"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 0.6756\n",
            "Epoch [2/10] - Loss: 0.5994\n",
            "Epoch [3/10] - Loss: 0.5245\n",
            "Epoch [4/10] - Loss: 0.4587\n",
            "Epoch [5/10] - Loss: 0.4209\n",
            "Epoch [6/10] - Loss: 0.3617\n",
            "Epoch [7/10] - Loss: 0.3262\n",
            "Epoch [8/10] - Loss: 0.2997\n",
            "Epoch [9/10] - Loss: 0.2748\n",
            "Epoch [10/10] - Loss: 0.2672\n",
            "outputs shape: torch.Size([12, 1]) || X example tensor([0.2990])\n",
            "y shape: torch.Size([12, 1]) || y example tensor([1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleMLP(input_dim=num_features, hidden_dim=16, output_dim=1)\n",
        "\n",
        "criterion = nn.BCELoss()  # For multi-class\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "model.train()\n",
        "losses = []\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_y = batch_y\n",
        "\n",
        "        outputs = torch.sigmoid(model(batch_x)) # Tirando a sigmoid da saída da rede. Tipicamente isso estaria dentro do modelo\n",
        "        outputs = outputs.squeeze()  # Removendo uma dimensão para que o formato da saída seja [B]\n",
        "        loss = criterion(outputs.squeeze(), batch_y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    epoch_loss /= len(dataloader)\n",
        "    losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(f'outputs shape: {outputs.shape} || X example {outputs[0].detach()}')\n",
        "print(f'y shape: {batch_y.shape} || y example {batch_y[0]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4iGxdPKB8dE",
        "outputId": "0b7fc868-3f49-42c6-a221-a9c721308384"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 0.6812\n",
            "Epoch [2/10] - Loss: 0.5939\n",
            "Epoch [3/10] - Loss: 0.5132\n",
            "Epoch [4/10] - Loss: 0.4413\n",
            "Epoch [5/10] - Loss: 0.3875\n",
            "Epoch [6/10] - Loss: 0.3412\n",
            "Epoch [7/10] - Loss: 0.3078\n",
            "Epoch [8/10] - Loss: 0.2844\n",
            "Epoch [9/10] - Loss: 0.2567\n",
            "Epoch [10/10] - Loss: 0.2313\n",
            "outputs shape: torch.Size([12]) || X example 0.9987576007843018\n",
            "y shape: torch.Size([12]) || y example 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classificação Multi-Classe"
      ],
      "metadata": {
        "id": "8nmQdL_z_shU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classificação multi-classe é aquela em que temos múltiplas classes possíveis, mas apenas uma delas está presente em cada dado. Um exemplo desse tipo de classificação seria a classificação de uma imagem entre gatos, cachorros e alpacas, em um dataset em que exatamente um desses animais está presente por foto.\n",
        "\n",
        "Nesse tipo de tarefas, a saída de nossa rede deve indicar a probabilidade do dado pertencer a cada classe, ou seja, devemos ter um vetor em que cada valor $x_i$ indica a probabilidade de $X$ pertencer à classe $i$. Para manter a noção de probabilidade, devemos respeitar $\\sum_{x_i} = 1$, e para isso usamos a função softmax como ativação final de nossa rede.\n",
        "\n",
        "Aqui, podemos usar 2 funções de perda no pytorch: a CrossEntropyLoss e a NLLLoss"
      ],
      "metadata": {
        "id": "LONp2AYWRI3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CrossEntropyLoss"
      ],
      "metadata": {
        "id": "PrX2TtHNSLK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando a CrossEntropyLoss, o target deve ser um vetor de formato [B], em cada cada elemento é um número natural entre 0 e c-1 (c é o número de classes). Se $y_i = 4$, o dado $i$ pertence à classe 4. A saída da rede, em contrapartida, deve ser um vetor one-hot-encoded das labels, ou seja, deve ter o formato [B, C].\n",
        "\n",
        "A ativação softmax já é feita internamente na função, e logo não deve estar presente em seu modelo."
      ],
      "metadata": {
        "id": "jg0qpK1MSMqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_samples = 300\n",
        "num_classes = 3\n",
        "num_features = 5\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=num_samples,\n",
        "    n_features=num_features,\n",
        "    n_informative=4,\n",
        "    n_redundant=0,\n",
        "    n_classes=num_classes,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long) # Número inteiro\n",
        "\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Osofyse29gh6"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = SimpleMLP(input_dim=num_features, hidden_dim=16, output_dim=num_classes)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "model.train()\n",
        "losses = []\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_y = batch_y\n",
        "\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    epoch_loss /= len(dataloader)\n",
        "    losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(f'outputs shape: {outputs.shape} || X example {outputs[0].detach()}')\n",
        "print(f'y shape: {batch_y.shape} || y example {batch_y[0]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyxRViIE9luy",
        "outputId": "2c6f9ec7-7d86-476e-cd77-dad180d72ea8"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 1.0291\n",
            "Epoch [2/10] - Loss: 0.8586\n",
            "Epoch [3/10] - Loss: 0.7232\n",
            "Epoch [4/10] - Loss: 0.5967\n",
            "Epoch [5/10] - Loss: 0.5121\n",
            "Epoch [6/10] - Loss: 0.4587\n",
            "Epoch [7/10] - Loss: 0.4301\n",
            "Epoch [8/10] - Loss: 0.3975\n",
            "Epoch [9/10] - Loss: 0.3915\n",
            "Epoch [10/10] - Loss: 0.3610\n",
            "outputs shape: torch.Size([12, 3]) || X example tensor([-0.9542,  0.9502,  0.5576])\n",
            "y shape: torch.Size([12]) || y example 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NLLLoss\n",
        "\n",
        "A Negative Loss Likelihood funciona igual a entropia curzada anterior, com exceção que aqui o softmax não é feito internamente.\n",
        "\n",
        "É importante notar que, além do softmax, a NLL também requer que tiremos o log."
      ],
      "metadata": {
        "id": "O15gInN8Tgf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = SimpleMLP(input_dim=num_features, hidden_dim=16, output_dim=num_classes)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "model.train()\n",
        "losses = []\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_y = batch_y\n",
        "\n",
        "        #outputs = torch.log(torch.nn.functional.softmax(model(batch_x)))\n",
        "        outputs = torch.nn.functional.log_softmax(model(batch_x), dim=1) # Tirando o log da softmax para passar para a NLL\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    epoch_loss /= len(dataloader)\n",
        "    losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(f'outputs shape: {outputs.shape} || X example {outputs[0].detach()}')\n",
        "print(f'y shape: {batch_y.shape} || y example {batch_y[0]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVn3rvQB-oHW",
        "outputId": "cb6263dd-9631-4d94-fee1-9241e21520bc"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 1.0298\n",
            "Epoch [2/10] - Loss: 0.8680\n",
            "Epoch [3/10] - Loss: 0.7542\n",
            "Epoch [4/10] - Loss: 0.6551\n",
            "Epoch [5/10] - Loss: 0.5735\n",
            "Epoch [6/10] - Loss: 0.5301\n",
            "Epoch [7/10] - Loss: 0.4689\n",
            "Epoch [8/10] - Loss: 0.4267\n",
            "Epoch [9/10] - Loss: 0.4030\n",
            "Epoch [10/10] - Loss: 0.3786\n",
            "outputs shape: torch.Size([12, 3]) || X example tensor([-1.6816, -1.6763, -0.4670])\n",
            "y shape: torch.Size([12]) || y example 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Label Classification"
      ],
      "metadata": {
        "id": "SkQqWmltEo_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classificação multi-label é aquela em que temos múltiplas classes, e várias delas podem estar presentes simultâneamente em um dado. Um exemplo disso é a identificação de objetos em imagens: cada imagem contém vários objetos diferentes que queremos localizar.\n",
        "\n",
        "Nesses casos, a saída de nossa rede é um vetor te formato [B, C], em que cada valor é um número entre 0 e 1, indicando a probabilidade daquela classe estar presente naquele dado. Tipicamente, nesse caso, usamos a sigmoid como função de ativação.\n",
        "\n",
        "Como funções de perda, temos as mesmas que as da classificação binária (note que a classificação multi-label se resume a C classificações binárias distintas). Aqui, no entanto, devemos necessariamente ter uma saída de formato [B, C]."
      ],
      "metadata": {
        "id": "qkhCGTXDT3lM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a multi-class dataset\n",
        "num_samples = 300\n",
        "num_classes = 5\n",
        "num_features = 5\n",
        "\n",
        "X, y = make_multilabel_classification(\n",
        "    n_samples=num_samples,\n",
        "    n_features=num_features,\n",
        "    n_classes=num_classes,\n",
        "    n_labels=2,\n",
        "    allow_unlabeled=False,\n",
        "    random_state=42\n",
        "\n",
        ")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float)  # Must be long for classification\n",
        "\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "krfCjjQPErpH"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleMLP(input_dim=num_features, hidden_dim=16, output_dim=num_classes)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()  # For multi-class\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "model.train()\n",
        "losses = []\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_y = batch_y\n",
        "\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    epoch_loss /= len(dataloader)\n",
        "    losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(f'outputs shape: {outputs.shape} || X example {outputs[0].detach()}')\n",
        "print(f'y shape: {batch_y.shape} || y example {batch_y[0]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSnJofQdCxai",
        "outputId": "0e11f1b1-71b9-4740-b55d-ff46c6b1443e"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 1.0743\n",
            "Epoch [2/10] - Loss: 0.6407\n",
            "Epoch [3/10] - Loss: 0.5543\n",
            "Epoch [4/10] - Loss: 0.5017\n",
            "Epoch [5/10] - Loss: 0.4698\n",
            "Epoch [6/10] - Loss: 0.4504\n",
            "Epoch [7/10] - Loss: 0.4359\n",
            "Epoch [8/10] - Loss: 0.4287\n",
            "Epoch [9/10] - Loss: 0.4067\n",
            "Epoch [10/10] - Loss: 0.3996\n",
            "outputs shape: torch.Size([12, 5]) || X example tensor([-1.7118,  2.8388,  1.6806,  0.2157, -3.4287])\n",
            "y shape: torch.Size([12, 5]) || y example tensor([0., 1., 1., 1., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleMLP(input_dim=num_features, hidden_dim=16, output_dim=num_classes)\n",
        "\n",
        "criterion = nn.BCELoss()  # For multi-class\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "model.train()\n",
        "losses = []\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_y = batch_y\n",
        "\n",
        "        outputs = torch.sigmoid(model(batch_x))\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    epoch_loss /= len(dataloader)\n",
        "    losses.append(epoch_loss)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(f'outputs shape: {outputs.shape} || X example {outputs[0].detach()}')\n",
        "print(f'y shape: {batch_y.shape} || y example {batch_y[0]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AFkjksDFfQA",
        "outputId": "41126121-3826-4f0a-c65f-4afcc06651af"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 0.6376\n",
            "Epoch [2/10] - Loss: 0.4693\n",
            "Epoch [3/10] - Loss: 0.4228\n",
            "Epoch [4/10] - Loss: 0.4049\n",
            "Epoch [5/10] - Loss: 0.3812\n",
            "Epoch [6/10] - Loss: 0.3794\n",
            "Epoch [7/10] - Loss: 0.3775\n",
            "Epoch [8/10] - Loss: 0.3830\n",
            "Epoch [9/10] - Loss: 0.3703\n",
            "Epoch [10/10] - Loss: 0.3672\n",
            "outputs shape: torch.Size([12, 5]) || X example tensor([0.0288, 0.3328, 0.9738, 0.3886, 0.0761])\n",
            "y shape: torch.Size([12, 5]) || y example tensor([0., 1., 1., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggplqwPLFoFq"
      },
      "execution_count": 124,
      "outputs": []
    }
  ]
}